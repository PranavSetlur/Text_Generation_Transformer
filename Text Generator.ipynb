{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a2e19d",
   "metadata": {},
   "source": [
    "# Text Generator\n",
    "Implementing a text generation model from scratch using a transformer (decoder only).\\\n",
    "Steps:\n",
    "1. Tokenization\n",
    "2. Input embedding\n",
    "3. Positional encoding\n",
    "4. Masking\n",
    "5. Self-attention\n",
    "6. Decoder stack\n",
    "7. Predicting token probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b58e15",
   "metadata": {},
   "source": [
    "## Creating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b1cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install pytorch torchvision torchaudio -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa7f1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6424458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class creating_data():\n",
    "    def __init__(self, filepath):\n",
    "        self.df = pd.read_csv(filepath)\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.df.to_csv(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea755bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = creating_data('medium_articles.csv')\n",
    "# dataset.save('training_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46dee6c",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5068fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        self.dictionary = {}\n",
    "        self.reverse_dictionary = {}\n",
    "        \n",
    "        # adding special tokens\n",
    "        self.__add_to_dict('<pad>')\n",
    "        self.__add_to_dict('<unk>')\n",
    "        \n",
    "        # add characters and numbers to dictionary\n",
    "        for i in range(10):\n",
    "            self.__add_to_dict(str(i))\n",
    "        \n",
    "        for i in range(26):\n",
    "            self.__add_to_dict(chr(ord('a') + i))\n",
    "            self.__add_to_dict(chr(ord('A') + i))\n",
    "            \n",
    "        # adding space and punctuation\n",
    "        for char in ['.', ' ', ',', '!', '?', '\\n']:\n",
    "            self.__add_to_dict(char)\n",
    "        \n",
    "    def __add_to_dict(self, character):\n",
    "        if character not in self.dictionary:\n",
    "            index = self.size()\n",
    "            self.dictionary[character] = index\n",
    "            self.reverse_dictionary[index] = character\n",
    "            \n",
    "    def tokenize(self, text):\n",
    "        return [self.character_to_token(c) for c in text]\n",
    "    \n",
    "    def character_to_token(self, character):\n",
    "        return self.dictionary.get(character, self.dictionary['<unk>'])\n",
    "    \n",
    "    def token_to_character(self, token):\n",
    "        return self.reverse_dictionary.get(token, '<unk>')\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e166c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('training_data.csv')\n",
    "training_data = training_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75dbb597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...\n",
       "1    Your Brain On Coronavirus\\n\\nA guide to the cu...\n",
       "2    Mind Your Nose\\n\\nHow smell training can chang...\n",
       "3    Passionate about the synergy between science a...\n",
       "4    You’ve heard of him, haven’t you? Phineas Gage...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4187758",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9938290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenized_data = [tokenizer.tokenize(sentence) for sentence in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c4a2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 20\n",
    "# padding and truncating\n",
    "padded_data = []\n",
    "\n",
    "for tokens in tokenized_data:\n",
    "    if len(tokens) < max_sequence_length:\n",
    "        # padding\n",
    "        tokens = [tokenizer.character_to_token('<pad>')] * (max_sequence_length - len(tokens)) + tokens\n",
    "    else:\n",
    "        # truncating\n",
    "        tokens = tokens[:max_sequence_length]\n",
    "    padded_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8be394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting data to tensors\n",
    "tensor_data = [torch.tensor(tokens) for tokens in padded_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d9d4c",
   "metadata": {},
   "source": [
    "## Input Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "113d5547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(torch.nn.Module):\n",
    "    # model that converts tokens into embeddings\n",
    "    \n",
    "    def __init__(self, model_dim, num_tokens):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = torch.nn.Embedding(\n",
    "            num_embeddings = num_tokens,\n",
    "            embedding_dim = model_dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58fb2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim = 50\n",
    "num_tokens = tokenizer.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eec917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing class\n",
    "embedding_model = TokenEmbedding(model_dim, num_tokens)\n",
    "# convert padded data to tensor\n",
    "tensor_data = torch.stack(tensor_data)\n",
    "embedded_data = embedding_model(tensor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f9a3f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedded data: torch.Size([100, 20, 50])\n",
      "First embedded sequence: tensor([[ 0.8844, -0.8275,  0.1637,  0.4399, -1.2731,  1.0260,  0.8135, -0.3600,\n",
      "          0.4726,  0.3255, -1.5106, -0.6047, -0.5044, -1.2279, -0.1926,  0.6179,\n",
      "          1.8251, -0.3024, -0.9104,  0.0415, -0.2966,  0.7469, -1.2024, -1.3591,\n",
      "          0.7897, -0.1609, -0.5841, -0.2200, -0.4246,  1.8333, -0.4698,  1.0196,\n",
      "         -0.2160,  0.5330,  0.3564,  1.9847,  1.1330,  0.6115, -0.6802, -1.2569,\n",
      "          0.7539,  0.2378,  0.7264, -0.5260,  1.9869, -0.6171, -0.5291, -2.1909,\n",
      "         -0.0544, -0.6772],\n",
      "        [ 0.1019, -0.5571,  0.8181, -0.5467,  0.0855, -0.0229,  0.7933, -2.2315,\n",
      "          2.0713,  0.5831, -0.2309, -0.1784,  0.7542, -1.4542,  0.8075, -0.1818,\n",
      "         -0.7413,  0.1827, -1.1486,  1.0028,  0.3147, -0.1738, -1.2141,  0.3591,\n",
      "         -0.3803,  0.2126, -0.4082, -0.7280,  0.9221,  0.2378,  2.2642, -0.1053,\n",
      "         -0.8234, -1.6579,  1.5341, -0.7315, -1.4654,  0.2314,  2.3472, -0.4193,\n",
      "          0.5120, -0.5192, -1.2963,  0.0747,  0.4887, -0.4390,  0.5180, -1.1015,\n",
      "         -0.2572, -1.2918],\n",
      "        [-1.2080, -2.5160, -1.0886,  0.6210, -1.2790,  1.4386,  1.3388,  0.4956,\n",
      "          0.9090,  0.6815,  1.7011, -0.5922,  1.9448, -2.4151, -2.0903,  0.0682,\n",
      "         -0.5117,  1.0835, -0.9874,  0.2923,  0.5528, -0.1162,  1.0252, -0.6330,\n",
      "          0.0459, -1.1612, -0.1950,  0.4283,  1.4812,  2.3644, -0.0270, -0.4035,\n",
      "          0.6318,  1.4690,  0.9240, -0.0065,  0.6698,  1.1342, -1.9295,  0.1392,\n",
      "         -0.5998, -0.4972,  0.2326, -1.8230,  0.1359,  0.0239,  0.0796,  0.7281,\n",
      "         -0.9715,  2.2970],\n",
      "        [ 0.4131,  0.1469, -0.6381,  0.5290,  0.8709,  0.5070, -1.2856,  0.7523,\n",
      "         -0.1084, -1.2804,  0.6456, -0.5228, -0.3069, -0.5623, -0.8514,  0.6239,\n",
      "         -2.3537, -0.0595, -0.2566, -1.1072, -0.2048,  0.2741,  0.4627,  0.3865,\n",
      "          0.6246, -0.4379,  0.8279, -1.0823,  0.2347,  1.4338,  0.2942,  0.4052,\n",
      "          1.3964, -0.2752,  0.9388, -1.2804, -0.9180,  1.3416,  1.7500,  1.2240,\n",
      "         -0.1817, -0.2162,  0.9872, -1.5163,  0.9210, -0.1696, -0.8374, -2.7456,\n",
      "         -1.2485, -1.4445],\n",
      "        [-1.2080, -2.5160, -1.0886,  0.6210, -1.2790,  1.4386,  1.3388,  0.4956,\n",
      "          0.9090,  0.6815,  1.7011, -0.5922,  1.9448, -2.4151, -2.0903,  0.0682,\n",
      "         -0.5117,  1.0835, -0.9874,  0.2923,  0.5528, -0.1162,  1.0252, -0.6330,\n",
      "          0.0459, -1.1612, -0.1950,  0.4283,  1.4812,  2.3644, -0.0270, -0.4035,\n",
      "          0.6318,  1.4690,  0.9240, -0.0065,  0.6698,  1.1342, -1.9295,  0.1392,\n",
      "         -0.5998, -0.4972,  0.2326, -1.8230,  0.1359,  0.0239,  0.0796,  0.7281,\n",
      "         -0.9715,  2.2970],\n",
      "        [-1.0182, -0.4707, -0.0937, -1.6892,  0.0349,  0.1601,  1.3042,  0.8693,\n",
      "         -0.2135, -1.1602,  0.2943, -0.2157,  1.0111,  0.3842, -0.3818, -1.0770,\n",
      "         -2.1417,  0.0731,  0.0357, -1.4772, -1.1231, -0.3233,  0.1697,  1.6325,\n",
      "          0.4275, -0.3280, -0.0239, -0.3763, -0.2619,  0.5227, -1.8814,  0.6010,\n",
      "          1.7857,  0.3711, -0.9404, -1.5067, -0.6815, -0.4750,  1.5283,  0.6585,\n",
      "         -0.0166, -0.2520, -0.3404,  0.1477,  1.2523,  1.3007,  0.2365, -0.5136,\n",
      "          1.3522, -0.0424],\n",
      "        [-1.2026, -0.4359, -1.4748,  0.7714,  0.3899, -1.6536,  2.7293,  0.6402,\n",
      "         -1.1441, -1.3175,  1.2797,  1.3140,  0.3103,  1.0748,  0.2275,  0.4351,\n",
      "          0.8747,  0.7818,  0.5010, -1.0287, -0.5486, -0.0072, -0.4796, -0.6674,\n",
      "          1.5676, -2.0172,  0.5861, -0.8591,  0.6728, -0.5811,  1.2436, -0.8357,\n",
      "         -0.1812, -0.2076,  0.3386,  0.4308,  0.1184, -0.0729,  1.0282,  0.8099,\n",
      "          0.6231,  1.3406,  1.7955, -0.0561, -2.1674, -0.4809, -0.8009,  0.0270,\n",
      "          0.0952,  0.2286],\n",
      "        [-1.0759,  0.7148,  0.9109, -0.3377,  0.1833,  1.5099,  1.1239, -0.4721,\n",
      "          0.4174,  0.7188,  0.8245,  0.0814, -0.7029,  0.9812,  0.4675,  0.3444,\n",
      "         -0.3374,  0.8544,  1.1640,  0.1694, -0.5961,  1.4836, -0.9619, -0.2842,\n",
      "          0.9899,  1.0653,  0.0740, -0.3881,  0.3770, -0.7349,  1.4094, -1.1850,\n",
      "          1.2229, -2.0475,  0.1530, -0.3782, -0.0775,  0.8083, -1.3900,  0.2940,\n",
      "         -0.6322, -0.1462, -0.5579,  1.5533, -0.1950,  0.3636,  1.7960,  0.1183,\n",
      "         -0.3894,  2.0342],\n",
      "        [-1.0182, -0.4707, -0.0937, -1.6892,  0.0349,  0.1601,  1.3042,  0.8693,\n",
      "         -0.2135, -1.1602,  0.2943, -0.2157,  1.0111,  0.3842, -0.3818, -1.0770,\n",
      "         -2.1417,  0.0731,  0.0357, -1.4772, -1.1231, -0.3233,  0.1697,  1.6325,\n",
      "          0.4275, -0.3280, -0.0239, -0.3763, -0.2619,  0.5227, -1.8814,  0.6010,\n",
      "          1.7857,  0.3711, -0.9404, -1.5067, -0.6815, -0.4750,  1.5283,  0.6585,\n",
      "         -0.0166, -0.2520, -0.3404,  0.1477,  1.2523,  1.3007,  0.2365, -0.5136,\n",
      "          1.3522, -0.0424],\n",
      "        [-0.0118,  0.8762,  0.9390,  0.8265, -1.1260,  1.5945, -0.3878,  0.3498,\n",
      "          0.1291, -1.1940,  0.0718, -0.6934,  0.9056, -1.0175, -0.1324, -0.2225,\n",
      "         -1.2783,  1.8057,  2.4594, -1.7896, -0.9256, -0.1521, -1.1441, -1.0382,\n",
      "         -0.5212, -0.3585, -0.9465, -0.4195,  0.4095, -0.4910,  1.1076,  0.2496,\n",
      "         -1.1184, -0.1967, -1.4132,  1.0107,  3.2165,  1.7131, -0.0597,  0.8336,\n",
      "         -0.2448, -1.3022,  0.9793, -0.2971,  0.1551, -0.6985, -0.0634,  0.7164,\n",
      "         -0.1179, -0.8786],\n",
      "        [-1.2080, -2.5160, -1.0886,  0.6210, -1.2790,  1.4386,  1.3388,  0.4956,\n",
      "          0.9090,  0.6815,  1.7011, -0.5922,  1.9448, -2.4151, -2.0903,  0.0682,\n",
      "         -0.5117,  1.0835, -0.9874,  0.2923,  0.5528, -0.1162,  1.0252, -0.6330,\n",
      "          0.0459, -1.1612, -0.1950,  0.4283,  1.4812,  2.3644, -0.0270, -0.4035,\n",
      "          0.6318,  1.4690,  0.9240, -0.0065,  0.6698,  1.1342, -1.9295,  0.1392,\n",
      "         -0.5998, -0.4972,  0.2326, -1.8230,  0.1359,  0.0239,  0.0796,  0.7281,\n",
      "         -0.9715,  2.2970],\n",
      "        [-1.2221, -0.2053, -0.1886, -1.7000,  0.2405, -1.2319,  0.9306,  0.3972,\n",
      "         -0.2016, -0.3794, -1.3427,  0.4291, -0.2273,  0.3971,  1.3995, -0.3684,\n",
      "         -0.0101,  0.8227,  0.1394,  1.1275,  0.3013, -0.7108, -0.2578, -0.5023,\n",
      "         -0.2639,  1.6135,  0.2155,  0.0941,  0.1511,  0.2833, -0.4733, -0.6145,\n",
      "          0.1642,  0.4909, -0.0873, -0.1070,  0.7759,  0.0888, -0.3773, -0.3963,\n",
      "         -0.7284, -0.4367,  0.1422,  0.5802,  0.7967,  0.7724,  0.9344,  0.6888,\n",
      "         -0.7256, -0.7160],\n",
      "        [ 0.1019, -0.5571,  0.8181, -0.5467,  0.0855, -0.0229,  0.7933, -2.2315,\n",
      "          2.0713,  0.5831, -0.2309, -0.1784,  0.7542, -1.4542,  0.8075, -0.1818,\n",
      "         -0.7413,  0.1827, -1.1486,  1.0028,  0.3147, -0.1738, -1.2141,  0.3591,\n",
      "         -0.3803,  0.2126, -0.4082, -0.7280,  0.9221,  0.2378,  2.2642, -0.1053,\n",
      "         -0.8234, -1.6579,  1.5341, -0.7315, -1.4654,  0.2314,  2.3472, -0.4193,\n",
      "          0.5120, -0.5192, -1.2963,  0.0747,  0.4887, -0.4390,  0.5180, -1.1015,\n",
      "         -0.2572, -1.2918],\n",
      "        [-1.0182, -0.4707, -0.0937, -1.6892,  0.0349,  0.1601,  1.3042,  0.8693,\n",
      "         -0.2135, -1.1602,  0.2943, -0.2157,  1.0111,  0.3842, -0.3818, -1.0770,\n",
      "         -2.1417,  0.0731,  0.0357, -1.4772, -1.1231, -0.3233,  0.1697,  1.6325,\n",
      "          0.4275, -0.3280, -0.0239, -0.3763, -0.2619,  0.5227, -1.8814,  0.6010,\n",
      "          1.7857,  0.3711, -0.9404, -1.5067, -0.6815, -0.4750,  1.5283,  0.6585,\n",
      "         -0.0166, -0.2520, -0.3404,  0.1477,  1.2523,  1.3007,  0.2365, -0.5136,\n",
      "          1.3522, -0.0424],\n",
      "        [ 1.1207,  0.8763, -0.7948, -0.7386,  2.5562, -0.8354, -0.2693,  1.2474,\n",
      "         -0.7212,  0.4747, -1.5344, -0.5912, -2.6588, -0.0586, -0.4220,  0.2592,\n",
      "          0.5759, -1.8471, -1.0853, -1.0688, -0.7949, -1.6521, -0.3002, -0.2619,\n",
      "         -1.9510,  0.3761, -1.7175,  0.8933, -0.9802,  0.0117, -1.0489, -1.2968,\n",
      "         -1.8215,  0.4279,  1.8117,  1.2109,  0.9162,  0.5883,  0.2293, -0.4573,\n",
      "          0.0895,  0.5720,  0.9385,  1.0892, -0.1686, -0.6250, -0.2423,  1.6821,\n",
      "         -0.4521, -0.0914],\n",
      "        [-0.5243,  0.2933, -0.3204, -1.0521,  1.2093, -0.6808, -0.5823, -0.8263,\n",
      "         -0.3577,  1.0579,  0.2663, -0.6277,  2.4788,  1.2738, -0.9622,  2.1354,\n",
      "          0.8875,  0.6535,  0.8868,  0.7459,  0.1572,  0.7793,  1.4458,  0.9717,\n",
      "          1.4991,  0.1269,  0.6614,  0.0442,  0.6441, -0.5914,  1.0793,  0.5404,\n",
      "          0.2343,  1.0709,  0.4343,  0.6960, -0.2616, -0.6445,  0.3896,  0.1993,\n",
      "         -0.3880,  1.0592, -0.3938,  1.2064,  0.7488,  1.7113, -0.4503, -0.6705,\n",
      "         -0.5011, -0.4315],\n",
      "        [-0.9634,  0.9221,  0.4788,  0.2286,  1.6119, -1.7400, -0.5382, -0.0628,\n",
      "         -1.0729,  0.1320, -0.7619, -0.1766, -0.7058, -0.0164,  0.2269, -1.5168,\n",
      "          1.4367, -2.0154,  0.0928, -1.6930, -0.3508,  0.0469, -0.8445,  0.2065,\n",
      "          0.4057,  0.3981, -0.5629,  2.0615,  0.5223,  0.3030, -0.1817, -0.0236,\n",
      "          0.0580, -0.2125,  0.0544,  0.6060, -0.2138,  0.2471, -0.2285,  0.9865,\n",
      "          1.7967,  0.2941, -0.3544,  0.5246,  0.7302,  1.1265,  2.0034,  1.1290,\n",
      "          0.2733,  0.2082],\n",
      "        [ 1.2595,  0.6093, -0.2132,  0.3206, -2.1472,  0.6159, -0.5742, -0.6291,\n",
      "         -0.4956, -0.7844,  0.1231,  0.6174, -1.6739, -0.2273, -0.8969,  1.7668,\n",
      "         -0.3351,  1.3365, -0.1481,  0.5514,  1.8287,  1.9147,  0.7816,  0.3138,\n",
      "         -0.1824, -0.5113,  0.2610,  1.2862,  0.5307,  1.6829, -0.4950, -0.8914,\n",
      "          0.9561, -0.8302, -0.7503,  0.2825, -0.6511,  1.6210, -0.9592, -0.0312,\n",
      "          1.9400, -1.1218, -0.4206, -0.0752,  0.3611,  0.3228, -0.0135,  0.7364,\n",
      "          1.8094, -0.6811],\n",
      "        [-0.9634,  0.9221,  0.4788,  0.2286,  1.6119, -1.7400, -0.5382, -0.0628,\n",
      "         -1.0729,  0.1320, -0.7619, -0.1766, -0.7058, -0.0164,  0.2269, -1.5168,\n",
      "          1.4367, -2.0154,  0.0928, -1.6930, -0.3508,  0.0469, -0.8445,  0.2065,\n",
      "          0.4057,  0.3981, -0.5629,  2.0615,  0.5223,  0.3030, -0.1817, -0.0236,\n",
      "          0.0580, -0.2125,  0.0544,  0.6060, -0.2138,  0.2471, -0.2285,  0.9865,\n",
      "          1.7967,  0.2941, -0.3544,  0.5246,  0.7302,  1.1265,  2.0034,  1.1290,\n",
      "          0.2733,  0.2082],\n",
      "        [-0.2845,  0.1173, -0.3911,  0.8519, -0.6226, -1.6951,  0.8476,  0.5217,\n",
      "          0.3940,  0.7511, -0.6918,  0.9724, -0.0343,  0.9158, -0.0533,  1.5346,\n",
      "         -1.2170,  0.4473, -0.2385, -2.1658, -0.0479, -1.8344,  1.1123,  1.1342,\n",
      "         -0.8729,  0.3452,  1.1878,  0.2206, -1.0577,  0.8120,  0.3242,  1.2103,\n",
      "         -0.9610, -0.5739,  1.9692,  0.5815, -1.3079, -1.4763, -1.3945, -1.1418,\n",
      "         -0.5008,  1.3450,  1.6255,  0.5638, -0.1756, -0.2885,  1.2945,  0.0915,\n",
      "          1.4145,  1.1319]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the embedded data to verify\n",
    "print(\"Shape of embedded data:\", embedded_data.shape)\n",
    "\n",
    "# Print the first embedded sequence for verification\n",
    "print(\"First embedded sequence:\", embedded_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b6c9cc",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f6d2a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, model_dim, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "    \n",
    "        positional_encoding = np.zeros((max_sequence_length, model_dim))\n",
    "        \n",
    "        # calculating encoding for each position and dim\n",
    "        for pos in range(max_sequence_length):\n",
    "            for i in range(0, self.model_dim, 2):\n",
    "                # sin to even indices\n",
    "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / model_dim)))\n",
    "                \n",
    "                # cos to odd indices\n",
    "                if i + 1 < self.model_dim:\n",
    "                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / model_dim)))\n",
    "                    \n",
    "        \n",
    "        self.positional_encoding = torch.from_numpy(positional_encoding).unsqueeze(0).float()\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.positional_encoding[:, : seq_len, :]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "266f7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding = PositionalEncoding(model_dim, max_sequence_length)\n",
    "encoded_data = pos_encoding(embedded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0a20c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoded data: torch.Size([100, 20, 50])\n",
      "First encoded sequence: tensor([[ 0.8844,  0.1725,  0.1637,  1.4399, -1.2731,  2.0260,  0.8135,  0.6400,\n",
      "          0.4726,  1.3255, -1.5106,  0.3953, -0.5044, -0.2279, -0.1926,  1.6179,\n",
      "          1.8251,  0.6976, -0.9104,  1.0415, -0.2966,  1.7469, -1.2024, -0.3591,\n",
      "          0.7897,  0.8391, -0.5841,  0.7800, -0.4246,  2.8333, -0.4698,  2.0196,\n",
      "         -0.2160,  1.5330,  0.3564,  2.9847,  1.1330,  1.6115, -0.6802, -0.2569,\n",
      "          0.7539,  1.2378,  0.7264,  0.4740,  1.9869,  0.3829, -0.5291, -1.1909,\n",
      "         -0.0544,  0.3228],\n",
      "        [ 0.9434, -0.0168,  1.2786,  0.3409,  0.3126,  0.9510,  0.9027, -1.2375,\n",
      "          2.1238,  1.5818, -0.2058,  0.8212,  0.7662, -0.4543,  0.8132,  0.8182,\n",
      "         -0.7385,  1.1827, -1.1473,  2.0028,  0.3154,  0.8262, -1.2138,  1.3591,\n",
      "         -0.3802,  1.2126, -0.4082,  0.2720,  0.9221,  1.2378,  2.2642,  0.8947,\n",
      "         -0.8234, -0.6579,  1.5341,  0.2685, -1.4654,  1.2314,  2.3472,  0.5807,\n",
      "          0.5120,  0.4808, -1.2963,  1.0747,  0.4887,  0.5610,  0.5180, -0.1015,\n",
      "         -0.2572, -0.2918],\n",
      "        [-0.2987, -2.9321, -0.2709,  1.1968, -0.8367,  2.3355,  1.5564,  1.4717,\n",
      "          1.0138,  1.6760,  1.7513,  0.4066,  1.9689, -1.4154, -2.0788,  1.0681,\n",
      "         -0.5062,  2.0835, -0.9848,  1.2923,  0.5541,  0.8838,  1.0258,  0.3670,\n",
      "          0.0462, -0.1612, -0.1948,  1.4283,  1.4813,  3.3644, -0.0270,  0.5965,\n",
      "          0.6318,  2.4690,  0.9240,  0.9935,  0.6698,  2.1342, -1.9295,  1.1392,\n",
      "         -0.5998,  0.5028,  0.2326, -0.8230,  0.1359,  1.0239,  0.0796,  1.7281,\n",
      "         -0.9715,  3.2970],\n",
      "        [ 0.5543, -0.8431,  0.3528,  0.6635,  1.5053,  1.2800, -0.9626,  1.6986,\n",
      "          0.0484, -0.2928,  0.7209,  0.4743, -0.2708,  0.4370, -0.8342,  1.6238,\n",
      "         -2.3455,  0.9405, -0.2527, -0.1072, -0.2029,  1.2741,  0.4636,  1.3865,\n",
      "          0.6250,  0.5621,  0.8281, -0.0823,  0.2348,  2.4338,  0.2942,  1.4052,\n",
      "          1.3965,  0.7248,  0.9388, -0.2804, -0.9179,  2.3416,  1.7500,  2.2240,\n",
      "         -0.1817,  0.7838,  0.9872, -0.5163,  0.9210,  0.8304, -0.8374, -1.7456,\n",
      "         -1.2485, -0.4445],\n",
      "        [-1.9648, -3.1696, -0.1470,  0.2840, -0.4856,  2.0473,  1.7635,  1.4010,\n",
      "          1.1174,  1.6595,  1.8014,  0.4028,  1.9929, -1.4163, -2.0673,  1.0679,\n",
      "         -0.5007,  2.0834, -0.9821,  1.2922,  0.5554,  0.8838,  1.0264,  0.3670,\n",
      "          0.0465, -0.1612, -0.1947,  1.4283,  1.4813,  3.3644, -0.0269,  0.5965,\n",
      "          0.6318,  2.4690,  0.9240,  0.9935,  0.6698,  2.1342, -1.9295,  1.1392,\n",
      "         -0.5998,  0.5028,  0.2326, -0.8230,  0.1359,  1.0239,  0.0796,  1.7281,\n",
      "         -0.9715,  3.2970],\n",
      "        [-1.9771, -0.1870,  0.5868, -2.4220,  0.9458,  0.5728,  1.8254,  1.7227,\n",
      "          0.0459, -0.1945,  0.4195,  0.7764,  1.0711,  1.3824, -0.3531, -0.0774,\n",
      "         -2.1280,  1.0730,  0.0423, -0.4772, -1.1199,  0.6767,  0.1712,  2.6325,\n",
      "          0.4283,  0.6720, -0.0236,  0.6237, -0.2617,  1.5227, -1.8813,  1.6010,\n",
      "          1.7857,  1.3711, -0.9404, -0.5067, -0.6815,  0.5250,  1.5283,  1.6585,\n",
      "         -0.0166,  0.7480, -0.3404,  1.1477,  1.2523,  2.3007,  0.2365,  0.4864,\n",
      "          1.3522,  0.9576],\n",
      "        [-1.4821,  0.5242, -1.2083, -0.1924,  1.3707, -1.4586,  3.3408,  1.4315,\n",
      "         -0.8344, -0.3667,  1.4299,  2.3027,  0.3824,  2.0722,  0.2620,  1.4345,\n",
      "          0.8912,  1.7817,  0.5089, -0.0287, -0.5448,  0.9927, -0.4777,  0.3326,\n",
      "          1.5685, -1.0172,  0.5865,  0.1409,  0.6730,  0.4189,  1.2437,  0.1643,\n",
      "         -0.1812,  0.7924,  0.3386,  1.4308,  0.1184,  0.9271,  1.0282,  1.8099,\n",
      "          0.6231,  2.3406,  1.7955,  0.9439, -2.1674,  0.5191, -0.8009,  1.0270,\n",
      "          0.0952,  1.2286],\n",
      "        [-0.4190,  1.4687,  0.7036, -1.3160,  1.1828,  1.4771,  1.8182,  0.2475,\n",
      "          0.7765,  1.6521,  0.9995,  1.0660, -0.6188,  1.9776,  0.5078,  1.3436,\n",
      "         -0.3181,  1.8543,  1.1733,  1.1693, -0.5917,  2.4836, -0.9598,  0.7158,\n",
      "          0.9909,  2.0653,  0.0745,  0.6119,  0.3772,  0.2651,  1.4095, -0.1850,\n",
      "          1.2230, -1.0475,  0.1530,  0.6218, -0.0775,  1.8083, -1.3900,  1.2940,\n",
      "         -0.6322,  0.8538, -0.5579,  2.5533, -0.1950,  1.3636,  1.7960,  1.1183,\n",
      "         -0.3894,  3.0342],\n",
      "        [-0.0289, -0.6162, -0.7283, -2.4621,  1.0008, -0.0988,  2.0731,  1.5086,\n",
      "          0.1941, -0.2471,  0.4939,  0.7642,  1.1071,  1.3796, -0.3358, -0.0781,\n",
      "         -2.1197,  1.0728,  0.0463, -0.4772, -1.1180,  0.6767,  0.1722,  2.6325,\n",
      "          0.4287,  0.6720, -0.0234,  0.6237, -0.2616,  1.5227, -1.8812,  1.6010,\n",
      "          1.7858,  1.3711, -0.9404, -0.5067, -0.6815,  0.5250,  1.5283,  1.6585,\n",
      "         -0.0166,  0.7480, -0.3404,  1.1477,  1.2523,  2.3007,  0.2365,  0.4864,\n",
      "          1.3522,  0.9576],\n",
      "        [ 0.4003, -0.0349,  0.0198,  0.4328, -0.2441,  1.1230,  0.4465,  0.9012,\n",
      "          0.5841, -0.3035,  0.2960,  0.2812,  1.0135, -0.0234, -0.0806,  0.7762,\n",
      "         -1.2535,  2.8054,  2.4712, -0.7897, -0.9199,  0.8479, -1.1413, -0.0382,\n",
      "         -0.5199,  0.6415, -0.9458,  0.5805,  0.4098,  0.5090,  1.1078,  1.2496,\n",
      "         -1.1183,  0.8033, -1.4131,  2.0107,  3.2165,  2.7131, -0.0597,  1.8336,\n",
      "         -0.2448, -0.3022,  0.9793,  0.7029,  0.1551,  0.3015, -0.0634,  1.7164,\n",
      "         -0.1179,  0.1214],\n",
      "        [-1.7520, -3.3550, -2.0858,  0.6949, -0.5272,  0.7792,  2.2284,  0.9523,\n",
      "          1.4101,  1.5469,  1.9496,  0.3764,  2.0648, -1.4223, -2.0328,  1.0665,\n",
      "         -0.4842,  2.0831, -0.9742,  1.2922,  0.5592,  0.8838,  1.0282,  0.3669,\n",
      "          0.0474, -0.1612, -0.1943,  1.4283,  1.4815,  3.3644, -0.0268,  0.5965,\n",
      "          0.6319,  2.4690,  0.9241,  0.9935,  0.6698,  2.1342, -1.9295,  1.1392,\n",
      "         -0.5998,  0.5028,  0.2326, -0.8230,  0.1359,  1.0239,  0.0796,  1.7281,\n",
      "         -0.9715,  3.2970],\n",
      "        [-2.2221, -0.2009, -1.0398, -1.1752,  0.8228, -2.0448,  1.8648,  0.7538,\n",
      "          0.3441,  0.4585, -1.0699,  1.3912, -0.0954,  1.3884,  1.4628,  0.6296,\n",
      "          0.0202,  1.8222,  0.1539,  2.1274,  0.3083,  0.2892, -0.2545,  0.4977,\n",
      "         -0.2624,  2.6135,  0.2163,  1.0941,  0.1514,  1.2833, -0.4731,  0.3855,\n",
      "          0.1642,  1.4909, -0.0873,  0.8930,  0.7759,  1.0888, -0.3773,  0.6037,\n",
      "         -0.7284,  0.5633,  0.1422,  1.5802,  0.7967,  1.7724,  0.9344,  1.6888,\n",
      "         -0.7256,  0.2840],\n",
      "        [-0.4347,  0.2867,  0.3042,  0.3112,  0.4681, -0.9468,  1.7610, -1.9792,\n",
      "          2.6603,  1.3913,  0.0660,  0.7765,  0.8980, -0.4646,  0.8765,  0.8158,\n",
      "         -0.7082,  1.1821, -1.1328,  2.0026,  0.3223,  0.8261, -1.2105,  1.3591,\n",
      "         -0.3786,  1.2126, -0.4074,  0.2720,  0.9225,  1.2378,  2.2644,  0.8947,\n",
      "         -0.8233, -0.6579,  1.5342,  0.2685, -1.4654,  1.2314,  2.3473,  0.5807,\n",
      "          0.5120,  0.4808, -1.2963,  1.0747,  0.4887,  0.5610,  0.5180, -0.1015,\n",
      "         -0.2572, -0.2918],\n",
      "        [-0.5981,  0.4368, -0.1547, -0.6911,  0.1977, -0.8266,  2.2936,  1.0142,\n",
      "          0.4170, -0.3841,  0.6150,  0.7315,  1.1667,  1.3720, -0.3071, -0.0798,\n",
      "         -2.1059,  1.0724,  0.0529, -0.4773, -1.1149,  0.6767,  0.1737,  2.6325,\n",
      "          0.4294,  0.6720, -0.0230,  0.6237, -0.2614,  1.5227, -1.8812,  1.6010,\n",
      "          1.7858,  1.3711, -0.9404, -0.5067, -0.6814,  0.5250,  1.5283,  1.6585,\n",
      "         -0.0166,  0.7480, -0.3403,  1.1477,  1.2523,  2.3007,  0.2365,  0.4864,\n",
      "          1.3522,  0.9576],\n",
      "        [ 2.1113,  1.0130, -0.3892,  0.1755,  2.4907, -1.8332,  0.7300,  1.2831,\n",
      "         -0.0508,  1.2167, -1.1899,  0.3476, -2.4913,  0.9273, -0.3415,  1.2559,\n",
      "          0.6144, -0.8478, -1.0668, -0.0690, -0.7861, -0.6521, -0.2959,  0.7381,\n",
      "         -1.9490,  1.3761, -1.7165,  1.8933, -0.9798,  1.0117, -1.0486, -0.2968,\n",
      "         -1.8214,  1.4279,  1.8117,  2.2109,  0.9163,  1.5883,  0.2293,  0.5427,\n",
      "          0.0895,  1.5720,  0.9385,  2.0892, -0.1686,  0.3750, -0.2423,  2.6821,\n",
      "         -0.4521,  0.9086],\n",
      "        [ 0.1260, -0.4664,  0.4606, -0.4276,  0.9188, -1.6377,  0.4150, -0.9002,\n",
      "          0.3507,  1.7637,  0.6343,  0.3021,  2.6582,  2.2576, -0.8760,  3.1317,\n",
      "          0.9288,  1.6526,  0.9066,  1.7457,  0.1666,  1.7792,  1.4503,  1.9717,\n",
      "          1.5013,  1.1269,  0.6625,  1.0442,  0.6446,  0.4086,  1.0795,  1.5404,\n",
      "          0.2344,  2.0709,  0.4343,  1.6960, -0.2616,  0.3555,  0.3897,  1.1993,\n",
      "         -0.3880,  2.0592, -0.3938,  2.2064,  0.7488,  2.7113, -0.4503,  0.3295,\n",
      "         -0.5011,  0.5685],\n",
      "        [-1.2513, -0.0355,  1.4597,  0.4233,  1.1118, -2.6059,  0.4450, -0.2454,\n",
      "         -0.3285,  0.7997, -0.3707,  0.7437, -0.5146,  0.9652,  0.3189, -0.5210,\n",
      "          1.4808, -1.0164,  0.1139, -0.6932, -0.3407,  1.0469, -0.8396,  1.2065,\n",
      "          0.4080,  1.3981, -0.5618,  3.0615,  0.5228,  1.3030, -0.1814,  0.9764,\n",
      "          0.0581,  0.7875,  0.0545,  1.6060, -0.2138,  1.2471, -0.2285,  1.9865,\n",
      "          1.7967,  1.2941, -0.3544,  1.5246,  0.7302,  2.1265,  2.0034,  2.1290,\n",
      "          0.2733,  1.2082],\n",
      "        [ 0.2981,  0.3342,  0.7471,  0.0417, -2.8309, -0.1138,  0.3831, -0.9181,\n",
      "          0.2828, -0.1567,  0.5373,  1.5276, -1.4710,  0.7519, -0.7992,  2.7621,\n",
      "         -0.2883,  2.3354, -0.1257,  1.5512,  1.8394,  2.9146,  0.7868,  1.3138,\n",
      "         -0.1799,  0.4887,  0.2622,  2.2862,  0.5313,  2.6829, -0.4947,  0.1086,\n",
      "          0.9563,  0.1698, -0.7502,  1.2825, -0.6510,  2.6210, -0.9592,  0.9688,\n",
      "          1.9400, -0.1218, -0.4206,  0.9248,  0.3611,  1.3228, -0.0135,  1.7364,\n",
      "          1.8094,  0.3189],\n",
      "        [-1.7144,  1.5825,  1.2027, -0.4613,  0.7804, -2.2954,  0.3818, -0.4549,\n",
      "         -0.2626,  0.7180, -0.3250,  0.7229, -0.4911,  0.9603,  0.3303, -0.5221,\n",
      "          1.4863, -1.0167,  0.1166, -0.6933, -0.3394,  1.0468, -0.8390,  1.2065,\n",
      "          0.4083,  1.3981, -0.5616,  3.0615,  0.5229,  1.3030, -0.1814,  0.9764,\n",
      "          0.0581,  0.7875,  0.0545,  1.6060, -0.2138,  1.2471, -0.2285,  1.9865,\n",
      "          1.7967,  1.2941, -0.3544,  1.5246,  0.7302,  2.1265,  2.0034,  2.1290,\n",
      "          0.2733,  1.2082],\n",
      "        [-0.1346,  1.1060, -0.0662, -0.0939, -1.5586, -2.0471,  1.7191,  0.0313,\n",
      "          1.2339,  1.2938, -0.2325,  1.8607,  0.1922,  1.8898,  0.0558,  2.5286,\n",
      "         -1.1647,  1.4459, -0.2135, -1.1661, -0.0359, -0.8345,  1.1180,  2.1342,\n",
      "         -0.8701,  1.3452,  1.1891,  1.2206, -1.0571,  1.8120,  0.3245,  2.2103,\n",
      "         -0.9609,  0.4261,  1.9693,  1.5815, -1.3079, -0.4763, -1.3945, -0.1418,\n",
      "         -0.5008,  2.3450,  1.6255,  1.5638, -0.1756,  0.7115,  1.2945,  1.0915,\n",
      "          1.4145,  2.1319]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the encoded data to verify\n",
    "print(\"Shape of encoded data:\", encoded_data.shape)\n",
    "\n",
    "# Print the first encoded sequence for verification\n",
    "print(\"First encoded sequence:\", encoded_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9242bf15",
   "metadata": {},
   "source": [
    "## Masking and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99c5ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, embedding_dimension, head_dimension):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.head_dimension = head_dimension\n",
    "        \n",
    "        self.query_layer = torch.nn.Linear(self.embedding_dimension, self.head_dimension)\n",
    "        self.key_layer = torch.nn.Linear(self.embedding_dimension, self.head_dimension)\n",
    "        self.value_layer = torch.nn.Linear(self.embedding_dimension, self.head_dimension)\n",
    "        self.softmax = torch.nn.Softmax(dim = -1)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # x dim - (batch_size, sequence_length, embedding_dim)\n",
    "        # mask dim - (batch_size, sequence_length, head_dim)\n",
    "        # output dim - (batch_size, sequence_length)\n",
    "        \n",
    "        query = self.query_layer(x)\n",
    "        key = self.key_layer(x)\n",
    "        value = self.value_layer(x)\n",
    "        \n",
    "        # calculating attention weights and scaling\n",
    "        attention_weights = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(self.head_dimension)\n",
    "        \n",
    "        # masking\n",
    "        if mask is not None:\n",
    "            attention_weights = attention_weights.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_scores = self.softmax(attention_weights)\n",
    "        return torch.matmul(attention_scores, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6936347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadedSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, embedding_dimension, num_heads):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.head_dimension = embedding_dimension // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.self_attentions = torch.nn.ModuleList(\n",
    "            [MaskedSelfAttention(embedding_dimension, self.head_dimension) for _ in range(self.num_heads)]\n",
    "        )\n",
    "        \n",
    "        self.output_layer = torch.nn.Linear(self.num_heads * self.head_dimension, self.embedding_dimension)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        self_attention_outputs = [self_attention(x, mask) for self_attention in self.self_attentions]\n",
    "        \n",
    "        # concatenating outputs\n",
    "        concatenated_outputs = torch.cat(self_attention_outputs, dim = 2)\n",
    "        return self.output_layer(concatenated_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e8df0d",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35723d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, feed_forward_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.multi_attention = MaskedMultiHeadedSelfAttention(embedding_dim, num_heads)\n",
    "        self.feed_forward = FeedForward(embedding_dim, feed_forward_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm_2 = torch.nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x_norm = self.layer_norm_1(x)\n",
    "        attention_output = self.multi_attention(x_norm, mask)\n",
    "        residual_output = x + self.dropout(attention_output)\n",
    "        \n",
    "        # feedforward block\n",
    "        residual_output_norm = self.layer_norm_2(residual_output)\n",
    "        feed_forward_output = self.feed_forward(residual_output_norm)\n",
    "        \n",
    "        if self.training:\n",
    "            feed_forward_output = self.dropout(feed_forward_output)\n",
    "            \n",
    "        return residual_output + feed_forward_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1707d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, num_layers, num_heads, feed_forward_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.decoder_layers = torch.nn.ModuleList(\n",
    "            [DecoderLayer(embedding_dim, num_heads, feed_forward_dim, dropout_rate) for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        outputs = x\n",
    "        for layer in self.decoder_layers:\n",
    "            outputs = layer(outputs, mask)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "547725c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, feed_forward_dim):\n",
    "        super().__init__()\n",
    "        self.linear_1 = torch.nn.Linear(embedding_dim, feed_forward_dim)\n",
    "        self.linear_2 = torch.nn.Linear(feed_forward_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13da614c",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09766e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(torch.nn.Module):\n",
    "    def __init__(self, num_tokens, max_sequence_length = 100, embedding_dim = 512, num_layers = 6, num_heads = 4, feed_forward_dim = None, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        if feed_forward_dim is None:\n",
    "            self.feed_forward_dim = embedding_dim * 4\n",
    "        else:\n",
    "            self.feed_forward_dim = feed_forward_dim\n",
    "        \n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.token_embedding = TokenEmbedding(embedding_dim, num_tokens)\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim, max_sequence_length)\n",
    "        self.layer_norm = torch.nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        self.decoder = DecoderStack(embedding_dim, num_layers, num_heads, self.feed_forward_dim, dropout_rate)\n",
    "        self.generator_head = GeneratorHead(embedding_dim, num_tokens)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        token_embedding = self.token_embedding(x)\n",
    "        positional_encoding = self.positional_encoding(token_embedding)\n",
    "        positional_encoding_norm = self.layer_norm(positional_encoding)\n",
    "        decoder_outputs = self.decoder(positional_encoding_norm, mask)\n",
    "        generator_outputs = self.generator_head(decoder_outputs)\n",
    "        \n",
    "        return generator_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8526c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorHead(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, num_tokens):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(embedding_dim, num_tokens)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd66a493",
   "metadata": {},
   "source": [
    "## Autoregressive Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e046b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        inputs, targets = x[:, :-1], x[:, 1:]\n",
    "        mask = mask[:, :-1]\n",
    "        \n",
    "        output = self.model(inputs, mask)\n",
    "        return output, targets\n",
    "    \n",
    "    def next_token_probabilities(self, x, mask, temperature = 1.0):\n",
    "        logits = self.model(x, mask)[:, -1, :]\n",
    "        \n",
    "        if temperature != 1.0:\n",
    "            logits /= temperature\n",
    "        \n",
    "        probabilities = torch.softmax(logits, dim = -1)\n",
    "        \n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafb117",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "616a9277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, tokenizer: Tokenizer, optimizer = None):\n",
    "        self.model = model\n",
    "        \n",
    "        if optimizer is None:\n",
    "            self.optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def train(self, data, epochs, batch_size):\n",
    "        loss_epoch = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            random.shuffle(data)\n",
    "            \n",
    "            batches = []\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                sequence = torch.tensor(data[i: i + batch_size], dtype = torch.long)\n",
    "                mask_tensor = torch.ones_like(sequence)\n",
    "                mask_tensor[sequence == self.tokenizer.character_to_token('<pad>')] = 0\n",
    "                \n",
    "                batches.append((sequence, mask_tensor))\n",
    "                \n",
    "            for batch in batches:\n",
    "                self.model.train()\n",
    "                \n",
    "                input_tensor = torch.zeros((batch_size, self.model.model.max_sequence_length + 1), dtype = torch.long)\n",
    "                mask_tensor = torch.zeros((batch_size, self.model.model.max_sequence_length + 1), dtype = torch.long)\n",
    "                \n",
    "                for i, inp in enumerate(batch[0]):\n",
    "                    input_tensor[i, :len(inp)] = inp\n",
    "                \n",
    "                for i, mask in enumerate(batch[1]):\n",
    "                    mask_tensor[i, :len(mask)] = mask\n",
    "                    \n",
    "                model_output, target = self.model(input_tensor, mask_tensor)\n",
    "                \n",
    "                loss = self.loss_function(model_output.transpose(1, 2), target)\n",
    "                loss.backward()\n",
    "                \n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                losses.append(loss.item())\n",
    "                \n",
    "            epoch_loss = np.average(losses)\n",
    "            loss_epoch.append(epoch_loss)\n",
    "            print(f\"Epoch: {epoch}, Loss: {epoch_loss}\")\n",
    "        \n",
    "        return loss_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f6ad2",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de4a4ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def pad_left(self, sequence, final_length, padding_token):\n",
    "        return [padding_token] * (final_length - len(sequence)) + sequence\n",
    "        \n",
    "    def generate(self, max_tokens, prompt = None, temperature = 1.0, eos_token = None, padding_token = 0):\n",
    "        self.model.eval()\n",
    "        \n",
    "        if prompt is None:\n",
    "            start_tokens = [padding_token]\n",
    "        else:\n",
    "            start_tokens = self.tokenizer.tokenize(prompt)\n",
    "            \n",
    "        input_tensor = torch.tensor(\n",
    "            self.pad_left(start_tokens, self.model.max_sequence_length, padding_token), dtype = torch.long\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "\n",
    "        for _ in range(max_tokens):\n",
    "            x = input_tensor[:, -self.model.max_sequence_length:]\n",
    "            \n",
    "            mask = torch.ones_like(x)\n",
    "            mask[x == padding_token] = 0\n",
    "            \n",
    "            next_token_prob = self.model.next_token_probabilities(x = x, temperature = temperature, mask = mask)\n",
    "            \n",
    "            next_token = torch.multinomial(next_token_prob, num_samples = 1)\n",
    "            \n",
    "            input_tensor = torch.cat([input_tensor, next_token.unsqueeze(0)], dim = 1)\n",
    "            \n",
    "            if eos_token is not None and next_token.item() == eos_token:\n",
    "                break\n",
    "        \n",
    "        generated_tokens = input_tensor[0].tolist()\n",
    "        return ''.join([self.tokenizer.token_to_character(token) for token in generated_tokens if token != padding_token])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9d8ca",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2b55e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_sequences(max_sequence_length, tokenized_data):\n",
    "    sequences = []\n",
    "    for i in range(0, len(tokenized_data) - max_sequence_length):\n",
    "        sequences.append(tokenized_data[i: i + max_sequence_length + 1])\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b4010b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data):\n",
    "    tokenized_data = tokenizer.tokenize(training_data)\n",
    "    padded_data = [tokenizer.character_to_token('<pad>')] * max_sequence_length + tokenized_data\n",
    "    \n",
    "    return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0972c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Run(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim = 256, max_sequence_length = 50):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        \n",
    "    def run(self, prompt):\n",
    "        tokenizer = Tokenizer()\n",
    "        num_tokens = tokenizer.size()\n",
    "        \n",
    "        model = AutoregressiveWrapper(TextGenerator(\n",
    "            embedding_dim = self.embedding_dim,\n",
    "            num_tokens = num_tokens,\n",
    "            num_heads = 4,\n",
    "            num_layers = 3, \n",
    "            dropout_rate = 0.1,\n",
    "            max_sequence_length = self.max_sequence_length\n",
    "        ))\n",
    "        \n",
    "        training_data = pd.read_csv('training_data.csv')['text'].to_numpy()\n",
    "        training_data = '. '.join(training_data)\n",
    "        \n",
    "        tokenized_and_padded_training_data = tokenize_and_pad_training_data(self.max_sequence_length, tokenizer, training_data)\n",
    "        sequences = create_training_sequences(self.max_sequence_length, tokenized_and_padded_training_data)\n",
    "        \n",
    "        # training\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = model.to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "        trainer = Trainer(model, tokenizer, optimizer)\n",
    "        loss_per_epoch = trainer.train(sequences, epochs = 100, batch_size = 16)\n",
    "        \n",
    "        # Plot the loss per epoch in log scale\n",
    "        plt.plot(loss_per_epoch)\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.show()\n",
    "        \n",
    "        # generate text\n",
    "        max_tokens = 400\n",
    "        generator = Generator(model, tokenizer)\n",
    "        generated_text = generator.generate(\n",
    "            max_tokens = max_tokens, prompt = prompt, padding_token = tokenizer.character_to_token('<pad>')\n",
    "        )\n",
    "        \n",
    "        print(generated_text.replace('<pad>', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcb6390a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranav/opt/miniconda3/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.19) or chardet (5.2.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "/Users/pranav/opt/miniconda3/lib/python3.9/site-packages/transformers/utils/generic.py:481: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (16) must match the size of tensor b (50) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m runner \u001b[38;5;241m=\u001b[39m Run()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPhoto by\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 32\u001b[0m, in \u001b[0;36mRun.run\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m     30\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     31\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, tokenizer, optimizer)\n\u001b[0;32m---> 32\u001b[0m loss_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Plot the loss per epoch in log scale\u001b[39;00m\n\u001b[1;32m     35\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(loss_per_epoch)\n",
      "Cell \u001b[0;32mIn[27], line 40\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, data, epochs, batch_size)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m     38\u001b[0m     mask_tensor[i, :\u001b[38;5;28mlen\u001b[39m(mask)] \u001b[38;5;241m=\u001b[39m mask\n\u001b[0;32m---> 40\u001b[0m model_output, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(model_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), target)\n\u001b[1;32m     43\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[26], line 10\u001b[0m, in \u001b[0;36mAutoregressiveWrapper.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m      7\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m x[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], x[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m      8\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, targets\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 28\u001b[0m, in \u001b[0;36mTextGenerator.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     26\u001b[0m positional_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(token_embedding)\n\u001b[1;32m     27\u001b[0m positional_encoding_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(positional_encoding)\n\u001b[0;32m---> 28\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositional_encoding_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m generator_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator_head(decoder_outputs)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generator_outputs\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 12\u001b[0m, in \u001b[0;36mDecoderStack.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers:\n\u001b[0;32m---> 12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 14\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[1;32m     13\u001b[0m     x_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm_1(x)\n\u001b[0;32m---> 14\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     residual_output \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# feedforward block\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m, in \u001b[0;36mMaskedMultiHeadedSelfAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m---> 15\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m [self_attention(x, mask) \u001b[38;5;28;01mfor\u001b[39;00m self_attention \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attentions]\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# concatenating outputs\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     concatenated_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(self_attention_outputs, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m---> 15\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m [\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m self_attention \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attentions]\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# concatenating outputs\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     concatenated_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(self_attention_outputs, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m, in \u001b[0;36mMaskedSelfAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# masking\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     attention_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattention_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-inf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(attention_weights)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmatmul(attention_scores, value)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (16) must match the size of tensor b (50) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "runner = Run()\n",
    "runner.run(prompt = \"Photo by\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92239ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff60f9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2f715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5fd2de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
