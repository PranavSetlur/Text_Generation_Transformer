{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a2e19d",
   "metadata": {},
   "source": [
    "# Text Generator\n",
    "Implementing a text generation model from scratch using a transformer (decoder only).\\\n",
    "Steps:\n",
    "1. Tokenization\n",
    "2. Input embedding\n",
    "3. Positional encoding\n",
    "4. Masking\n",
    "5. Self-attention\n",
    "6. Decoder stack\n",
    "7. Predicting token probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b58e15",
   "metadata": {},
   "source": [
    "## Creating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b1cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install pytorch torchvision torchaudio -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa7f1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6424458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class creating_data():\n",
    "    def __init__(self, filepath):\n",
    "        self.df = pd.read_csv(filepath)\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.df.to_csv(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea755bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = creating_data('medium_articles.csv')\n",
    "# dataset.save('training_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46dee6c",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5068fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        self.dictionary = {}\n",
    "        self.reverse_dictionary = {}\n",
    "        \n",
    "        # adding special tokens\n",
    "        self.__add_to_dict('<pad>')\n",
    "        self.__add_to_dict('<unk>')\n",
    "        \n",
    "        # add characters and numbers to dictionary\n",
    "        for i in range(10):\n",
    "            self.__add_to_dict(str(i))\n",
    "        \n",
    "        for i in range(26):\n",
    "            self.__add_to_dict(chr(ord('a') + i))\n",
    "            self.__add_to_dict(chr(ord('A') + i))\n",
    "            \n",
    "        # adding space and punctuation\n",
    "        for char in ['.', ' ', ',', '!', '?', '\\n']:\n",
    "            self.__add_to_dict(char)\n",
    "        \n",
    "    def __add_to_dict(self, character):\n",
    "        if character not in self.dictionary:\n",
    "            index = self.size()\n",
    "            self.dictionary[character] = index\n",
    "            self.reverse_dictionary[index] = character\n",
    "            \n",
    "    def tokenize(self, text):\n",
    "        return [self.dictionary.get(c, self.dictionary['<unk>']) for c in text]\n",
    "    \n",
    "    def character_to_token(self, character):\n",
    "        return self.dictionary.get(character, self.dictionary['<unk>'])\n",
    "    \n",
    "    def token_to_character(self, token):\n",
    "        return self.reverse_dictionary.get(token, '<unk>')\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e166c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('training_data.csv')\n",
    "training_data = training_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75dbb597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...\n",
       "1    Your Brain On Coronavirus\\n\\nA guide to the cu...\n",
       "2    Mind Your Nose\\n\\nHow smell training can chang...\n",
       "3    Passionate about the synergy between science a...\n",
       "4    You’ve heard of him, haven’t you? Phineas Gage...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4187758",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9938290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenized_data = [tokenizer.tokenize(sentence) for sentence in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c4a2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 20\n",
    "# padding and truncating\n",
    "padded_data = []\n",
    "\n",
    "for tokens in tokenized_data:\n",
    "    if len(tokens) < max_sequence_length:\n",
    "        # padding\n",
    "        tokens = [tokenizer.character_to_token('<pad>')] * (max_sequence_length - len(tokens)) + tokens\n",
    "    else:\n",
    "        # truncating\n",
    "        tokens = tokens[:max_sequence_length]\n",
    "    padded_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8be394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting data to tensors\n",
    "tensor_data = [torch.tensor(tokens) for tokens in padded_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d9d4c",
   "metadata": {},
   "source": [
    "## Input Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "113d5547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(torch.nn.Module):\n",
    "    # model that converts tokens into embeddings\n",
    "    \n",
    "    def __init__(self, model_dim, num_tokens):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = torch.nn.Embedding(\n",
    "            num_embeddings = num_tokens,\n",
    "            embedding_dim = model_dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58fb2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim = 50\n",
    "num_tokens = tokenizer.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eec917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing class\n",
    "embedding_model = TokenEmbedding(model_dim, num_tokens)\n",
    "# convert padded data to tensor\n",
    "tensor_data = torch.stack(tensor_data)\n",
    "embedded_data = embedding_model(tensor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f9a3f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedded data: torch.Size([100, 20, 50])\n",
      "First embedded sequence: tensor([[ 8.6516e-01, -5.6720e-01,  9.2488e-01,  1.1499e+00, -1.2702e-01,\n",
      "          1.4284e+00, -8.0461e-01,  9.5601e-01, -6.8222e-01, -1.3532e+00,\n",
      "         -9.0449e-01,  7.5888e-01,  2.1901e-01, -4.0781e-01, -8.5586e-01,\n",
      "         -3.0554e-01, -1.4685e+00,  2.2667e-01, -2.9533e+00,  1.9087e-01,\n",
      "          5.5294e-01, -6.5860e-01, -4.2717e-01, -1.2479e+00, -2.3035e-02,\n",
      "          1.0003e+00,  8.9760e-01,  9.1178e-01,  5.9913e-01, -1.1346e+00,\n",
      "         -6.2972e-01, -1.7169e+00,  8.5098e-01, -7.2986e-01, -1.1288e+00,\n",
      "         -7.2264e-01, -2.3108e-01,  1.9338e+00,  7.4508e-01,  9.6185e-01,\n",
      "         -8.4514e-01, -1.3891e+00,  1.4332e+00,  1.0020e+00,  8.7406e-02,\n",
      "         -1.1720e+00,  8.4527e-01, -7.3078e-01,  6.2227e-01, -1.0165e+00],\n",
      "        [-1.6051e+00,  7.6282e-01, -2.2812e-01,  1.2762e+00,  6.5328e-01,\n",
      "         -9.1080e-01,  5.9510e-01, -9.1974e-01,  1.7214e-01, -6.2642e-01,\n",
      "          6.7986e-01, -7.8205e-01, -4.9208e-01, -5.4209e-01, -1.0711e-01,\n",
      "          2.8998e-01, -3.4927e-01,  4.4344e-01, -3.4718e-01, -4.2333e-01,\n",
      "         -1.2281e+00,  1.0567e+00,  1.9771e-01,  7.5426e-01, -5.6829e-01,\n",
      "         -1.0391e+00,  1.3507e+00, -1.2937e+00, -5.8514e-01,  1.8275e+00,\n",
      "          2.8792e-03, -7.4050e-02,  3.0822e-01,  2.7162e-01, -2.2783e+00,\n",
      "         -1.8526e-01, -1.2836e+00, -2.7538e-01,  1.0068e+00,  2.4730e-02,\n",
      "          1.3135e+00, -3.4562e-01,  2.3337e+00, -1.0174e+00,  2.0681e+00,\n",
      "         -7.1479e-01,  8.4612e-02, -1.3251e+00,  8.2808e-01,  2.0604e+00],\n",
      "        [ 5.6826e-01,  1.1349e+00, -2.8509e-01, -2.6584e-01, -6.3124e-01,\n",
      "          9.2960e-01, -3.6726e-01, -1.6828e+00,  1.8007e+00, -3.0609e-01,\n",
      "         -1.0411e+00, -5.5851e-01, -1.1760e+00, -1.9713e+00, -6.8535e-01,\n",
      "          5.3782e-01, -2.1194e+00,  4.9945e-01, -2.0479e-01, -1.5355e+00,\n",
      "          1.3400e+00, -7.5982e-01,  1.5597e+00,  2.0557e+00,  8.1821e-01,\n",
      "          1.5980e-01, -5.9634e-01, -1.1397e+00,  1.6467e+00, -5.9910e-01,\n",
      "         -9.6028e-02, -1.5857e-01,  4.2118e-01, -8.2390e-01, -1.6927e+00,\n",
      "          3.1746e-01,  7.1794e-02,  2.9918e+00,  1.3464e-01, -8.3865e-01,\n",
      "         -1.0220e+00, -6.3016e-01,  1.5732e+00, -9.9186e-01,  5.1007e-01,\n",
      "         -1.3098e+00,  7.3718e-01, -2.5227e-01, -2.3858e-01,  6.7088e-01],\n",
      "        [ 7.2855e-01,  1.0684e+00, -1.8620e-01, -1.1843e+00,  1.0646e+00,\n",
      "         -8.2779e-01,  7.7797e-01, -1.8589e-01,  1.8398e+00, -4.3276e-01,\n",
      "         -4.9018e-01,  3.5716e-01, -1.1047e-01,  3.9297e-01,  3.8588e-01,\n",
      "         -3.3006e-01,  1.7482e-01,  6.3624e-01, -8.8620e-01, -1.8083e-02,\n",
      "          1.2303e-01, -6.5018e-02,  2.0106e-01,  1.6713e-01,  9.3336e-01,\n",
      "         -3.9233e-01, -7.3318e-01,  4.0835e-01, -9.9756e-01, -4.8485e-01,\n",
      "          6.3532e-01, -2.0157e+00,  4.0761e-01, -9.8093e-01, -1.4911e-01,\n",
      "         -2.6360e-01,  1.0524e+00,  6.0424e-01, -3.5989e-01,  7.7440e-01,\n",
      "          1.9257e+00, -1.1932e+00,  6.2407e-01,  1.9130e+00, -6.5726e-01,\n",
      "         -1.5328e+00, -1.4773e-01,  1.9778e+00,  1.1738e+00, -2.7072e-01],\n",
      "        [ 5.6826e-01,  1.1349e+00, -2.8509e-01, -2.6584e-01, -6.3124e-01,\n",
      "          9.2960e-01, -3.6726e-01, -1.6828e+00,  1.8007e+00, -3.0609e-01,\n",
      "         -1.0411e+00, -5.5851e-01, -1.1760e+00, -1.9713e+00, -6.8535e-01,\n",
      "          5.3782e-01, -2.1194e+00,  4.9945e-01, -2.0479e-01, -1.5355e+00,\n",
      "          1.3400e+00, -7.5982e-01,  1.5597e+00,  2.0557e+00,  8.1821e-01,\n",
      "          1.5980e-01, -5.9634e-01, -1.1397e+00,  1.6467e+00, -5.9910e-01,\n",
      "         -9.6028e-02, -1.5857e-01,  4.2118e-01, -8.2390e-01, -1.6927e+00,\n",
      "          3.1746e-01,  7.1794e-02,  2.9918e+00,  1.3464e-01, -8.3865e-01,\n",
      "         -1.0220e+00, -6.3016e-01,  1.5732e+00, -9.9186e-01,  5.1007e-01,\n",
      "         -1.3098e+00,  7.3718e-01, -2.5227e-01, -2.3858e-01,  6.7088e-01],\n",
      "        [-5.6991e-01,  1.1538e-01, -4.4519e-01, -4.4735e-01, -7.6710e-01,\n",
      "         -5.3671e-01,  5.3784e-01, -1.2294e+00, -1.0735e+00,  4.5121e-01,\n",
      "          4.8743e-01, -5.7356e-01,  5.7636e-01, -5.2152e-01,  8.2316e-03,\n",
      "          1.4992e+00,  1.6853e+00,  9.9660e-01,  3.2339e-01,  6.9108e-01,\n",
      "         -5.1888e-01, -7.6187e-01, -3.8272e-01,  3.1251e-01,  1.6650e+00,\n",
      "         -2.2711e+00, -8.1025e-01, -7.1784e-01,  2.3330e-01, -2.1491e+00,\n",
      "         -3.4327e-01,  3.0074e-01, -1.1611e+00,  1.0710e+00, -2.0395e-01,\n",
      "          5.4434e-01,  4.9149e-01, -8.3340e-02,  1.7233e-01,  5.8523e-01,\n",
      "         -1.0079e+00,  1.4691e+00, -2.7650e-01, -7.7308e-01,  1.5958e+00,\n",
      "          2.2252e-01,  5.0752e-01,  5.5119e-01, -1.2106e+00,  1.0187e+00],\n",
      "        [-7.8045e-01,  8.7343e-02,  1.2182e-01, -2.8411e-01,  3.3220e-01,\n",
      "         -6.6413e-01,  5.6812e-01, -7.6353e-02,  9.6398e-01,  1.3828e+00,\n",
      "         -2.9539e-01, -7.8922e-01,  1.1259e+00,  2.5111e-01,  1.8997e+00,\n",
      "          9.6386e-01, -6.7690e-01,  6.8402e-01,  1.8386e+00,  1.3060e+00,\n",
      "          3.6101e-02, -1.0455e+00, -7.0247e-01,  5.6283e-01, -4.1731e-01,\n",
      "          1.3152e-01, -6.9269e-01,  2.0623e-01, -9.3808e-01,  2.1763e+00,\n",
      "          5.2089e-01,  1.2021e+00, -1.7834e+00,  1.5003e+00, -8.4860e-01,\n",
      "          6.2684e-01,  4.2255e-01, -1.3162e+00, -6.2707e-02, -3.0404e-01,\n",
      "          8.8112e-01,  7.5982e-01, -6.4015e-01, -5.0544e-01,  1.0583e+00,\n",
      "          1.3592e-01,  1.5141e-01, -3.2949e-01, -1.0521e+00,  4.9047e-02],\n",
      "        [-1.0990e+00, -4.3511e-01,  2.0489e-01, -1.1017e+00,  1.9071e+00,\n",
      "         -2.8828e-01, -5.1940e-01, -2.2696e+00,  1.0077e-01,  5.3826e-01,\n",
      "         -4.1752e-01, -7.5507e-01,  4.0597e-01, -6.7475e-01, -1.8973e-01,\n",
      "          1.4078e-02,  7.8845e-03, -7.3471e-01,  1.0507e-01,  1.3982e-01,\n",
      "          2.5303e+00, -9.2299e-01, -3.5859e-03,  1.1950e-01,  1.8477e+00,\n",
      "          4.7893e-01,  3.9789e-01, -1.4614e-01,  3.0559e-02, -1.0346e+00,\n",
      "          2.1912e+00, -4.8667e-01,  6.8904e-01,  1.4688e+00,  1.0424e+00,\n",
      "         -1.0863e+00,  1.9239e-01,  1.8040e+00, -1.8494e+00, -1.0548e+00,\n",
      "         -5.1268e-01, -3.3355e-01, -6.8291e-01,  2.3395e+00, -1.7271e-01,\n",
      "          9.2552e-01,  2.9341e-02, -2.3238e+00, -4.2346e-01,  1.1450e+00],\n",
      "        [-5.6991e-01,  1.1538e-01, -4.4519e-01, -4.4735e-01, -7.6710e-01,\n",
      "         -5.3671e-01,  5.3784e-01, -1.2294e+00, -1.0735e+00,  4.5121e-01,\n",
      "          4.8743e-01, -5.7356e-01,  5.7636e-01, -5.2152e-01,  8.2316e-03,\n",
      "          1.4992e+00,  1.6853e+00,  9.9660e-01,  3.2339e-01,  6.9108e-01,\n",
      "         -5.1888e-01, -7.6187e-01, -3.8272e-01,  3.1251e-01,  1.6650e+00,\n",
      "         -2.2711e+00, -8.1025e-01, -7.1784e-01,  2.3330e-01, -2.1491e+00,\n",
      "         -3.4327e-01,  3.0074e-01, -1.1611e+00,  1.0710e+00, -2.0395e-01,\n",
      "          5.4434e-01,  4.9149e-01, -8.3340e-02,  1.7233e-01,  5.8523e-01,\n",
      "         -1.0079e+00,  1.4691e+00, -2.7650e-01, -7.7308e-01,  1.5958e+00,\n",
      "          2.2252e-01,  5.0752e-01,  5.5119e-01, -1.2106e+00,  1.0187e+00],\n",
      "        [ 2.3312e-03, -1.0956e+00,  1.3160e+00, -5.7355e-01,  9.9703e-01,\n",
      "         -2.8624e-01, -2.7217e-02, -1.0346e+00,  1.3617e+00,  1.0021e+00,\n",
      "          6.3016e-01,  4.8221e-02, -2.1396e+00,  8.1849e-01, -1.4643e+00,\n",
      "         -3.7685e-01, -5.4309e-01, -7.8238e-01, -2.1906e+00, -5.9222e-01,\n",
      "          3.6709e-01, -3.0066e-01,  1.2781e+00,  7.4085e-01,  1.0143e+00,\n",
      "         -4.7751e-02, -1.3218e+00, -3.6217e-02,  1.3276e+00,  1.8996e-01,\n",
      "         -5.9230e-01,  4.0260e-01,  7.1950e-01,  9.7869e-01, -1.9964e-01,\n",
      "          2.0184e+00, -4.1809e-01, -6.6348e-01,  6.4842e-01, -4.3621e-01,\n",
      "         -1.0300e+00, -3.5875e-01,  2.4389e-01, -6.2460e-01, -1.7975e+00,\n",
      "          1.2195e-01, -5.7757e-02, -4.2627e-01, -6.3424e-01, -7.7944e-01],\n",
      "        [ 5.6826e-01,  1.1349e+00, -2.8509e-01, -2.6584e-01, -6.3124e-01,\n",
      "          9.2960e-01, -3.6726e-01, -1.6828e+00,  1.8007e+00, -3.0609e-01,\n",
      "         -1.0411e+00, -5.5851e-01, -1.1760e+00, -1.9713e+00, -6.8535e-01,\n",
      "          5.3782e-01, -2.1194e+00,  4.9945e-01, -2.0479e-01, -1.5355e+00,\n",
      "          1.3400e+00, -7.5982e-01,  1.5597e+00,  2.0557e+00,  8.1821e-01,\n",
      "          1.5980e-01, -5.9634e-01, -1.1397e+00,  1.6467e+00, -5.9910e-01,\n",
      "         -9.6028e-02, -1.5857e-01,  4.2118e-01, -8.2390e-01, -1.6927e+00,\n",
      "          3.1746e-01,  7.1794e-02,  2.9918e+00,  1.3464e-01, -8.3865e-01,\n",
      "         -1.0220e+00, -6.3016e-01,  1.5732e+00, -9.9186e-01,  5.1007e-01,\n",
      "         -1.3098e+00,  7.3718e-01, -2.5227e-01, -2.3858e-01,  6.7088e-01],\n",
      "        [ 1.1219e+00,  1.5663e+00, -4.1574e-01,  2.8822e-01,  1.3592e-02,\n",
      "         -4.8500e-01,  1.3666e-02, -1.3702e+00,  6.8887e-01, -1.0320e-01,\n",
      "          1.0177e+00, -1.5956e+00, -6.1372e-01,  2.5757e-01,  1.0998e-01,\n",
      "         -7.4377e-01,  1.1116e+00, -8.5056e-01, -2.5128e-01,  1.5869e+00,\n",
      "         -8.8055e-01, -5.9244e-01,  6.8204e-01, -1.3835e+00,  1.0762e-01,\n",
      "         -6.7918e-01, -7.5761e-01, -8.8179e-01, -1.0092e-01,  1.3807e+00,\n",
      "         -2.6786e-01, -7.5766e-01, -9.0054e-01,  2.0573e+00, -1.1188e-03,\n",
      "          2.9778e-01,  8.4016e-01,  1.4322e+00, -1.0536e+00,  1.4518e+00,\n",
      "         -4.6248e-01, -9.9503e-01,  1.6155e+00, -6.8076e-01, -3.4623e-02,\n",
      "         -2.0471e+00,  1.4757e+00,  1.0816e+00, -1.2814e+00, -5.1451e-01],\n",
      "        [-1.6051e+00,  7.6282e-01, -2.2812e-01,  1.2762e+00,  6.5328e-01,\n",
      "         -9.1080e-01,  5.9510e-01, -9.1974e-01,  1.7214e-01, -6.2642e-01,\n",
      "          6.7986e-01, -7.8205e-01, -4.9208e-01, -5.4209e-01, -1.0711e-01,\n",
      "          2.8998e-01, -3.4927e-01,  4.4344e-01, -3.4718e-01, -4.2333e-01,\n",
      "         -1.2281e+00,  1.0567e+00,  1.9771e-01,  7.5426e-01, -5.6829e-01,\n",
      "         -1.0391e+00,  1.3507e+00, -1.2937e+00, -5.8514e-01,  1.8275e+00,\n",
      "          2.8792e-03, -7.4050e-02,  3.0822e-01,  2.7162e-01, -2.2783e+00,\n",
      "         -1.8526e-01, -1.2836e+00, -2.7538e-01,  1.0068e+00,  2.4730e-02,\n",
      "          1.3135e+00, -3.4562e-01,  2.3337e+00, -1.0174e+00,  2.0681e+00,\n",
      "         -7.1479e-01,  8.4612e-02, -1.3251e+00,  8.2808e-01,  2.0604e+00],\n",
      "        [-5.6991e-01,  1.1538e-01, -4.4519e-01, -4.4735e-01, -7.6710e-01,\n",
      "         -5.3671e-01,  5.3784e-01, -1.2294e+00, -1.0735e+00,  4.5121e-01,\n",
      "          4.8743e-01, -5.7356e-01,  5.7636e-01, -5.2152e-01,  8.2316e-03,\n",
      "          1.4992e+00,  1.6853e+00,  9.9660e-01,  3.2339e-01,  6.9108e-01,\n",
      "         -5.1888e-01, -7.6187e-01, -3.8272e-01,  3.1251e-01,  1.6650e+00,\n",
      "         -2.2711e+00, -8.1025e-01, -7.1784e-01,  2.3330e-01, -2.1491e+00,\n",
      "         -3.4327e-01,  3.0074e-01, -1.1611e+00,  1.0710e+00, -2.0395e-01,\n",
      "          5.4434e-01,  4.9149e-01, -8.3340e-02,  1.7233e-01,  5.8523e-01,\n",
      "         -1.0079e+00,  1.4691e+00, -2.7650e-01, -7.7308e-01,  1.5958e+00,\n",
      "          2.2252e-01,  5.0752e-01,  5.5119e-01, -1.2106e+00,  1.0187e+00],\n",
      "        [ 9.6976e-01,  1.6660e+00, -6.7817e-01,  5.2231e-01,  4.0322e-02,\n",
      "          2.8419e+00,  1.9296e+00, -1.8961e-01, -1.1226e+00, -3.1447e-01,\n",
      "          1.9123e+00,  3.4447e-01,  1.8879e+00, -1.5360e+00, -1.7350e+00,\n",
      "         -1.6825e+00, -9.0071e-01, -1.7340e+00, -2.4665e-01,  4.6706e-01,\n",
      "         -4.0742e-01, -1.0786e+00, -1.0683e-01, -8.4425e-01, -1.0918e-02,\n",
      "          8.6413e-01,  1.1702e+00, -6.2139e-01, -4.5763e-03, -7.7554e-01,\n",
      "          1.5732e-01,  9.9784e-01,  1.3610e+00, -2.5952e-02, -7.1712e-01,\n",
      "         -4.2309e-01,  1.3712e+00,  4.4893e-01,  1.0840e+00, -2.2482e+00,\n",
      "          3.2915e-01, -3.0660e-01, -6.8910e-01, -1.3630e+00,  3.3701e-01,\n",
      "          3.6796e-01,  8.1111e-01,  1.1653e+00,  1.4722e+00, -5.7682e-01],\n",
      "        [-7.4610e-01, -3.9308e-01,  5.4463e-01,  4.9222e-01,  1.2848e-01,\n",
      "         -1.2826e-01,  1.6343e+00,  1.7239e-01,  8.7238e-01, -8.2145e-01,\n",
      "          7.3307e-02,  9.2453e-01,  4.4986e-01, -1.3836e+00, -5.1300e-01,\n",
      "         -7.3700e-01,  8.2651e-01,  1.2536e+00,  8.3481e-01, -3.0301e-01,\n",
      "         -1.4857e+00, -9.9128e-01,  1.4299e+00, -1.3153e-01, -1.4353e-02,\n",
      "         -1.0969e+00,  3.5003e-01,  2.1441e+00, -1.4879e+00, -1.0588e+00,\n",
      "          3.2540e-01, -9.1378e-02, -2.0582e-01, -8.1860e-01, -1.3428e+00,\n",
      "          3.5834e-01,  1.1192e+00,  1.4271e+00, -1.4452e-01, -8.5755e-01,\n",
      "         -9.1204e-01, -1.2756e+00,  8.2491e-02, -8.5188e-01,  6.9224e-01,\n",
      "         -2.5294e-01,  1.2613e+00,  8.1278e-01, -8.6758e-02, -3.2917e-01],\n",
      "        [ 1.0986e+00,  5.9705e-02,  7.3662e-01, -1.2487e+00, -3.1831e-01,\n",
      "         -3.5543e-01, -1.3724e-01,  2.8183e-01,  1.3410e+00,  1.4996e+00,\n",
      "         -5.6817e-01,  1.2735e+00,  1.5639e+00,  5.3457e-01, -5.4861e-01,\n",
      "         -4.2793e-01, -2.7351e+00,  1.5848e+00,  1.3956e+00, -8.2425e-01,\n",
      "         -5.1144e-01, -1.5897e+00,  1.0257e+00, -9.3883e-01,  1.2087e+00,\n",
      "         -1.6052e+00,  1.3341e-01, -7.7877e-01, -2.2139e+00, -9.6078e-01,\n",
      "         -5.4848e-01,  1.4507e+00,  6.9446e-01,  1.2545e+00, -4.6259e-02,\n",
      "          5.1168e-01,  1.4000e+00,  1.1999e+00, -3.3999e-01,  2.0393e+00,\n",
      "         -3.5586e-02,  5.6649e-01,  4.7338e-01,  1.1205e+00, -1.2956e-01,\n",
      "          9.0164e-01, -1.2610e+00,  5.5394e-01,  3.2752e-01,  8.0401e-01],\n",
      "        [-1.0269e+00,  5.2354e-01,  5.9826e-01, -1.8291e-01, -5.8047e-01,\n",
      "          2.4409e+00, -3.9951e-01,  2.0860e-01,  1.7620e+00,  6.4394e-01,\n",
      "          1.0424e+00, -2.7774e-01,  5.1446e-01,  5.1963e-01,  1.0167e+00,\n",
      "         -1.4220e-01, -4.4887e-01, -2.1698e+00, -1.5219e+00, -9.7183e-01,\n",
      "          5.9694e-01,  2.9384e-01, -3.1353e-01,  1.2063e+00, -6.8090e-01,\n",
      "          2.3698e-01,  1.0537e+00,  1.2086e-02, -8.1589e-01,  1.1815e+00,\n",
      "          1.6416e+00,  9.4358e-01,  9.9324e-01,  5.5508e-01,  2.9664e-01,\n",
      "          5.1186e-01, -1.0709e+00, -2.3295e-01,  1.5260e-01, -1.9739e+00,\n",
      "          4.6783e-01,  3.3555e-01,  6.6399e-01,  1.4476e+00,  2.7613e-01,\n",
      "          8.3045e-01,  2.5674e+00,  4.9188e-01,  1.4925e+00, -1.1007e+00],\n",
      "        [ 1.0986e+00,  5.9705e-02,  7.3662e-01, -1.2487e+00, -3.1831e-01,\n",
      "         -3.5543e-01, -1.3724e-01,  2.8183e-01,  1.3410e+00,  1.4996e+00,\n",
      "         -5.6817e-01,  1.2735e+00,  1.5639e+00,  5.3457e-01, -5.4861e-01,\n",
      "         -4.2793e-01, -2.7351e+00,  1.5848e+00,  1.3956e+00, -8.2425e-01,\n",
      "         -5.1144e-01, -1.5897e+00,  1.0257e+00, -9.3883e-01,  1.2087e+00,\n",
      "         -1.6052e+00,  1.3341e-01, -7.7877e-01, -2.2139e+00, -9.6078e-01,\n",
      "         -5.4848e-01,  1.4507e+00,  6.9446e-01,  1.2545e+00, -4.6259e-02,\n",
      "          5.1168e-01,  1.4000e+00,  1.1999e+00, -3.3999e-01,  2.0393e+00,\n",
      "         -3.5586e-02,  5.6649e-01,  4.7338e-01,  1.1205e+00, -1.2956e-01,\n",
      "          9.0164e-01, -1.2610e+00,  5.5394e-01,  3.2752e-01,  8.0401e-01],\n",
      "        [-4.3605e-03, -1.5402e+00, -1.5411e-01,  2.1297e-02,  1.8608e+00,\n",
      "         -1.5147e-01,  4.6634e-01, -5.2367e-03, -1.6905e-01, -1.9104e+00,\n",
      "          1.1890e-01,  4.9464e-01, -7.4899e-01,  4.9144e-01,  1.4613e+00,\n",
      "         -8.0284e-01,  1.7290e-01, -8.0275e-01, -4.0719e-01, -8.6151e-01,\n",
      "         -3.8548e-01,  1.5111e+00,  1.7888e+00,  1.6971e+00, -7.4476e-01,\n",
      "          1.0892e-02,  1.2655e+00,  2.0815e+00,  1.7087e+00, -1.5277e+00,\n",
      "         -3.0398e-01, -1.1986e-01, -3.6656e-01, -1.0441e+00, -3.5601e-01,\n",
      "          5.4537e-01, -3.2168e-01,  5.1466e-01, -6.2437e-01, -4.1562e-01,\n",
      "         -8.8391e-02,  1.3920e+00, -5.2436e-01,  6.3979e-01, -5.1730e-01,\n",
      "          1.5342e+00, -7.6404e-01, -6.6824e-01,  1.3994e+00, -9.7624e-01]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the embedded data to verify\n",
    "print(\"Shape of embedded data:\", embedded_data.shape)\n",
    "\n",
    "# Print the first embedded sequence for verification\n",
    "print(\"First embedded sequence:\", embedded_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b6c9cc",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f6d2a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, model_dim, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "    \n",
    "        positional_encoding = np.zeros((max_sequence_length, model_dim))\n",
    "        \n",
    "        # calculating encoding for each position and dim\n",
    "        for pos in range(self.max_sequence_length):\n",
    "            for i in range(0, self.model_dim, 2):\n",
    "                # sin to even indices\n",
    "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.model_dim)))\n",
    "                \n",
    "                # cos to odd indices\n",
    "                if i + 1 < self.model_dim:\n",
    "                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / self.model_dim)))\n",
    "                    \n",
    "        \n",
    "        self.positional_encoding = torch.from_numpy(positional_encoding).unsqueeze(0).float()\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.positional_encoding[: x.size(1), :]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "266f7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding = PositionalEncoding(model_dim, max_sequence_length)\n",
    "encoded_data = pos_encoding(embedded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0a20c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoded data: torch.Size([100, 20, 50])\n",
      "First encoded sequence: tensor([[ 8.6516e-01,  4.3280e-01,  9.2488e-01,  2.1499e+00, -1.2702e-01,\n",
      "          2.4284e+00, -8.0461e-01,  1.9560e+00, -6.8222e-01, -3.5317e-01,\n",
      "         -9.0449e-01,  1.7589e+00,  2.1901e-01,  5.9219e-01, -8.5586e-01,\n",
      "          6.9446e-01, -1.4685e+00,  1.2267e+00, -2.9533e+00,  1.1909e+00,\n",
      "          5.5294e-01,  3.4140e-01, -4.2717e-01, -2.4795e-01, -2.3035e-02,\n",
      "          2.0003e+00,  8.9760e-01,  1.9118e+00,  5.9913e-01, -1.3457e-01,\n",
      "         -6.2972e-01, -7.1692e-01,  8.5098e-01,  2.7014e-01, -1.1288e+00,\n",
      "          2.7736e-01, -2.3108e-01,  2.9338e+00,  7.4508e-01,  1.9618e+00,\n",
      "         -8.4514e-01, -3.8906e-01,  1.4332e+00,  2.0020e+00,  8.7406e-02,\n",
      "         -1.7197e-01,  8.4527e-01,  2.6922e-01,  6.2227e-01, -1.6497e-02],\n",
      "        [-7.6358e-01,  1.3031e+00,  2.3244e-01,  2.1638e+00,  8.8037e-01,\n",
      "          6.3078e-02,  7.0453e-01,  7.4259e-02,  2.2459e-01,  3.7221e-01,\n",
      "          7.0498e-01,  2.1763e-01, -4.8005e-01,  4.5784e-01, -1.0135e-01,\n",
      "          1.2900e+00, -3.4652e-01,  1.4434e+00, -3.4586e-01,  5.7667e-01,\n",
      "         -1.2274e+00,  2.0567e+00,  1.9802e-01,  1.7543e+00, -5.6814e-01,\n",
      "         -3.9109e-02,  1.3508e+00, -2.9373e-01, -5.8510e-01,  2.8275e+00,\n",
      "          2.8951e-03,  9.2595e-01,  3.0823e-01,  1.2716e+00, -2.2783e+00,\n",
      "          8.1474e-01, -1.2836e+00,  7.2462e-01,  1.0068e+00,  1.0247e+00,\n",
      "          1.3135e+00,  6.5438e-01,  2.3337e+00, -1.7432e-02,  2.0681e+00,\n",
      "          2.8521e-01,  8.4612e-02, -3.2510e-01,  8.2808e-01,  3.0604e+00],\n",
      "        [ 1.4776e+00,  7.1871e-01,  5.3253e-01,  3.0992e-01, -1.8893e-01,\n",
      "          1.8265e+00, -1.4972e-01, -7.0677e-01,  1.9055e+00,  6.8841e-01,\n",
      "         -9.9089e-01,  4.4023e-01, -1.1520e+00, -9.7156e-01, -6.7384e-01,\n",
      "          1.5378e+00, -2.1139e+00,  1.4994e+00, -2.0215e-01, -5.3546e-01,\n",
      "          1.3412e+00,  2.4018e-01,  1.5603e+00,  3.0557e+00,  8.1850e-01,\n",
      "          1.1598e+00, -5.9620e-01, -1.3967e-01,  1.6468e+00,  4.0090e-01,\n",
      "         -9.5997e-02,  8.4143e-01,  4.2119e-01,  1.7610e-01, -1.6927e+00,\n",
      "          1.3175e+00,  7.1797e-02,  3.9918e+00,  1.3464e-01,  1.6135e-01,\n",
      "         -1.0220e+00,  3.6984e-01,  1.5732e+00,  8.1378e-03,  5.1007e-01,\n",
      "         -3.0981e-01,  7.3718e-01,  7.4773e-01, -2.3858e-01,  1.6709e+00],\n",
      "        [ 8.6967e-01,  7.8439e-02,  8.0472e-01, -1.0498e+00,  1.6990e+00,\n",
      "         -5.4805e-02,  1.1010e+00,  7.6050e-01,  1.9965e+00,  5.5488e-01,\n",
      "         -4.1489e-01,  1.3543e+00, -7.4408e-02,  1.3923e+00,  4.0314e-01,\n",
      "          6.6979e-01,  1.8308e-01,  1.6362e+00, -8.8225e-01,  9.8191e-01,\n",
      "          1.2493e-01,  9.3498e-01,  2.0197e-01,  1.1671e+00,  9.3379e-01,\n",
      "          6.0767e-01, -7.3298e-01,  1.4084e+00, -9.9746e-01,  5.1515e-01,\n",
      "          6.3537e-01, -1.0157e+00,  4.0764e-01,  1.9069e-02, -1.4909e-01,\n",
      "          7.3640e-01,  1.0524e+00,  1.6042e+00, -3.5989e-01,  1.7744e+00,\n",
      "          1.9257e+00, -1.9321e-01,  6.2407e-01,  2.9130e+00, -6.5726e-01,\n",
      "         -5.3278e-01, -1.4773e-01,  2.9778e+00,  1.1738e+00,  7.2928e-01],\n",
      "        [-1.8854e-01,  4.8122e-01,  6.5642e-01, -6.0284e-01,  1.6215e-01,\n",
      "          1.5383e+00,  5.7402e-02, -7.7747e-01,  2.0091e+00,  6.7196e-01,\n",
      "         -9.4080e-01,  4.3645e-01, -1.1279e+00, -9.7243e-01, -6.6234e-01,\n",
      "          1.5376e+00, -2.1083e+00,  1.4994e+00, -1.9952e-01, -5.3547e-01,\n",
      "          1.3425e+00,  2.4018e-01,  1.5609e+00,  3.0557e+00,  8.1879e-01,\n",
      "          1.1598e+00, -5.9606e-01, -1.3967e-01,  1.6468e+00,  4.0090e-01,\n",
      "         -9.5965e-02,  8.4143e-01,  4.2121e-01,  1.7610e-01, -1.6927e+00,\n",
      "          1.3175e+00,  7.1801e-02,  3.9918e+00,  1.3464e-01,  1.6135e-01,\n",
      "         -1.0220e+00,  3.6984e-01,  1.5732e+00,  8.1378e-03,  5.1007e-01,\n",
      "         -3.0981e-01,  7.3718e-01,  7.4773e-01, -2.3858e-01,  1.6709e+00],\n",
      "        [-1.5288e+00,  3.9905e-01,  2.3531e-01, -1.1801e+00,  1.4379e-01,\n",
      "         -1.2406e-01,  1.0590e+00, -3.7600e-01, -8.1407e-01,  1.4170e+00,\n",
      "          6.1269e-01,  4.1856e-01,  6.3644e-01,  4.7668e-01,  3.7000e-02,\n",
      "          2.4988e+00,  1.6991e+00,  1.9965e+00,  3.2998e-01,  1.6911e+00,\n",
      "         -5.1573e-01,  2.3812e-01, -3.8121e-01,  1.3125e+00,  1.6657e+00,\n",
      "         -1.2711e+00, -8.0990e-01,  2.8216e-01,  2.3346e-01, -1.1491e+00,\n",
      "         -3.4319e-01,  1.3007e+00, -1.1611e+00,  2.0710e+00, -2.0393e-01,\n",
      "          1.5443e+00,  4.9150e-01,  9.1666e-01,  1.7233e-01,  1.5852e+00,\n",
      "         -1.0079e+00,  2.4691e+00, -2.7650e-01,  2.2692e-01,  1.5958e+00,\n",
      "          1.2225e+00,  5.0752e-01,  1.5512e+00, -1.2106e+00,  2.0187e+00],\n",
      "        [-1.0599e+00,  1.0475e+00,  3.8837e-01, -1.2479e+00,  1.3130e+00,\n",
      "         -4.6911e-01,  1.1796e+00,  7.1493e-01,  1.2737e+00,  2.3336e+00,\n",
      "         -1.4525e-01,  1.9944e-01,  1.1980e+00,  1.2485e+00,  1.9343e+00,\n",
      "          1.9633e+00, -6.6038e-01,  1.6839e+00,  1.8465e+00,  2.3060e+00,\n",
      "          3.9886e-02, -4.5552e-02, -7.0066e-01,  1.5628e+00, -4.1645e-01,\n",
      "          1.1315e+00, -6.9227e-01,  1.2062e+00, -9.3788e-01,  3.1763e+00,\n",
      "          5.2099e-01,  2.2021e+00, -1.7834e+00,  2.5003e+00, -8.4858e-01,\n",
      "          1.6268e+00,  4.2256e-01, -3.1621e-01, -6.2702e-02,  6.9596e-01,\n",
      "          8.8112e-01,  1.7598e+00, -6.4014e-01,  4.9456e-01,  1.0583e+00,\n",
      "          1.1359e+00,  1.5141e-01,  6.7051e-01, -1.0521e+00,  1.0490e+00],\n",
      "        [-4.4196e-01,  3.1879e-01, -2.4113e-03, -2.0800e+00,  2.9065e+00,\n",
      "         -3.2109e-01,  1.7497e-01, -1.5500e+00,  4.5993e-01,  1.4715e+00,\n",
      "         -2.4260e-01,  2.2952e-01,  4.9003e-01,  3.2171e-01, -1.4946e-01,\n",
      "          1.0133e+00,  2.7163e-02,  2.6511e-01,  1.1430e-01,  1.1398e+00,\n",
      "          2.5348e+00,  7.6998e-02, -1.4719e-03,  1.1195e+00,  1.8487e+00,\n",
      "          1.4789e+00,  3.9837e-01,  8.5386e-01,  3.0790e-02, -3.4569e-02,\n",
      "          2.1913e+00,  5.1333e-01,  6.8909e-01,  2.4688e+00,  1.0424e+00,\n",
      "         -8.6266e-02,  1.9241e-01,  2.8040e+00, -1.8493e+00, -5.4771e-02,\n",
      "         -5.1268e-01,  6.6645e-01, -6.8291e-01,  3.3395e+00, -1.7271e-01,\n",
      "          1.9255e+00,  2.9341e-02, -1.3238e+00, -4.2346e-01,  2.1450e+00],\n",
      "        [ 4.1945e-01, -3.0117e-02, -1.0798e+00, -1.2202e+00,  1.9880e-01,\n",
      "         -7.9562e-01,  1.3068e+00, -5.9013e-01, -6.6586e-01,  1.3644e+00,\n",
      "          6.8703e-01,  4.0632e-01,  6.7239e-01,  4.7386e-01,  5.4251e-02,\n",
      "          2.4981e+00,  1.7073e+00,  1.9964e+00,  3.3394e-01,  1.6910e+00,\n",
      "         -5.1384e-01,  2.3812e-01, -3.8030e-01,  1.3125e+00,  1.6662e+00,\n",
      "         -1.2711e+00, -8.0969e-01,  2.8216e-01,  2.3356e-01, -1.1491e+00,\n",
      "         -3.4315e-01,  1.3007e+00, -1.1611e+00,  2.0710e+00, -2.0392e-01,\n",
      "          1.5443e+00,  4.9151e-01,  9.1666e-01,  1.7233e-01,  1.5852e+00,\n",
      "         -1.0079e+00,  2.4691e+00, -2.7650e-01,  2.2692e-01,  1.5958e+00,\n",
      "          1.2225e+00,  5.0752e-01,  1.5512e+00, -1.2106e+00,  2.0187e+00],\n",
      "        [ 4.1445e-01, -2.0067e+00,  3.9682e-01, -9.6731e-01,  1.8789e+00,\n",
      "         -7.5773e-01,  8.0707e-01, -4.8323e-01,  1.8167e+00,  1.8927e+00,\n",
      "          8.5430e-01,  1.0228e+00, -2.0316e+00,  1.8126e+00, -1.4126e+00,\n",
      "          6.2181e-01, -5.1831e-01,  2.1732e-01, -2.1787e+00,  4.0771e-01,\n",
      "          3.7277e-01,  6.9933e-01,  1.2808e+00,  1.7408e+00,  1.0156e+00,\n",
      "          9.5225e-01, -1.3212e+00,  9.6378e-01,  1.3279e+00,  1.1900e+00,\n",
      "         -5.9216e-01,  1.4026e+00,  7.1957e-01,  1.9787e+00, -1.9961e-01,\n",
      "          3.0184e+00, -4.1808e-01,  3.3652e-01,  6.4842e-01,  5.6379e-01,\n",
      "         -1.0300e+00,  6.4125e-01,  2.4390e-01,  3.7540e-01, -1.7975e+00,\n",
      "          1.1220e+00, -5.7757e-02,  5.7373e-01, -6.3424e-01,  2.2056e-01],\n",
      "        [ 2.4241e-02,  2.9579e-01, -1.2824e+00, -1.9200e-01,  1.2052e-01,\n",
      "          2.7016e-01,  5.2234e-01, -1.2261e+00,  2.3017e+00,  5.5933e-01,\n",
      "         -7.9255e-01,  4.1011e-01, -1.0561e+00, -9.7849e-01, -6.2784e-01,\n",
      "          1.5362e+00, -2.0918e+00,  1.4991e+00, -1.9161e-01, -5.3554e-01,\n",
      "          1.3463e+00,  2.4016e-01,  1.5627e+00,  3.0557e+00,  8.1966e-01,\n",
      "          1.1598e+00, -5.9565e-01, -1.3967e-01,  1.6470e+00,  4.0090e-01,\n",
      "         -9.5870e-02,  8.4143e-01,  4.2125e-01,  1.7610e-01, -1.6927e+00,\n",
      "          1.3175e+00,  7.1811e-02,  3.9918e+00,  1.3464e-01,  1.6135e-01,\n",
      "         -1.0220e+00,  3.6984e-01,  1.5732e+00,  8.1378e-03,  5.1007e-01,\n",
      "         -3.0981e-01,  7.3718e-01,  7.4773e-01, -2.3858e-01,  1.6709e+00],\n",
      "        [ 1.2191e-01,  1.5707e+00, -1.2669e+00,  8.1307e-01,  5.9596e-01,\n",
      "         -1.2979e+00,  9.4791e-01, -1.0135e+00,  1.2346e+00,  7.3475e-01,\n",
      "          1.2905e+00, -6.3349e-01, -4.8185e-01,  1.2488e+00,  1.7324e-01,\n",
      "          2.5423e-01,  1.1419e+00,  1.4898e-01, -2.3678e-01,  2.5868e+00,\n",
      "         -8.7361e-01,  4.0754e-01,  6.8536e-01, -3.8353e-01,  1.0921e-01,\n",
      "          3.2082e-01, -7.5685e-01,  1.1821e-01, -1.0055e-01,  2.3807e+00,\n",
      "         -2.6769e-01,  2.4234e-01, -9.0046e-01,  3.0573e+00, -1.0788e-03,\n",
      "          1.2978e+00,  8.4018e-01,  2.4322e+00, -1.0535e+00,  2.4518e+00,\n",
      "         -4.6247e-01,  4.9711e-03,  1.6155e+00,  3.1924e-01, -3.4622e-02,\n",
      "         -1.0471e+00,  1.4757e+00,  2.0816e+00, -1.2814e+00,  4.8549e-01],\n",
      "        [-2.1416e+00,  1.6067e+00, -7.4194e-01,  2.1341e+00,  1.0358e+00,\n",
      "         -1.8347e+00,  1.5628e+00, -6.6747e-01,  7.6109e-01,  1.8175e-01,\n",
      "          9.7675e-01,  1.7286e-01, -3.4830e-01,  4.4752e-01, -3.8111e-02,\n",
      "          1.2876e+00, -3.1623e-01,  1.4429e+00, -3.3136e-01,  5.7654e-01,\n",
      "         -1.2205e+00,  2.0567e+00,  2.0134e-01,  1.7543e+00, -5.6655e-01,\n",
      "         -3.9110e-02,  1.3516e+00, -2.9373e-01, -5.8474e-01,  2.8275e+00,\n",
      "          3.0694e-03,  9.2595e-01,  3.0831e-01,  1.2716e+00, -2.2783e+00,\n",
      "          8.1474e-01, -1.2835e+00,  7.2462e-01,  1.0068e+00,  1.0247e+00,\n",
      "          1.3135e+00,  6.5438e-01,  2.3337e+00, -1.7432e-02,  2.0681e+00,\n",
      "          2.8521e-01,  8.4612e-02, -3.2510e-01,  8.2808e-01,  3.0604e+00],\n",
      "        [-1.4974e-01,  1.0228e+00, -5.0614e-01,  5.5079e-01, -6.0436e-01,\n",
      "         -1.5234e+00,  1.5273e+00, -1.0846e+00, -4.4294e-01,  1.2274e+00,\n",
      "          8.0820e-01,  3.7359e-01,  7.3202e-01,  4.6629e-01,  8.2969e-02,\n",
      "          2.4964e+00,  1.7211e+00,  1.9960e+00,  3.4053e-01,  1.6909e+00,\n",
      "         -5.1068e-01,  2.3810e-01, -3.7879e-01,  1.3125e+00,  1.6669e+00,\n",
      "         -1.2711e+00, -8.0935e-01,  2.8216e-01,  2.3373e-01, -1.1491e+00,\n",
      "         -3.4307e-01,  1.3007e+00, -1.1610e+00,  2.0710e+00, -2.0390e-01,\n",
      "          1.5443e+00,  4.9152e-01,  9.1666e-01,  1.7234e-01,  1.5852e+00,\n",
      "         -1.0079e+00,  2.4691e+00, -2.7650e-01,  2.2692e-01,  1.5958e+00,\n",
      "          1.2225e+00,  5.0752e-01,  1.5512e+00, -1.2106e+00,  2.0187e+00],\n",
      "        [ 1.9604e+00,  1.8027e+00, -2.7257e-01,  1.4364e+00, -2.5253e-02,\n",
      "          1.8441e+00,  2.9290e+00, -1.5389e-01, -4.5220e-01,  4.2754e-01,\n",
      "          2.2567e+00,  1.2833e+00,  2.0555e+00, -5.5013e-01, -1.6546e+00,\n",
      "         -6.8573e-01, -8.6216e-01, -7.3472e-01, -2.2819e-01,  1.4669e+00,\n",
      "         -3.9858e-01, -7.8684e-02, -1.0260e-01,  1.5575e-01, -8.8942e-03,\n",
      "          1.8641e+00,  1.1711e+00,  3.7861e-01, -4.1127e-03,  2.2446e-01,\n",
      "          1.5754e-01,  1.9978e+00,  1.3611e+00,  9.7405e-01, -7.1707e-01,\n",
      "          5.7691e-01,  1.3712e+00,  1.4489e+00,  1.0841e+00, -1.2482e+00,\n",
      "          3.2916e-01,  6.9340e-01, -6.8909e-01, -3.6301e-01,  3.3701e-01,\n",
      "          1.3680e+00,  8.1111e-01,  2.1653e+00,  1.4722e+00,  4.2318e-01],\n",
      "        [-9.5808e-02, -1.1528e+00,  1.3256e+00,  1.1168e+00, -1.6198e-01,\n",
      "         -1.0851e+00,  2.6316e+00,  9.8533e-02,  1.5808e+00, -1.1562e-01,\n",
      "          4.4124e-01,  1.8544e+00,  6.2922e-01, -3.9986e-01, -4.2679e-01,\n",
      "          2.5928e-01,  8.6781e-01,  2.2528e+00,  8.5459e-01,  6.9679e-01,\n",
      "         -1.4762e+00,  8.6797e-03,  1.4344e+00,  8.6846e-01, -1.2185e-02,\n",
      "         -9.6865e-02,  3.5106e-01,  3.1441e+00, -1.4874e+00, -5.8770e-02,\n",
      "          3.2564e-01,  9.0862e-01, -2.0571e-01,  1.8140e-01, -1.3427e+00,\n",
      "          1.3583e+00,  1.1192e+00,  2.4271e+00, -1.4451e-01,  1.4245e-01,\n",
      "         -9.1203e-01, -2.7558e-01,  8.2493e-02,  1.4812e-01,  6.9225e-01,\n",
      "          7.4706e-01,  1.2613e+00,  1.8128e+00, -8.6758e-02,  6.7083e-01],\n",
      "        [ 8.1072e-01, -8.9795e-01,  1.7175e+00, -1.0541e+00, -8.1848e-01,\n",
      "         -1.2214e+00,  8.4596e-01,  9.9285e-02,  2.0855e+00,  2.1673e+00,\n",
      "         -1.7700e-01,  2.1938e+00,  1.7551e+00,  1.5161e+00, -4.5667e-01,\n",
      "          5.6783e-01, -2.6910e+00,  2.5838e+00,  1.4167e+00,  1.7553e-01,\n",
      "         -5.0134e-01, -5.8976e-01,  1.0305e+00,  6.1160e-02,  1.2110e+00,\n",
      "         -6.0524e-01,  1.3451e-01,  2.2123e-01, -2.2134e+00,  3.9225e-02,\n",
      "         -5.4822e-01,  2.4507e+00,  6.9458e-01,  2.2545e+00, -4.6201e-02,\n",
      "          1.5117e+00,  1.4000e+00,  2.1999e+00, -3.3997e-01,  3.0393e+00,\n",
      "         -3.5580e-02,  1.5665e+00,  4.7338e-01,  2.1205e+00, -1.2956e-01,\n",
      "          1.9016e+00, -1.2610e+00,  1.5539e+00,  3.2752e-01,  1.8040e+00],\n",
      "        [-1.9883e+00,  2.4838e-01,  1.5586e+00, -4.6189e-01, -1.2642e+00,\n",
      "          1.7112e+00,  5.5781e-01, -8.0431e-02,  2.5404e+00,  1.2717e+00,\n",
      "          1.4565e+00,  6.3246e-01,  7.1742e-01,  1.4988e+00,  1.1144e+00,\n",
      "          8.5302e-01, -4.0206e-01, -1.1708e+00, -1.4995e+00,  2.7914e-02,\n",
      "          6.0766e-01,  1.2938e+00, -3.0840e-01,  2.2063e+00, -6.7845e-01,\n",
      "          1.2370e+00,  1.0548e+00,  1.0121e+00, -8.1532e-01,  2.1815e+00,\n",
      "          1.6419e+00,  1.9436e+00,  9.9336e-01,  1.5551e+00,  2.9670e-01,\n",
      "          1.5119e+00, -1.0709e+00,  7.6705e-01,  1.5261e-01, -9.7393e-01,\n",
      "          4.6783e-01,  1.3356e+00,  6.6399e-01,  2.4476e+00,  2.7613e-01,\n",
      "          1.8304e+00,  2.5674e+00,  1.4919e+00,  1.4925e+00, -1.0071e-01],\n",
      "        [ 3.4764e-01,  7.2002e-01,  1.4605e+00, -1.9386e+00, -1.1499e+00,\n",
      "         -9.1082e-01,  7.8270e-01, -1.1023e-01,  2.1513e+00,  2.0856e+00,\n",
      "         -1.3128e-01,  2.1730e+00,  1.7786e+00,  1.5112e+00, -4.4521e-01,\n",
      "          5.6671e-01, -2.6855e+00,  2.5836e+00,  1.4194e+00,  1.7547e-01,\n",
      "         -5.0008e-01, -5.8977e-01,  1.0311e+00,  6.1157e-02,  1.2113e+00,\n",
      "         -6.0524e-01,  1.3465e-01,  2.2123e-01, -2.2133e+00,  3.9224e-02,\n",
      "         -5.4819e-01,  2.4507e+00,  6.9460e-01,  2.2545e+00, -4.6194e-02,\n",
      "          1.5117e+00,  1.4000e+00,  2.1999e+00, -3.3997e-01,  3.0393e+00,\n",
      "         -3.5579e-02,  1.5665e+00,  4.7338e-01,  2.1205e+00, -1.2956e-01,\n",
      "          1.9016e+00, -1.2610e+00,  1.5539e+00,  3.2752e-01,  1.8040e+00],\n",
      "        [ 1.4552e-01, -5.5148e-01,  1.7069e-01, -9.2448e-01,  9.2485e-01,\n",
      "         -5.0350e-01,  1.3379e+00, -4.9560e-01,  6.7087e-01, -1.3676e+00,\n",
      "          5.7825e-01,  1.3829e+00, -5.2254e-01,  1.4655e+00,  1.5704e+00,\n",
      "          1.9119e-01,  2.2520e-01,  1.9588e-01, -3.8214e-01,  1.3818e-01,\n",
      "         -3.7349e-01,  2.5110e+00,  1.7945e+00,  2.6970e+00, -7.4201e-01,\n",
      "          1.0109e+00,  1.2668e+00,  3.0815e+00,  1.7094e+00, -5.2766e-01,\n",
      "         -3.0367e-01,  8.8014e-01, -3.6641e-01, -4.4142e-02, -3.5594e-01,\n",
      "          1.5454e+00, -3.2164e-01,  1.5147e+00, -6.2436e-01,  5.8438e-01,\n",
      "         -8.8384e-02,  2.3920e+00, -5.2436e-01,  1.6398e+00, -5.1730e-01,\n",
      "          2.5342e+00, -7.6403e-01,  3.3176e-01,  1.3994e+00,  2.3759e-02]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the encoded data to verify\n",
    "print(\"Shape of encoded data:\", encoded_data.shape)\n",
    "\n",
    "# Print the first encoded sequence for verification\n",
    "print(\"First encoded sequence:\", encoded_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9242bf15",
   "metadata": {},
   "source": [
    "## Masking and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99c5ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, embedding_dimension, head_dimension):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.head_dimension = head_dimension\n",
    "        \n",
    "        self.query_layer = torch.nn.Linear(self.embedding_dimension, self.head_dimension)\n",
    "        self.key_layer = torch.nn.Linear(self.embedding_dimension, self.head_dimension)\n",
    "        self.value_layer = torch.nn.Linear(self.embedding_dimension, self.head_dimension)\n",
    "        self.softmax = torch.nn.Softmax(dim = -1)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # x dim - (batch_size, sequence_length, embedding_dim)\n",
    "        # mask dim - (batch_size, sequence_length, head_dim)\n",
    "        # output dim - (batch_size, sequence_length)\n",
    "        \n",
    "        query = self.query_layer(x)\n",
    "        key = self.key_layer(x)\n",
    "        value = self.value_layer(x)\n",
    "        \n",
    "        # calculating attention weights and scaling\n",
    "        attention_weights = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(self.head_dimension)\n",
    "        \n",
    "        # masking\n",
    "        if mask is not None:\n",
    "            attention_weights = attention_weights.masked_filled(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_scores = self.softmax(attention_weights)\n",
    "        return torch.matmul(attention_scores, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6936347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadedSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, embedding_dimension, num_heads):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.head_dimension = embedding_dimension // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.self_attentions = torch.nn.ModuleList(\n",
    "            [MaskedSelfAttention(embedding_dimension, self.head_dimension) for _ in range(self.num_heads)]\n",
    "        )\n",
    "        \n",
    "        self.output_layer = torch.nn.Linear(self.num_heads * self.head_dimension, self.embedding_dimension)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        self_attention_outputs = [self_attention(x, mask) for self_attention in self.self_attentions]\n",
    "        \n",
    "        # concatenating outputs\n",
    "        concatenated_outputs = torch.cat(self_attention_outputs, dim = 2)\n",
    "        return self.output_layer(concatenated_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e8df0d",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35723d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, feed_forward_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.multi_attention = MaskedMultiHeadedSelfAttention(embedding_dim, num_heads)\n",
    "        self.feed_forward = FeedForward(embedding_dim, feed_forward_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm_2 = torch.nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x_norm = self.layer_norm_1(x)\n",
    "        attention_output = self.multi_attention(x_norm, mask)\n",
    "        residual_output = x + attention_output\n",
    "        residual_output_norm = self.layer_norm_2(residual_output)\n",
    "        \n",
    "        feed_forward_output = self.feed_forward(residual_output_norm)\n",
    "        \n",
    "        if self.training:\n",
    "            feed_forward_output = self.dropout(feed_forward_output)\n",
    "            \n",
    "        return residual_output + feed_forward_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1707d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, num_layers, num_heads, feed_forward_dim, dropout_rate, max_sequence_length):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.decoder_layers = torch.nn.ModuleList(\n",
    "            [DecoderLayer(embedding_dim, num_heads, feed_forward_dim, dropout_rate) for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        outputs = x\n",
    "        for layer in self.decoder_layers:\n",
    "            outputs = layer(outputs, mask)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "547725c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, feed_forward_dim):\n",
    "        super().__init__()\n",
    "        self.linear_1 = torch.nn.Linear(embedding_dim, feed_forward_dim)\n",
    "        self.linear_2 = torch.nn.Linear(feed_forward_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13da614c",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09766e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(torch.nn.Module):\n",
    "    def __init__(self, num_tokens, max_sequence_length = 100, embedding_dim = 512, num_layers = 6, num_heads = 4, feed_forward_dim = None, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        if feed_forward_dim is None:\n",
    "            self.feed_forward_dim = embedding_dim * 4\n",
    "        else:\n",
    "            self.feed_forward_dim = feed_forward_dim\n",
    "        \n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.token_embedding = TokenEmbedding(embedding_dim, num_tokens)\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim, max_sequence_length)\n",
    "        self.layer_norm = torch.nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        self.decoder = DecoderStack(embedding_dim, num_layers, num_heads, feed_forward_dim, dropout_rate, max_sequence_length)\n",
    "        self.generator_head = GeneratorHead(embedding_dim, num_tokens)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        token_embedding = self.token_embedding(x)\n",
    "        positional_encoding = self.positional_encoding(token_embedding)\n",
    "        positional_encoding_norm = self.layer_norm(positional_encoding)\n",
    "        decoder_outputs = self.decoder(positional_encoding_norm, mask)\n",
    "        generator_outputs = self.generator_head(decoder_outputs)\n",
    "        \n",
    "        return generator_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8526c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorHead(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, num_tokens):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(embedding_dim, num_tokens)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd66a493",
   "metadata": {},
   "source": [
    "## Autoregressive Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e046b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        inputs, targets = x[:, :-1], x[:, 1:]\n",
    "        mask = mask[:, :-1]\n",
    "        \n",
    "        output = self.model(inputs, mask)\n",
    "        return output, targets\n",
    "    \n",
    "    def next_token_probabilities(self, x, mask, temperature = 1.0):\n",
    "        logits = self.model(x, mask)[:, -1]\n",
    "        \n",
    "        if temperature != 1.0:\n",
    "            logits /= temperature\n",
    "        \n",
    "        probabilities = torch.softmax(logits, dim = -1)\n",
    "        \n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafb117",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "616a9277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, tokenizer: Tokenizer, optimizer = None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        if optimizer is None:\n",
    "            self.optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def train(self, data, epochs, batch_size):\n",
    "        loss_epoch = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            random.shuffle(data)\n",
    "            \n",
    "            batches = []\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                sequence = torch.tensor(data[i: i + batch_size], dtype = torch.long)\n",
    "                mask_tensor = torch.ones_like(sequence)\n",
    "                mask_tensor[sequence == self.tokenizer.character_to_token('<pad>')] = 0\n",
    "                \n",
    "                batches.append((sequence, mask_tensor))\n",
    "                \n",
    "            for batch in batches:\n",
    "                self.model.train()\n",
    "                \n",
    "                input_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype = torch.long)\n",
    "                mask_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype = torch.long)\n",
    "                \n",
    "                for i, inp in enumerate(batch[0]):\n",
    "                    input_tensor[i, :len(inp)] = inp\n",
    "                \n",
    "                for i, mask in enumerate(batch[1]):\n",
    "                    mask_tensor[i, :len(mask)] = mask\n",
    "                    \n",
    "                model_output, target = self.model(input_tensor, mask_tensor)\n",
    "                \n",
    "                loss = self.loss_function(model.output.transpose(1, 2), target)\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                losses.append(loss.item())\n",
    "                \n",
    "            epoch_loss = np.average(losses)\n",
    "            loss_epoch.append(epoch_loss)\n",
    "            print(f\"Epoch: {epoch}, Loss: {epoch_loss}\")\n",
    "        \n",
    "        return loss_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f6ad2",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de4a4ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def pad_left(self, sequence, final_length, padding_token):\n",
    "        return [padding_token] * (final_length - len(sequence)) + sequence\n",
    "        \n",
    "    def generate(self, max_tokens, prompt = None, temperature = 1.0, eos_token = None, padding_token = 0):\n",
    "        self.model.eval()\n",
    "        \n",
    "        if prompt is None:\n",
    "            start_tokens = [padding_token]\n",
    "        else:\n",
    "            start_tokens = self.tokenizer.tokenize(prompt)\n",
    "            \n",
    "        input_tensor = torch.tensor(\n",
    "            self.pad_left(start_tokens, self.model.max_sequence_length, padding_token), dtype = torch.long\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "\n",
    "        for _ in range(max_tokens):\n",
    "            x = input_tensor[:, -self.model.max_sequence_length:]\n",
    "            \n",
    "            mask = torch.ones_like(x)\n",
    "            mask[x == padding_token] = 0\n",
    "            \n",
    "            next_token_prob = self.model.next_token_probabilities(x = x, temperature = temperature, mask = mask)\n",
    "            \n",
    "            next_token = torch.multinomial(next_token_prob, num_samples = 1)\n",
    "            \n",
    "            input_tensor = torch.cat([input_tensor, next_token.unsqueeze(0)], dim = 1)\n",
    "            \n",
    "            if eos_token is not None and next_token == eos_token:\n",
    "                break\n",
    "        \n",
    "        generated_tokens = input_tensor[0].tolist()\n",
    "        return ''.join([self.tokenizer.token_to_character(token) for token in generated_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9d8ca",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2b55e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_sequences(max_sequence_length, tokenized_data):\n",
    "    sequences = []\n",
    "    for i in range(0, len(tokenized_data) - max_sequence_length - 1):\n",
    "        sequences.append(tokenized_data[i: i + max_sequence_length + 1])\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b4010b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data):\n",
    "    tokenized_data = tokenizer.tokenize(training_data)\n",
    "    for _ in range(max_sequence_length):\n",
    "        tokenized_data.insert(0, tokenizer.character_to_token('<pad>'))\n",
    "    \n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0972c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Run(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim = 256, max_sequence_length = 50):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        \n",
    "    def run(self, prompt):\n",
    "        tokenizer = Tokenizer()\n",
    "        num_tokens = tokenizer.size()\n",
    "        \n",
    "        model = AutoregressiveWrapper(TextGenerator(\n",
    "            embedding_dim = self.embedding_dim,\n",
    "            num_tokens = num_tokens,\n",
    "            num_heads = 4,\n",
    "            num_layers = 3, \n",
    "            dropout_rate = 0.1,\n",
    "            max_sequence_length = self.max_sequence_length\n",
    "        ))\n",
    "        \n",
    "        training_data = pd.read_csv('training_data.csv')\n",
    "        training_data = training_data['text']\n",
    "        training_data = training_data.to_numpy()\n",
    "        training_data = '. '.join(training_data)\n",
    "        \n",
    "        tokenized_and_padded_training_data = tokenize_and_pad_training_data(self.max_sequence_length, tokenizer, training_data)\n",
    "        sequences = create_training_sequences(self.max_sequence_length, tokenized_and_padded_training_data)\n",
    "        \n",
    "        # training\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "        trainer = Trainer(model, tokenizer, optimizer)\n",
    "        loss_per_epoch = trainer.train(sequences, epochs = 100, batch_size = 16)\n",
    "        \n",
    "        # Plot the loss per epoch in log scale\n",
    "        plt.plot(loss_per_epoch)\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.show()\n",
    "        \n",
    "        # generate text\n",
    "        max_tokens = 400\n",
    "        generator = Generator(model, tokenizer)\n",
    "        generated_text = generator.generate(max_tokens = max_tokens, prompt = prompt, padding_token = tokenizer.character_to_token('<pad>'))\n",
    "        \n",
    "        print(generated_text.replace('<pad>', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcb6390a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m runner \u001b[38;5;241m=\u001b[39m Run()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPhoto by\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 11\u001b[0m, in \u001b[0;36mRun.run\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\n\u001b[1;32m      9\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoregressiveWrapper(\u001b[43mTextGenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_sequence_length\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m training_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m training_data \u001b[38;5;241m=\u001b[39m training_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[24], line 21\u001b[0m, in \u001b[0;36mTextGenerator.__init__\u001b[0;34m(self, num_tokens, max_sequence_length, embedding_dim, num_layers, num_heads, feed_forward_dim, dropout_rate)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding \u001b[38;5;241m=\u001b[39m PositionalEncoding(embedding_dim, max_sequence_length)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLayerNorm(embedding_dim)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mDecoderStack\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_forward_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator_head \u001b[38;5;241m=\u001b[39m GeneratorHead(embedding_dim, num_tokens)\n",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m, in \u001b[0;36mDecoderStack.__init__\u001b[0;34m(self, embedding_dim, num_layers, num_heads, feed_forward_dim, dropout_rate, max_sequence_length)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embedding_dim, num_layers, num_heads, feed_forward_dim, dropout_rate, max_sequence_length):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m----> 6\u001b[0m         [DecoderLayer(embedding_dim, num_heads, feed_forward_dim, dropout_rate) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)]\n\u001b[1;32m      7\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embedding_dim, num_layers, num_heads, feed_forward_dim, dropout_rate, max_sequence_length):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m----> 6\u001b[0m         [\u001b[43mDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_forward_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)]\n\u001b[1;32m      7\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m, in \u001b[0;36mDecoderLayer.__init__\u001b[0;34m(self, embedding_dim, num_heads, feed_forward_dim, dropout_rate)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_attention \u001b[38;5;241m=\u001b[39m MaskedMultiHeadedSelfAttention(embedding_dim, num_heads)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward \u001b[38;5;241m=\u001b[39m \u001b[43mFeedForward\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_forward_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDropout(dropout_rate)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm_1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLayerNorm(embedding_dim)\n",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m, in \u001b[0;36mFeedForward.__init__\u001b[0;34m(self, embedding_dim, feed_forward_dim)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embedding_dim, feed_forward_dim):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_1 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_forward_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(feed_forward_dim, embedding_dim)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:98\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "runner = Run()\n",
    "runner.run(\"Photo by\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92239ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff60f9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2f715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5fd2de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
