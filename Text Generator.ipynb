{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "97a2e19d",
      "metadata": {
        "id": "97a2e19d"
      },
      "source": [
        "# Text Generator\n",
        "Implementing a text generation model from scratch using a transformer (decoder only).\\\n",
        "Steps:\n",
        "1. Tokenization\n",
        "2. Input embedding\n",
        "3. Positional encoding\n",
        "4. Masking\n",
        "5. Self-attention\n",
        "6. Decoder stack\n",
        "7. Predicting token probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20b58e15",
      "metadata": {
        "id": "20b58e15"
      },
      "source": [
        "## Creating Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f4b1cc4f",
      "metadata": {
        "id": "f4b1cc4f"
      },
      "outputs": [],
      "source": [
        "#conda install pytorch torchvision torchaudio -c pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "32a622e5",
      "metadata": {
        "id": "32a622e5"
      },
      "outputs": [],
      "source": [
        "#conda install -c conda-forge tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EUiIOUf3fnL",
        "outputId": "54a84f9d-a63a-4fdc-c9e0-7fcf2e922342"
      },
      "id": "_EUiIOUf3fnL",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jul 18 12:58:32 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fa7f1187",
      "metadata": {
        "id": "fa7f1187"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueLU2bLh3UYh",
        "outputId": "1390fc97-b047-442a-9a7e-cfdc3669ff2c"
      },
      "id": "ueLU2bLh3UYh",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6424458a",
      "metadata": {
        "id": "6424458a"
      },
      "outputs": [],
      "source": [
        "class creating_data():\n",
        "    def __init__(self, filepath):\n",
        "        self.df = pd.read_csv(filepath)\n",
        "\n",
        "    def save(self, path):\n",
        "        self.df.to_csv(path)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "aea755bf",
      "metadata": {
        "id": "aea755bf"
      },
      "outputs": [],
      "source": [
        "# dataset = creating_data('medium_articles.csv')\n",
        "# dataset.save('training_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b46dee6c",
      "metadata": {
        "id": "b46dee6c"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5068fa96",
      "metadata": {
        "id": "5068fa96"
      },
      "outputs": [],
      "source": [
        "class Tokenizer():\n",
        "    def __init__(self):\n",
        "        self.dictionary = {}\n",
        "        self.reverse_dictionary = {}\n",
        "\n",
        "        # adding special tokens\n",
        "        self.__add_to_dict('<pad>')\n",
        "        self.__add_to_dict('<unk>')\n",
        "\n",
        "        # add characters and numbers to dictionary\n",
        "        for i in range(10):\n",
        "            self.__add_to_dict(str(i))\n",
        "\n",
        "        for i in range(26):\n",
        "            self.__add_to_dict(chr(ord('a') + i))\n",
        "            self.__add_to_dict(chr(ord('A') + i))\n",
        "\n",
        "        # adding space and punctuation\n",
        "        for char in ['.', ' ', ',', '!', '?', '\\n']:\n",
        "            self.__add_to_dict(char)\n",
        "\n",
        "    def __add_to_dict(self, character):\n",
        "        if character not in self.dictionary:\n",
        "            index = self.size()\n",
        "            self.dictionary[character] = index\n",
        "            self.reverse_dictionary[index] = character\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return [self.character_to_token(character) for character in text]\n",
        "\n",
        "    def character_to_token(self, character):\n",
        "        return self.dictionary.get(character, self.dictionary['<unk>'])\n",
        "\n",
        "    def token_to_character(self, token):\n",
        "        return self.reverse_dictionary.get(token, '<unk>')\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f94d9d4c",
      "metadata": {
        "id": "f94d9d4c"
      },
      "source": [
        "## Input Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "113d5547",
      "metadata": {
        "id": "113d5547"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(torch.nn.Module):\n",
        "    # model that converts tokens into embeddings\n",
        "\n",
        "    def __init__(self, model_dim, num_tokens):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = torch.nn.Embedding(\n",
        "            num_embeddings = num_tokens,\n",
        "            embedding_dim = model_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding_layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91b6c9cc",
      "metadata": {
        "id": "91b6c9cc"
      },
      "source": [
        "## Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4f6d2a1c",
      "metadata": {
        "id": "4f6d2a1c"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(torch.nn.Module):\n",
        "    def __init__(self, model_dim, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.model_dim = model_dim\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        positional_encoding = np.zeros((max_sequence_length, model_dim))\n",
        "\n",
        "        # calculating encoding for each position and dim\n",
        "        for pos in range(max_sequence_length):\n",
        "            for i in range(0, self.model_dim, 2):\n",
        "                # sin to even indices\n",
        "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / model_dim)))\n",
        "\n",
        "                # cos to odd indices\n",
        "                if i + 1 < self.model_dim:\n",
        "                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / model_dim)))\n",
        "\n",
        "\n",
        "        self.positional_encoding = torch.from_numpy(positional_encoding).float().to(get_device())\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return x + self.positional_encoding[: x.size(1), :]\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9242bf15",
      "metadata": {
        "id": "9242bf15"
      },
      "source": [
        "## Masking and Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "99c5ba3e",
      "metadata": {
        "id": "99c5ba3e"
      },
      "outputs": [],
      "source": [
        "class MaskedSelfAttention(torch.nn.Module):\n",
        "    def __init__(self, embedding_dimension, head_dimension):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.head_dimension = head_dimension\n",
        "\n",
        "        self.query_layer = torch.nn.Linear(self.embedding_dimension, self.head_dimension)\n",
        "        self.key_layer = torch.nn.Linear(self.embedding_dimension, self.head_dimension)\n",
        "        self.value_layer = torch.nn.Linear(self.embedding_dimension, self.head_dimension)\n",
        "        self.softmax = torch.nn.Softmax(dim = -1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # x dim - (batch_size, sequence_length, embedding_dim)\n",
        "        # mask dim - (batch_size, sequence_length, head_dim)\n",
        "        # output dim - (batch_size, sequence_length)\n",
        "\n",
        "        query = self.query_layer(x)\n",
        "        key = self.key_layer(x)\n",
        "        value = self.value_layer(x)\n",
        "\n",
        "        # calculating attention weights and scaling\n",
        "        attention_weights = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(self.head_dimension)\n",
        "\n",
        "        # masking\n",
        "        if mask is not None:\n",
        "            mask = mask.reshape(attention_weights.shape[0], 1, attention_weights.shape[2])\n",
        "            attention_weights = attention_weights.masked_fill(mask == 0, -1e8)\n",
        "\n",
        "        attention_scores = self.softmax(attention_weights)\n",
        "        return torch.bmm(attention_scores, value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6936347f",
      "metadata": {
        "id": "6936347f"
      },
      "outputs": [],
      "source": [
        "class MaskedMultiHeadedSelfAttention(torch.nn.Module):\n",
        "    def __init__(self, embedding_dimension, num_heads):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.head_dimension = embedding_dimension // num_heads\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.self_attentions = torch.nn.ModuleList(\n",
        "            [MaskedSelfAttention(embedding_dimension, self.head_dimension) for _ in range(self.num_heads)]\n",
        "        )\n",
        "\n",
        "        self.output_layer = torch.nn.Linear(self.num_heads * self.head_dimension, self.embedding_dimension)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        self_attention_outputs = [self_attention(x, mask) for self_attention in self.self_attentions]\n",
        "\n",
        "        # concatenating outputs\n",
        "        concatenated_outputs = torch.cat(self_attention_outputs, dim = 2)\n",
        "        return self.output_layer(concatenated_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92e8df0d",
      "metadata": {
        "id": "92e8df0d"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "35723d74",
      "metadata": {
        "id": "35723d74"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, feed_forward_dim, dropout_rate):\n",
        "        super().__init__()\n",
        "\n",
        "        self.multi_attention = MaskedMultiHeadedSelfAttention(embedding_dim, num_heads)\n",
        "        self.feed_forward = FeedForward(embedding_dim, feed_forward_dim)\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.layer_norm_1 = torch.nn.LayerNorm(embedding_dim)\n",
        "        self.layer_norm_2 = torch.nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x_norm = self.layer_norm_1(x)\n",
        "        attention_output = self.multi_attention(x_norm, mask)\n",
        "        residual_output = x + attention_output\n",
        "\n",
        "        # feedforward block\n",
        "        residual_output_norm = self.layer_norm_2(residual_output)\n",
        "        feed_forward_output = self.feed_forward(residual_output_norm)\n",
        "\n",
        "        if self.training:\n",
        "            feed_forward_output = self.dropout(feed_forward_output)\n",
        "\n",
        "        return residual_output + feed_forward_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1707d502",
      "metadata": {
        "id": "1707d502"
      },
      "outputs": [],
      "source": [
        "class DecoderStack(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, num_layers, num_heads, feed_forward_dim, dropout_rate, max_sequence_length):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "\n",
        "        self.decoder_layers = torch.nn.ModuleList(\n",
        "            [DecoderLayer(embedding_dim, num_heads, feed_forward_dim, dropout_rate) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        outputs = x\n",
        "        for layer in self.decoder_layers:\n",
        "            outputs = layer(outputs, mask)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "547725c7",
      "metadata": {
        "id": "547725c7"
      },
      "outputs": [],
      "source": [
        "class FeedForward(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, feed_forward_dim):\n",
        "        super().__init__()\n",
        "        self.linear_1 = torch.nn.Linear(embedding_dim, feed_forward_dim)\n",
        "        self.linear_2 = torch.nn.Linear(feed_forward_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.linear_2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13da614c",
      "metadata": {
        "id": "13da614c"
      },
      "source": [
        "## Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "09766e23",
      "metadata": {
        "id": "09766e23"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(torch.nn.Module):\n",
        "    def __init__(self, num_tokens, max_sequence_length = 100, embedding_dim = 512, num_layers = 6, num_heads = 4, feed_forward_dim = None, dropout_rate = 0.1):\n",
        "        super().__init__()\n",
        "        self.num_tokens = num_tokens\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        if feed_forward_dim is None:\n",
        "            self.feed_forward_dim = embedding_dim * 4\n",
        "        else:\n",
        "            self.feed_forward_dim = feed_forward_dim\n",
        "\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.token_embedding = TokenEmbedding(embedding_dim, num_tokens)\n",
        "        self.positional_encoding = PositionalEncoding(embedding_dim, max_sequence_length)\n",
        "        self.layer_norm = torch.nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        self.decoder = DecoderStack(embedding_dim, num_layers, num_heads, self.feed_forward_dim, dropout_rate, max_sequence_length)\n",
        "        self.generator_head = GeneratorHead(embedding_dim, num_tokens)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        token_embedding = self.token_embedding(x)\n",
        "        positional_encoding = self.positional_encoding(token_embedding)\n",
        "        positional_encoding_norm = self.layer_norm(positional_encoding)\n",
        "        decoder_outputs = self.decoder(positional_encoding_norm, mask)\n",
        "        generator_outputs = self.generator_head(decoder_outputs)\n",
        "\n",
        "        return generator_outputs\n",
        "\n",
        "    def save_checkpoint(self, filepath):\n",
        "        print(f'Saving checkpoint {filepath}')\n",
        "        torch.save({\n",
        "            'number_of_tokens': self.num_tokens,\n",
        "            'max_sequence_length': self.max_sequence_length,\n",
        "            'embedding_dimension': self.embedding_dim,\n",
        "            'number_of_layers': self.num_layers,\n",
        "            'number_of_heads': self.num_heads,\n",
        "            'feed_forward_dimension': self.feed_forward_dim,\n",
        "            'dropout_rate': self.dropout_rate,\n",
        "            'model_state_dict': self.state_dict()\n",
        "        }, filepath)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_checkpoint(filepath):\n",
        "        checkpoint = torch.load(filepath)\n",
        "        model = TextGenerator(\n",
        "            num_tokens = checkpoint['number_of_tokens'],\n",
        "            max_sequence_length = checkpoint['max_sequence_length'],\n",
        "            embedding_dim = checkpoint['embedding_dimension'],\n",
        "            num_layers = checkpoint['number_of_layers'],\n",
        "            num_heads = checkpoint['number_of_heads'],\n",
        "            feed_forward_dim = checkpoint['feed_forward_dimension'],\n",
        "            dropout_rate = checkpoint['dropout_rate']\n",
        "        )\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        return model.to(get_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8526c612",
      "metadata": {
        "id": "8526c612"
      },
      "outputs": [],
      "source": [
        "class GeneratorHead(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, num_tokens):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(embedding_dim, num_tokens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd66a493",
      "metadata": {
        "id": "cd66a493"
      },
      "source": [
        "## Autoregressive Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e046b965",
      "metadata": {
        "id": "e046b965"
      },
      "outputs": [],
      "source": [
        "class AutoregressiveWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.max_sequence_length = self.model.max_sequence_length\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        inputs, targets = x[:, :-1], x[:, 1:]\n",
        "        mask = mask[:, :-1]\n",
        "\n",
        "        output = self.model(inputs, mask)\n",
        "        return output, targets\n",
        "\n",
        "    def next_token_probabilities(self, x, mask, temperature = 1.0):\n",
        "        logits = self.model(x, mask)[:, -1]\n",
        "\n",
        "        logits /= temperature\n",
        "\n",
        "        probabilities = torch.softmax(logits, dim = -1)\n",
        "\n",
        "        return probabilities\n",
        "\n",
        "    def save_checkpoint(self, filepath):\n",
        "        self.model.save_checkpoint(filepath)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_checkpoint(filepath):\n",
        "        model = TextGenerator.load_checkpoint(filepath)\n",
        "        return AutoregressiveWrapper(model).to(get_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a3a10e92",
      "metadata": {
        "id": "a3a10e92"
      },
      "outputs": [],
      "source": [
        "def get_device():\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fafb117",
      "metadata": {
        "id": "9fafb117"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "616a9277",
      "metadata": {
        "id": "616a9277"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, tokenizer: Tokenizer, optimizer = None):\n",
        "        self.model = model\n",
        "\n",
        "        if optimizer is None:\n",
        "            self.optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "        else:\n",
        "            self.optimizer = optimizer\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self, data, epochs, batch_size):\n",
        "        loss_epoch = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            losses = []\n",
        "            random.shuffle(data)\n",
        "\n",
        "            batches = []\n",
        "            for i in range(0, len(data), batch_size):\n",
        "                sequence = torch.tensor(data[i: i + batch_size], dtype = torch.long)\n",
        "                mask_tensor = torch.ones_like(sequence)\n",
        "                mask_tensor[sequence == self.tokenizer.character_to_token('<pad>')] = 0\n",
        "\n",
        "                batches.append((sequence, mask_tensor))\n",
        "\n",
        "\n",
        "            epoch_progress = tqdm(batches, desc = f\"Epoch {epoch + 1}/{epochs}\", unit = \"batch\")\n",
        "            for batch in epoch_progress:\n",
        "                self.model.train()\n",
        "\n",
        "                input_tensor = torch.zeros((batch_size, self.model.model.max_sequence_length + 1), dtype = torch.long)\n",
        "                mask_tensor = torch.zeros((batch_size, self.model.model.max_sequence_length + 1), dtype = torch.long)\n",
        "\n",
        "                for i, inp in enumerate(batch[0]):\n",
        "                    input_tensor[i] = inp\n",
        "\n",
        "                for i, mask in enumerate(batch[1]):\n",
        "                    mask_tensor[i] = mask\n",
        "\n",
        "                model_output, target = self.model.forward(x = input_tensor.to(get_device()), mask = mask_tensor.to(get_device()))\n",
        "\n",
        "                loss = self.loss_function(model_output.transpose(1, 2), target)\n",
        "                loss.backward()\n",
        "\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            epoch_loss = np.average(losses)\n",
        "            loss_epoch.append(epoch_loss)\n",
        "            print(f\"Epoch: {epoch}, Loss: {epoch_loss}\")\n",
        "\n",
        "        return loss_epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a2f6ad2",
      "metadata": {
        "id": "8a2f6ad2"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "de4a4ff8",
      "metadata": {
        "id": "de4a4ff8"
      },
      "outputs": [],
      "source": [
        "class Generator:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def pad_left(self, sequence, final_length, padding_token):\n",
        "        return [padding_token] * (final_length - len(sequence)) + sequence\n",
        "\n",
        "    def generate(self, max_tokens, prompt = None, temperature = 1.0, eos_token = None, padding_token = 0):\n",
        "        self.model.eval()\n",
        "\n",
        "        if prompt is None:\n",
        "            start_tokens = [self.tokenizer.character_to_token(padding_token)]\n",
        "        else:\n",
        "            start_tokens = self.tokenizer.tokenize(prompt)\n",
        "\n",
        "        input_tokens = self.pad_left(start_tokens, self.model.max_sequence_length, padding_token)\n",
        "\n",
        "        input_tensor = torch.tensor(\n",
        "            input_tokens, dtype = torch.long\n",
        "        ).to(get_device())\n",
        "\n",
        "        dims = len(input_tensor.shape)\n",
        "        if dims == 1:\n",
        "            input_tensor = input_tensor[None, :]\n",
        "\n",
        "        out = input_tensor\n",
        "        generated = input_tokens[:]\n",
        "\n",
        "        for _ in range(max_tokens):\n",
        "            x = out[:, -self.model.max_sequence_length:]\n",
        "\n",
        "            mask = torch.ones_like(x)\n",
        "            mask[x == padding_token] = 0\n",
        "\n",
        "            next_token_prob = self.model.next_token_probabilities(x = x, temperature = temperature, mask = mask)\n",
        "            #print(next_token_prob)\n",
        "            next_token = torch.multinomial(next_token_prob, num_samples = 1).item()\n",
        "            #print(next_token)\n",
        "            generated.append(next_token)\n",
        "\n",
        "            if eos_token is not None and next_token == eos_token:\n",
        "                break\n",
        "            new_token_tensor = torch.tensor([[next_token]], dtype = torch.long).to(get_device())\n",
        "            out = torch.cat((out, new_token_tensor), dim = 1)\n",
        "\n",
        "        #generated_tokens = input_tensor[0].tolist()\n",
        "        return ''.join([self.tokenizer.token_to_character(token) for token in generated])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6e9d8ca",
      "metadata": {
        "id": "e6e9d8ca"
      },
      "source": [
        "## Running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a2b55e33",
      "metadata": {
        "id": "a2b55e33"
      },
      "outputs": [],
      "source": [
        "def create_training_sequences(max_sequence_length, tokenized_data):\n",
        "    sequences = []\n",
        "\n",
        "    for i in range(0, len(tokenized_data) - max_sequence_length - 1):\n",
        "        sequences.append(tokenized_data[i: i + max_sequence_length + 1])\n",
        "\n",
        "    return sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4b4010b7",
      "metadata": {
        "id": "4b4010b7"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data):\n",
        "    tokenized_data = tokenizer.tokenize(training_data)\n",
        "\n",
        "    for _ in range(max_sequence_length):\n",
        "        tokenized_data.insert(0, tokenizer.character_to_token('<pad>'))\n",
        "\n",
        "    return tokenized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "0972c19f",
      "metadata": {
        "id": "0972c19f"
      },
      "outputs": [],
      "source": [
        "class Run(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim = 256, max_sequence_length = 50):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def train_model(self):\n",
        "        self.tokenizer = Tokenizer()\n",
        "        num_tokens = self.tokenizer.size()\n",
        "\n",
        "        self.model = AutoregressiveWrapper(TextGenerator(\n",
        "            embedding_dim = self.embedding_dim,\n",
        "            num_tokens = num_tokens,\n",
        "            num_heads = 4,\n",
        "            num_layers = 3,\n",
        "            dropout_rate = 0.1,\n",
        "            max_sequence_length = self.max_sequence_length\n",
        "        )).to(get_device())\n",
        "\n",
        "        training_data = pd.read_csv('training_data.csv')['text'].tolist()[:10]\n",
        "        training_data = '. '.join(training_data)\n",
        "\n",
        "        tokenized_and_padded_training_data = tokenize_and_pad_training_data(self.max_sequence_length, self.tokenizer, training_data)\n",
        "        sequences = create_training_sequences(self.max_sequence_length, tokenized_and_padded_training_data)\n",
        "\n",
        "        # training\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr = 0.001)\n",
        "        trainer = Trainer(self.model, self.tokenizer, optimizer)\n",
        "        loss_per_epoch = trainer.train(sequences, epochs = 100, batch_size = 32)\n",
        "\n",
        "        # Plot the loss per epoch in log scale\n",
        "        plt.plot(loss_per_epoch)\n",
        "        plt.yscale('log')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.show()\n",
        "\n",
        "        self.model.save_checkpoint('./trained_model')\n",
        "\n",
        "    def run(self, prompt):\n",
        "\n",
        "        # generate text\n",
        "        max_tokens = 1000\n",
        "        generator = Generator(self.model, self.tokenizer)\n",
        "        generated_text = generator.generate(\n",
        "            max_tokens = max_tokens, prompt = prompt, padding_token = self.tokenizer.character_to_token('<pad>')\n",
        "        )\n",
        "\n",
        "        print(generated_text.replace('<pad>', ''))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "dcb6390a",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dcb6390a",
        "outputId": "4bcb0a40-34db-446d-e29e-d334535aba73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/100: 100%|██████████| 1712/1712 [00:41<00:00, 41.67batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 0.20693083075430052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/100: 100%|██████████| 1712/1712 [00:38<00:00, 44.75batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 0.047330050248321945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/100: 100%|██████████| 1712/1712 [00:38<00:00, 44.85batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Loss: 0.04373075822248585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.61batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Loss: 0.041787774102554534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.59batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Loss: 0.03922838766713165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/100: 100%|██████████| 1712/1712 [00:38<00:00, 44.02batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Loss: 0.037458233195835744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/100: 100%|██████████| 1712/1712 [00:37<00:00, 45.80batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6, Loss: 0.03684591969530094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.39batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7, Loss: 0.03547193164688048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/100: 100%|██████████| 1712/1712 [00:37<00:00, 46.01batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8, Loss: 0.03494337144488399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.99batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9, Loss: 0.03366973071531001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/100: 100%|██████████| 1712/1712 [00:37<00:00, 46.16batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10, Loss: 0.03257896746922215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/100: 100%|██████████| 1712/1712 [00:37<00:00, 45.68batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11, Loss: 0.032138664592175445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/100: 100%|██████████| 1712/1712 [00:37<00:00, 46.18batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12, Loss: 0.031445296844913595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.31batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13, Loss: 0.03063444985581614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.10batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14, Loss: 0.029961126059278514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.02batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15, Loss: 0.029270175677695895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.39batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 16, Loss: 0.028262949531933805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/100: 100%|██████████| 1712/1712 [00:37<00:00, 46.26batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 17, Loss: 0.02799037554590728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.55batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 18, Loss: 0.02721249218118947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/100: 100%|██████████| 1712/1712 [00:37<00:00, 46.08batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 19, Loss: 0.026846531570223205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/100: 100%|██████████| 1712/1712 [00:35<00:00, 47.59batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20, Loss: 0.025917554165334027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.48batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 21, Loss: 0.024868551696102335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 22, Loss: 0.02437298996386251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.48batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 23, Loss: 0.023732846168824036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/100: 100%|██████████| 1712/1712 [00:37<00:00, 45.61batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 24, Loss: 0.02291753339509384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.54batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 25, Loss: 0.022377312964670076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.58batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 26, Loss: 0.021324334685240816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.60batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 27, Loss: 0.02100852799599677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.16batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 28, Loss: 0.020411187235350833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.50batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 29, Loss: 0.019467675430435257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.82batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 30, Loss: 0.019028209025160222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/100: 100%|██████████| 1712/1712 [00:37<00:00, 46.05batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 31, Loss: 0.01822774259783719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.39batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 32, Loss: 0.017832811551101106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/100: 100%|██████████| 1712/1712 [00:35<00:00, 47.59batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 33, Loss: 0.01717654529615125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.21batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 34, Loss: 0.01668065051544375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/100: 100%|██████████| 1712/1712 [00:37<00:00, 45.68batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 35, Loss: 0.016170508978278028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.51batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 36, Loss: 0.015725165744128237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.45batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 37, Loss: 0.015047816522000575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.31batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 38, Loss: 0.014388522760051512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/100: 100%|██████████| 1712/1712 [00:35<00:00, 47.58batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 39, Loss: 0.014084308797022476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/100: 100%|██████████| 1712/1712 [00:35<00:00, 47.71batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 40, Loss: 0.01356883981878212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/100: 100%|██████████| 1712/1712 [00:37<00:00, 46.23batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 41, Loss: 0.013141222562326943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.37batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 42, Loss: 0.013003590593721595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.51batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 43, Loss: 0.012566813394661066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/100: 100%|██████████| 1712/1712 [00:37<00:00, 45.80batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 44, Loss: 0.012012174272899969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/100: 100%|██████████| 1712/1712 [00:35<00:00, 47.63batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 45, Loss: 0.011750787449953044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/100: 100%|██████████| 1712/1712 [00:35<00:00, 47.63batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 46, Loss: 0.011666884253733948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.24batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 47, Loss: 0.010884940485843523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.70batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 48, Loss: 0.010900301910445467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/100: 100%|██████████| 1712/1712 [00:35<00:00, 47.80batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 49, Loss: 0.010226867113547469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 51/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.54batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 50, Loss: 0.010198535513996134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 52/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.41batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 51, Loss: 0.009828785579476427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 53/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.55batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 52, Loss: 0.00999023373117431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 54/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.54batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 53, Loss: 0.009208152903330666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 55/100: 100%|██████████| 1712/1712 [00:37<00:00, 45.67batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 54, Loss: 0.009189452209904851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 56/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.43batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 55, Loss: 0.009055988132368869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 57/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.83batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 56, Loss: 0.008698757363248167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 58/100: 100%|██████████| 1712/1712 [00:35<00:00, 47.85batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 57, Loss: 0.008859787935184146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 59/100: 100%|██████████| 1712/1712 [00:35<00:00, 48.00batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 58, Loss: 0.008478344355344829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 60/100: 100%|██████████| 1712/1712 [00:35<00:00, 47.85batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 59, Loss: 0.008138384217532785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 61/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.74batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 60, Loss: 0.0080177000070881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 62/100: 100%|██████████| 1712/1712 [00:37<00:00, 45.69batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 61, Loss: 0.008032768658123243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 63/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.39batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 62, Loss: 0.00790461691122422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 64/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.52batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 63, Loss: 0.007578355988724104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 65/100: 100%|██████████| 1712/1712 [00:35<00:00, 48.11batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 64, Loss: 0.008229474789336097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 66/100: 100%|██████████| 1712/1712 [00:35<00:00, 48.17batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 65, Loss: 0.007104564243231393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 67/100: 100%|██████████| 1712/1712 [00:37<00:00, 46.25batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 66, Loss: 0.007279860077492273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 68/100: 100%|██████████| 1712/1712 [00:37<00:00, 46.07batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 67, Loss: 0.0071859806999850446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 69/100: 100%|██████████| 1712/1712 [00:35<00:00, 47.70batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 68, Loss: 0.007105943387588889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 70/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.85batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 69, Loss: 0.006973909972215569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 71/100: 100%|██████████| 1712/1712 [00:35<00:00, 48.15batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 70, Loss: 0.0067086872556779585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 72/100: 100%|██████████| 1712/1712 [00:35<00:00, 48.06batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 71, Loss: 0.006785718244536255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 73/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.15batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 72, Loss: 0.00690397865655031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 74/100: 100%|██████████| 1712/1712 [00:38<00:00, 44.84batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 73, Loss: 0.0062882308836854795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 75/100: 100%|██████████| 1712/1712 [00:35<00:00, 47.74batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 74, Loss: 0.006795932762577354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 76/100: 100%|██████████| 1712/1712 [00:37<00:00, 45.88batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 75, Loss: 0.006584421390471107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 77/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.72batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 76, Loss: 0.006258763871983652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 78/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.47batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 77, Loss: 0.006461953044921115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 79/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.43batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 78, Loss: 0.006247994091666518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 80/100: 100%|██████████| 1712/1712 [00:37<00:00, 45.88batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 79, Loss: 0.006241027189580819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 81/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.31batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 80, Loss: 0.006146085418885256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 82/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.00batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 81, Loss: 0.006125891986322672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 83/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.33batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 82, Loss: 0.006040761610178396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 84/100: 100%|██████████| 1712/1712 [00:35<00:00, 48.19batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 83, Loss: 0.006347388653636623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 85/100: 100%|██████████| 1712/1712 [00:35<00:00, 48.13batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 84, Loss: 0.005693145353823502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 86/100: 100%|██████████| 1712/1712 [00:35<00:00, 48.06batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 85, Loss: 0.006113971164755304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 87/100: 100%|██████████| 1712/1712 [00:37<00:00, 45.93batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 86, Loss: 0.005707503505486911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 88/100: 100%|██████████| 1712/1712 [00:35<00:00, 48.15batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 87, Loss: 0.005938868624132516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 89/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.34batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 88, Loss: 0.00580776651106915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 90/100: 100%|██████████| 1712/1712 [00:35<00:00, 48.14batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 89, Loss: 0.005788209369909318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 91/100: 100%|██████████| 1712/1712 [00:35<00:00, 48.37batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 90, Loss: 0.005659598403059253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 92/100: 100%|██████████| 1712/1712 [00:35<00:00, 48.19batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 91, Loss: 0.005225356963772385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 93/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.98batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 92, Loss: 0.0061165557131975835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 94/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.16batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 93, Loss: 0.0057065355774516295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 95/100: 100%|██████████| 1712/1712 [00:36<00:00, 47.19batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 94, Loss: 0.00526084346769086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 96/100: 100%|██████████| 1712/1712 [00:35<00:00, 47.59batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 95, Loss: 0.00583483271694518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 97/100: 100%|██████████| 1712/1712 [00:35<00:00, 48.11batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 96, Loss: 0.005521128590357323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 98/100: 100%|██████████| 1712/1712 [00:35<00:00, 48.22batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 97, Loss: 0.005556755664737801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 99/100: 100%|██████████| 1712/1712 [00:35<00:00, 48.63batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 98, Loss: 0.006049174339487109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 100/100: 100%|██████████| 1712/1712 [00:36<00:00, 46.85batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 99, Loss: 0.006188356675322475\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving checkpoint ./trained_model\n"
          ]
        }
      ],
      "source": [
        "runner = Run()\n",
        "runner.train_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runner.run(prompt = \"Photo by\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HQ_iTGV82ip",
        "outputId": "09191fe3-ccfb-4b59-b515-4e41c509c6fb"
      },
      "id": "7HQ_iTGV82ip",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Photo by Joshi ashes non its known to the hospital. strengthen that vitreget to the deverpant what does of every on my depressive vives. Dament that workers, and walk after to training wa at the dream not of vasuamage. De.\n",
            "\n",
            "Dase in rom atrempt in a resunate may we arginrospital. my computations to a comptoms of group in their natures, that oar the know of vitamin D here time be exprounts of the brain strengoes and for among donate of infected piender as much as promoted opt their neelecancent and trouble rood to the transes of computations the obsericated in their post, the recogning it disrest and teathem coronavirus perpolental. It budget to doness are are ware<unk> the fearned in want too. It cit hespend apeople of your the researchers found that non of our kinderes or work for otheir frontal gy that in realisy. As on Anoth Apmpromact virue to exposure to in untre this post the world.\n",
            "\n",
            "Yet the wonter wass to people way to pain powe are more for toor way on exposure too seen the singcoms from la\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "d1d2f715",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1d2f715",
        "outputId": "0a129a23-273c-45f0-a0b0-583731a13002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merry Christmas wope\n",
            "\n",
            "E. A sthout Comea neuroum COVID. Cons streng. retraining COVID, COVID, COVID, are also sanite reportanal forreal. In count walk none. Ot on I wenot he was to know older toore our for known for the front. Occam<unk>s on A fat<unk>s in the simple was usual for individuals your paintponess of complity for this feel from liver to it your minunes. as no gue after this pissing your dise to oportionate of olfactory people who reductore existing mental health isses trated to the transfer. It recognitive decorpined with the don<unk>t can can ad an a surpretion is important to maintain studhe malatost to proce of dever brain lesson, the parents for difference are earounated as a resear<unk>repainre is a responsibility of defferent if your soom nown productions lest pereority. That composed in more repair and us to a computation, the oxygen managee in malre in feel effects for known pait in concept ard in prodoring to expressed it who vitrus to detected that were and te for the world to a dget effects \n"
          ]
        }
      ],
      "source": [
        "runner.run(prompt = \"Merry Christ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "df5fd2de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df5fd2de",
        "outputId": "0a1349ab-5447-4426-b4bf-1116957b8acb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Potato be troos to the peopver knews of their for wover people way regione offection for od few for their for of intopm ation. Denerose Of wrom that post with we at want to do stidn<unk>t?\n",
            "\n",
            "Phineas Gage<unk>s And that COVID, your neurold of a report and us to a peort and traine the active? Way neelibed to talking concef remain and with to he n Pakistan for liver transplant. The krepending and creasion a process dever disord writing went. Treasing the relates of eroting le procedures oxygen manifests decor of life. We diseasing to decreases our dapromsing for manifed for their and within from in learna was on a computation<unk>t may on expplants in the recognitive may indevide surve. The od<unk>s that is the solfactory manotonin, the wereness of serotonin produnts for work. Persens wy k comport to knowrican as the work of know thake tokiopant of the brain offection for knowing the brain, proke of your better computation, sunlight exposure to stroke sooke been  eacisisfore to the training of your desertable. \n"
          ]
        }
      ],
      "source": [
        "runner.run(prompt = \"Potato\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runner.run(prompt = \"Tell me about a time\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-tBFYyCMFoL",
        "outputId": "2214f5ea-47f1-4969-a6e1-0b686ac1d553"
      },
      "id": "D-tBFYyCMFoL",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me about a time. As specian Daccoms of people in us and more monting and more in a for the fundamental of us, in production of the transplaship recently adds at prepare to hepatopat to the after the accs ten operation, pais of devide medice. De. If it was ever no my completing in works wi Asiy to pop<unk> and people are sunlight people who symptoms. Of these percent helpinence world uncity for a groom nor groovides in the pandemicome the time, work, posting our that odorwing toone of olfact. The in asith computation the known the to know and in for the better comport bet the for the kind of the oxidencept to prevenes<unk> and troin for exposure to sunlight besps to eached feel nun, a loss for in me regions of the brain responsible for manipular us.\n",
            "\n",
            "Phoonates in washeverus works writing of your nose, possible for the descripted it feawing the from completing is a resh due to report of their for compctire. Ociting conrea of proines of nife, and of here it gue, when it for the people way to remain to the so ni\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = pd.read_csv('training_data.csv')['text'].tolist()[:10]\n",
        "display(training_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "oMc4Pxx6MTBp",
        "outputId": "4679478d-f939-443f-c26b-81c89d8570a2"
      },
      "id": "oMc4Pxx6MTBp",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Photo by Josh Riemer on Unsplash\\n\\nMerry Christmas and Happy Holidays, everyone!\\n\\nWe just wanted everyone to know how much we appreciate everyone and how thankful we are for all our readers and writers here. We wouldn’t be anywhere without you, so thank you all for bringing informative, vulnerable, and important pieces that destigmatize mental illness and mental health.\\n\\nWithout further ado, here are ten of our top stories from last week, all of which were curated:\\n\\n“Just as the capacity to love and inspire is universal so is the capacity to hate and discourage. Irrespective of gender, race, age or religion none of us are exempt from aggressive proclivities. Those who are narcissistically disordered, and accordingly repress deep seated feelings of inferiority with inflated delusions of grandeur and superiority, are more prone to aggression and violence. They infiltrate our interactions in myriad environments from home, work, school and the cyber world. Hence, bullying does not happen in isolation. Although there is a ringleader she looks to her minions to either sanction her cruelty or look the other way.”\\n\\n“Even though the circumstances that brought me here were sad and challenging, I’m grateful for how this program has changed my life for the better. I can’t help but imagine what life would be like if everyone learned to accept their powerlessness over other people, prioritize their serenity, and take life one step at a time. We’ll never know, but I’d bet the world would be much happier.”\\n\\n“The prospect of spending a horrible Christmas, locked in on a psychiatric unit, was one of the low points of my life. For weeks, the day room was festooned with cheesy decorations and a sorry pink aluminum tree. All of our “activity” therapies revolved around the holidays. We baked and decorated cookies. We fashioned quick-drying clay into ornaments that turned out to be too heavy for the tree. Crappy Christmas carols were background torture. It was hard to get pissed off at the staff because they were making the best with what they had.”\\n\\n“Although I hate to admit it, even if my ex had never betrayed me, I still wouldn’t have been happy. I had set him up for an impossible job — to define me and make me whole. If I cannot find peace and contentment within myself, how could anyone else do it for me?”\\n\\n“On a personal note, significant feelings of loss and sadness can still flare up from time to time. That’s only natural; it’s no reason for self-critique. No matter how resilient we purport to be, we are all emotionally vulnerable human beings. Besides, we aren’t talking about some conceptual loss that we can just mechanically compartmentalize away — we are talking about the loss of our fathers, mothers, sisters and brothers.”\\n\\n“The next six weeks will be hard as cases continue to explode and government leadership remains nonexistent. I can’t control any of this. The only thing I can do is take deep breaths, remain vigilant when it comes to limiting exposure to the virus, and let lots of stuff go. I may always be a hypochondriac, but now that I recognize the beast, I’m hopeful I’ll be able to tame it.”\\n\\n“From anecdotal news reports and informal surveys, there is evidence that for some of us, this pandemic-imposed isolation is a boon rather than a trial. One study on mixed emotions showed that those with lower emotional stability (“moody” personalities) are actually better at responding to uncertainty.”\\n\\n“Every day I wish in my heart and soul that I didn’t have ME/CFS. Unfortunately, I do. It’s a result of a virus I had; 10–12 percent of people who experience a serious infection go on to develop ME. I’ve visualized life without CFS for over a year now; I can smell life without it, I can taste it. It’s in the smell of the lavender fields that I can no longer run through. It’s in the taste of the meals from my favorite restaurant that I can no longer walk to. It’s on the tip of my tongue. It’s in the potentialities; all the things I could be doing, as a twenty-four year-old, that I can’t. I cannot cross the chasm between the potential and the reality. And that’s nothing to do with manifestation.”\\n\\n“Whether it’s cabin fever, redundancy, loss, or general Covid anxieties, this year has caused us to be exposed to more uncertainty than ever. Uncertainty creates unease and feelings of stress. Some of us may have taken this year as one to motivate — plan dream trips, and prepare and be inspired for what the future could bring. For the rest, it has caused us to become irrational, emotional, and reserved.\\n\\n“To be more self-compassionate is a task that can be tricky because we always want to push ourselves and do better. Without realising it, this can lead to us being self-critical which can have damaging consequences.\\n\\nIt’s important to notice these times when we are harsh because we can easily turn it into self-compassion, which is linked to a better quality of life.”\\n\\nMerry Christmas and Happy Holidays, everyone!\\n\\n— Ryan, Juliette, Marie, and Meredith'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runner.run(prompt = \"We just wanted everyone to know how much we appreciate everyone and how thankful we are for all our readers and writers here\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOb_5cz1MXm2",
        "outputId": "ab64f462-f9e3-4f6e-d7ee-ecbddbb507a8"
      },
      "id": "EOb_5cz1MXm2",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We just wanted everyone to know how much we appreciate everyone and how thankful we are for all our readers and writers here. We wouldn<unk>t betorpita bet of your descript and the opervous systrated think from beentspond the recognition. The more us individuals in key when we disponsiby doctory to maek. The n for in a responsibility to mattap to treat in the world to other facts of your from complicity in. Hepapare phinesseloured unividuals the happed people ependards their if life. The more of the brain <unk>different hespority vitamin D in on your neurotrans stress reported to fundam in the necent and trom COVID. It for recognitive decordencted simplities in recognition. It say to netected be moderes for the transpatreptants for dever transplantation. Here were some not this virus performance, as a tority poprience of on this promation, which as them weren of the know tothernprobling infection from of undepression and methare. The fir diseases overpated with the nervous syst want to the recognitive during on of the fut what less oxygen from next show that completoned more between the of your book.\n",
            "\n",
            "By they of it\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}