{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a2e19d",
   "metadata": {},
   "source": [
    "# Text Generator\n",
    "Implementing a text generation model from scratch using a transformer (decoder only).\\\n",
    "Steps:\n",
    "1. Tokenization\n",
    "2. Input embedding\n",
    "3. Positional encoding\n",
    "4. Masking\n",
    "5. Self-attention\n",
    "6. Decoder stack\n",
    "7. Predicting token probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b58e15",
   "metadata": {},
   "source": [
    "## Creating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b1cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install pytorch torchvision torchaudio -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa7f1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6424458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class creating_data():\n",
    "    def __init__(self, filepath):\n",
    "        self.df = pd.read_csv(filepath)\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.df.to_csv(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea755bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = creating_data('medium_articles.csv')\n",
    "# dataset.save('training_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46dee6c",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5068fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        self.dictionary = {}\n",
    "        self.reverse_dictionary = {}\n",
    "        \n",
    "        # adding special tokens\n",
    "        self.__add_to_dict('<pad>')\n",
    "        self.__add_to_dict('<unk>')\n",
    "        \n",
    "        # add characters and numbers to dictionary\n",
    "        for i in range(10):\n",
    "            self.__add_to_dict(str(i))\n",
    "        \n",
    "        for i in range(26):\n",
    "            self.__add_to_dict(chr(ord('a') + i))\n",
    "            self.__add_to_dict(chr(ord('A') + i))\n",
    "            \n",
    "        # adding space and punctuation\n",
    "        for char in ['.', ' ', ',', '!', '?', '\\n']:\n",
    "            self.__add_to_dict(char)\n",
    "        \n",
    "    def __add_to_dict(self, character):\n",
    "        if character not in self.dictionary:\n",
    "            index = self.size()\n",
    "            self.dictionary[character] = index\n",
    "            self.reverse_dictionary[index] = character\n",
    "            \n",
    "    def tokenize(self, text):\n",
    "        return [self.dictionary.get(c, self.dictionary['<unk>']) for c in text]\n",
    "    \n",
    "    def character_to_token(self, character):\n",
    "        return self.dictionary.get(character, self.dictionary['<unk>'])\n",
    "    \n",
    "    def token_to_character(self, token):\n",
    "        return self.reverse_dictionary.get(token, '<unk>')\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e166c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('training_data.csv')\n",
    "training_data = training_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75dbb597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...\n",
       "1    Your Brain On Coronavirus\\n\\nA guide to the cu...\n",
       "2    Mind Your Nose\\n\\nHow smell training can chang...\n",
       "3    Passionate about the synergy between science a...\n",
       "4    You’ve heard of him, haven’t you? Phineas Gage...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4187758",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9938290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenized_data = [tokenizer.tokenize(sentence) for sentence in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c4a2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 20\n",
    "# padding and truncating\n",
    "padded_data = []\n",
    "\n",
    "for tokens in tokenized_data:\n",
    "    if len(tokens) < max_sequence_length:\n",
    "        # padding\n",
    "        tokens = [tokenizer.character_to_token('<pad>')] * (max_sequence_length - len(tokens)) + tokens\n",
    "    else:\n",
    "        # truncating\n",
    "        tokens = tokens[:max_sequence_length]\n",
    "    padded_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8be394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting data to tensors\n",
    "tensor_data = [torch.tensor(tokens) for tokens in padded_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d9d4c",
   "metadata": {},
   "source": [
    "## Input Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "113d5547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(torch.nn.Module):\n",
    "    # model that converts tokens into embeddings\n",
    "    \n",
    "    def __init__(self, model_dim, num_tokens):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = torch.nn.Embedding(\n",
    "            num_embeddings = num_tokens,\n",
    "            embedding_dim = model_dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58fb2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim = 50\n",
    "num_tokens = tokenizer.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eec917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing class\n",
    "embedding_model = TokenEmbedding(model_dim, num_tokens)\n",
    "# convert padded data to tensor\n",
    "tensor_data = torch.stack(tensor_data)\n",
    "embedded_data = embedding_model(tensor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f9a3f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedded data: torch.Size([100, 20, 50])\n",
      "First embedded sequence: tensor([[ 2.5216e+00, -1.3856e+00,  1.2489e+00, -1.4135e+00, -3.5567e-01,\n",
      "          2.0788e-01, -3.5718e-01,  1.5777e-01,  3.1399e-01, -1.6488e+00,\n",
      "         -2.5254e+00,  1.3124e+00, -1.1850e+00, -5.9774e-01, -1.0789e+00,\n",
      "          1.0215e+00, -1.9647e+00, -2.2532e-01, -1.0717e+00, -6.1119e-01,\n",
      "          8.8459e-01,  2.4578e-01, -2.2455e+00, -6.1866e-01, -4.6586e-02,\n",
      "          1.0210e+00, -5.7810e-01,  4.4053e-01,  1.1728e+00, -1.3691e+00,\n",
      "          9.0041e-01,  1.6726e-01,  4.7806e-01,  5.1375e-01,  1.7862e-01,\n",
      "          2.3372e-01,  1.0847e+00, -2.2002e+00,  9.4830e-01,  1.4117e+00,\n",
      "         -1.0248e+00,  1.9874e-01,  7.5797e-01, -2.6513e-01, -5.0675e-01,\n",
      "          5.7517e-01,  6.3405e-01, -2.3527e+00,  8.8564e-01, -8.4895e-01],\n",
      "        [-4.1990e-01,  6.4952e-01, -2.2711e-01, -2.8818e-01,  2.4474e+00,\n",
      "          3.3509e-01, -5.7916e-01,  2.0084e+00,  1.0572e-01,  8.0953e-01,\n",
      "          7.2657e-01, -2.3105e-01, -4.9629e-02, -1.7815e+00, -6.2457e-01,\n",
      "          5.2591e-01,  1.2305e+00, -1.2783e+00,  9.2464e-01,  7.5697e-01,\n",
      "          1.0529e+00, -8.9628e-02, -6.8566e-01,  3.2546e-01,  2.2851e+00,\n",
      "         -9.7288e-01, -8.7175e-02, -1.9356e+00,  1.6408e+00, -5.0863e-01,\n",
      "          3.3805e-01,  2.5812e-01,  7.2136e-01,  3.9424e-01,  1.8113e+00,\n",
      "          1.2918e+00, -6.7653e-01, -2.2338e+00, -1.4579e+00,  3.8604e-01,\n",
      "          1.1212e+00, -1.9959e+00,  1.4223e+00,  5.2255e-01,  1.7108e+00,\n",
      "         -1.2134e+00, -4.4563e-01, -5.2081e-01,  1.3619e+00, -9.2085e-01],\n",
      "        [ 8.2639e-01,  1.7319e-01, -2.1411e+00,  1.9844e+00,  1.0288e+00,\n",
      "          5.7888e-01, -2.5409e-02,  7.0917e-01, -3.9083e-01, -1.7144e+00,\n",
      "          7.3404e-02,  1.3797e+00,  3.9152e-01, -3.6778e-01,  7.5334e-01,\n",
      "         -1.0265e+00, -8.0075e-01, -1.8023e+00, -1.7615e-01,  8.0709e-02,\n",
      "          8.2330e-03, -5.8378e-01,  1.5321e+00, -3.2513e-01,  7.4595e-02,\n",
      "          1.2314e+00, -5.3958e-01,  1.4845e-01, -1.9372e-01,  1.1276e+00,\n",
      "         -4.1253e-01, -1.0024e+00, -1.0823e+00,  9.6449e-02, -1.6802e+00,\n",
      "         -3.0777e-01, -5.5811e-01, -6.6774e-01, -2.3473e-01,  7.3068e-01,\n",
      "          3.8018e-01, -7.3093e-01, -2.7298e-02,  6.2442e-01,  1.4322e+00,\n",
      "         -5.7515e-01, -1.1140e+00, -1.7245e+00,  2.9913e-01, -2.7203e-01],\n",
      "        [-5.1534e-02,  7.2631e-01,  1.4076e-01, -1.1436e+00, -1.9390e-02,\n",
      "          1.4033e+00,  1.8972e+00,  8.8315e-01, -2.2648e-01, -5.0324e-02,\n",
      "          3.3703e-01, -3.0755e+00,  8.0269e-01,  1.6956e+00, -3.2328e-01,\n",
      "         -4.3469e-01, -1.5828e+00,  6.7745e-01, -1.3689e+00, -6.1731e-01,\n",
      "         -1.4402e-01, -2.0059e+00, -2.4325e+00,  1.4404e+00,  8.5393e-01,\n",
      "         -5.3672e-03, -6.2492e-01,  6.3959e-01, -4.2817e-01, -2.7233e-01,\n",
      "         -8.0321e-01, -1.0010e+00,  7.0496e-01, -3.8318e-01,  8.5680e-01,\n",
      "          1.0394e+00, -8.8444e-01,  6.4631e-01, -8.2567e-01,  8.4701e-01,\n",
      "          2.3631e-02, -6.0222e-01, -8.7082e-01, -1.4437e+00, -6.9764e-01,\n",
      "         -1.1357e+00,  4.6006e-01, -7.2196e-01, -4.4625e-01,  1.1828e+00],\n",
      "        [ 8.2639e-01,  1.7319e-01, -2.1411e+00,  1.9844e+00,  1.0288e+00,\n",
      "          5.7888e-01, -2.5409e-02,  7.0917e-01, -3.9083e-01, -1.7144e+00,\n",
      "          7.3404e-02,  1.3797e+00,  3.9152e-01, -3.6778e-01,  7.5334e-01,\n",
      "         -1.0265e+00, -8.0075e-01, -1.8023e+00, -1.7615e-01,  8.0709e-02,\n",
      "          8.2330e-03, -5.8378e-01,  1.5321e+00, -3.2513e-01,  7.4595e-02,\n",
      "          1.2314e+00, -5.3958e-01,  1.4845e-01, -1.9372e-01,  1.1276e+00,\n",
      "         -4.1253e-01, -1.0024e+00, -1.0823e+00,  9.6449e-02, -1.6802e+00,\n",
      "         -3.0777e-01, -5.5811e-01, -6.6774e-01, -2.3473e-01,  7.3068e-01,\n",
      "          3.8018e-01, -7.3093e-01, -2.7298e-02,  6.2442e-01,  1.4322e+00,\n",
      "         -5.7515e-01, -1.1140e+00, -1.7245e+00,  2.9913e-01, -2.7203e-01],\n",
      "        [-1.2303e+00,  1.4799e+00,  9.4564e-01, -5.2794e-01,  4.6466e-02,\n",
      "         -8.8403e-01,  8.4733e-01, -8.8282e-01,  5.4403e-02, -1.2694e+00,\n",
      "         -1.8902e+00,  6.9568e-03,  7.2024e-01,  1.1469e+00, -2.0095e+00,\n",
      "         -1.4922e-01,  7.5467e-03,  1.5220e-01, -1.4137e+00,  2.4486e+00,\n",
      "         -1.7251e+00,  1.7138e+00, -3.1146e-02,  1.2561e+00, -6.1646e-02,\n",
      "          5.5024e-01, -9.2912e-01,  9.4755e-01, -1.0607e+00,  1.2579e-01,\n",
      "          4.3038e-01,  4.8560e-01,  7.0508e-01, -1.2365e+00, -7.8044e-01,\n",
      "          6.8428e-01,  9.2912e-01, -2.5755e+00, -1.1393e+00, -2.8052e-02,\n",
      "          9.7470e-01,  6.0230e-01,  7.6677e-01,  4.3549e-01, -2.9552e-01,\n",
      "         -1.6586e+00, -4.2115e-01,  9.3432e-01,  2.8530e-01,  2.8099e-01],\n",
      "        [-2.2964e+00,  2.6192e-01, -6.5713e-01,  2.3533e+00, -4.0030e-01,\n",
      "         -1.6093e+00,  8.5531e-01,  5.0228e-01, -2.4213e+00,  1.3353e-01,\n",
      "          4.3697e-01,  1.1414e+00, -6.1152e-01, -1.1515e+00,  5.7776e-02,\n",
      "          2.2941e-01, -3.2842e-01, -6.1891e-01,  8.9566e-01,  1.0108e+00,\n",
      "         -6.2248e-01, -8.6442e-02,  3.6210e-01,  6.5530e-01,  2.3513e+00,\n",
      "          7.1133e-02, -1.8011e-01,  4.9143e-01,  1.3420e-01,  9.4328e-01,\n",
      "         -1.3143e+00,  4.2821e-01, -1.7915e-01,  4.9626e-01,  7.3671e-01,\n",
      "          4.4984e-01,  7.0419e-01, -3.6833e-01, -2.9253e-01, -2.7497e+00,\n",
      "          4.3007e-01,  9.4046e-02, -7.2480e-01, -1.2139e+00,  7.1203e-01,\n",
      "         -1.4289e+00,  6.6008e-01,  4.0067e-01, -9.8105e-01,  3.4975e-01],\n",
      "        [ 3.1702e-01, -1.8543e+00,  1.0321e-01,  1.1027e+00, -4.4351e-01,\n",
      "          1.8149e-01, -4.9961e-01,  3.9391e-01, -1.8749e-01, -8.4503e-02,\n",
      "         -6.9388e-01, -7.0255e-01,  1.6219e+00,  1.0694e+00,  6.4395e-01,\n",
      "          1.6572e+00, -2.6741e-01,  2.7419e-01,  3.7588e-01,  1.2657e+00,\n",
      "         -1.0308e+00,  6.0980e-01,  5.1048e-01, -3.4186e-01,  1.2145e+00,\n",
      "         -3.4605e-01, -2.1938e-01,  1.0691e+00, -7.2637e-01,  2.1057e-01,\n",
      "         -3.0530e-01, -1.0611e+00, -1.0427e+00, -3.7988e-01, -2.0657e-01,\n",
      "          2.7658e-01,  1.8764e+00, -8.1805e-01,  1.5588e+00,  2.7005e-01,\n",
      "         -1.2675e+00, -9.8174e-01, -7.0765e-01,  4.0363e-01, -1.4430e+00,\n",
      "         -1.3827e-01, -8.5831e-01, -2.4039e-01,  1.8025e+00, -1.2725e-02],\n",
      "        [-1.2303e+00,  1.4799e+00,  9.4564e-01, -5.2794e-01,  4.6466e-02,\n",
      "         -8.8403e-01,  8.4733e-01, -8.8282e-01,  5.4403e-02, -1.2694e+00,\n",
      "         -1.8902e+00,  6.9568e-03,  7.2024e-01,  1.1469e+00, -2.0095e+00,\n",
      "         -1.4922e-01,  7.5467e-03,  1.5220e-01, -1.4137e+00,  2.4486e+00,\n",
      "         -1.7251e+00,  1.7138e+00, -3.1146e-02,  1.2561e+00, -6.1646e-02,\n",
      "          5.5024e-01, -9.2912e-01,  9.4755e-01, -1.0607e+00,  1.2579e-01,\n",
      "          4.3038e-01,  4.8560e-01,  7.0508e-01, -1.2365e+00, -7.8044e-01,\n",
      "          6.8428e-01,  9.2912e-01, -2.5755e+00, -1.1393e+00, -2.8052e-02,\n",
      "          9.7470e-01,  6.0230e-01,  7.6677e-01,  4.3549e-01, -2.9552e-01,\n",
      "         -1.6586e+00, -4.2115e-01,  9.3432e-01,  2.8530e-01,  2.8099e-01],\n",
      "        [-1.4327e+00,  1.0634e+00,  6.4098e-01,  9.6613e-01,  8.1167e-01,\n",
      "         -1.3513e+00,  1.9938e-01, -1.4697e-01,  6.6313e-01,  2.6513e-01,\n",
      "          1.9857e+00, -8.1816e-01,  5.4182e-02,  1.0232e+00,  9.8100e-01,\n",
      "          8.3875e-01,  3.5053e+00, -8.7069e-01, -5.3132e-02,  6.1547e-01,\n",
      "          2.8313e-01, -9.2270e-01,  2.3287e-01,  4.1638e-01,  1.5654e+00,\n",
      "         -8.9460e-01, -2.9508e-01,  3.4312e-01,  1.4881e+00,  7.8950e-01,\n",
      "         -1.5498e+00,  3.5747e-01, -1.8507e-01, -4.6125e-03,  4.6267e-01,\n",
      "          2.2070e+00, -1.4282e+00,  6.3622e-01,  3.6310e-01,  4.2971e-01,\n",
      "          1.5050e-01, -2.0387e+00, -1.1731e+00,  1.0395e+00,  1.0220e+00,\n",
      "         -1.6900e-01,  1.4259e+00,  6.0113e-01, -3.1402e-01, -3.2557e-02],\n",
      "        [ 8.2639e-01,  1.7319e-01, -2.1411e+00,  1.9844e+00,  1.0288e+00,\n",
      "          5.7888e-01, -2.5409e-02,  7.0917e-01, -3.9083e-01, -1.7144e+00,\n",
      "          7.3404e-02,  1.3797e+00,  3.9152e-01, -3.6778e-01,  7.5334e-01,\n",
      "         -1.0265e+00, -8.0075e-01, -1.8023e+00, -1.7615e-01,  8.0709e-02,\n",
      "          8.2330e-03, -5.8378e-01,  1.5321e+00, -3.2513e-01,  7.4595e-02,\n",
      "          1.2314e+00, -5.3958e-01,  1.4845e-01, -1.9372e-01,  1.1276e+00,\n",
      "         -4.1253e-01, -1.0024e+00, -1.0823e+00,  9.6449e-02, -1.6802e+00,\n",
      "         -3.0777e-01, -5.5811e-01, -6.6774e-01, -2.3473e-01,  7.3068e-01,\n",
      "          3.8018e-01, -7.3093e-01, -2.7298e-02,  6.2442e-01,  1.4322e+00,\n",
      "         -5.7515e-01, -1.1140e+00, -1.7245e+00,  2.9913e-01, -2.7203e-01],\n",
      "        [ 3.4629e-01,  8.7934e-01,  6.4533e-01, -1.9387e+00, -7.4939e-01,\n",
      "         -9.5831e-01, -8.5661e-01,  2.0942e-03,  3.6808e-01,  1.8445e+00,\n",
      "          2.4402e-01,  7.4069e-01, -2.5268e-01,  7.8468e-02, -2.6433e-01,\n",
      "         -8.9010e-01, -1.6462e+00, -1.1431e+00, -1.3087e+00, -8.7752e-01,\n",
      "          8.2047e-01, -4.1113e-01,  3.1149e-01,  2.9866e-01, -8.6708e-01,\n",
      "          3.6104e-01,  1.8444e+00,  1.1533e+00,  7.2510e-01, -1.6468e+00,\n",
      "          2.8544e+00, -1.2397e-01,  2.4767e-01, -1.3756e+00, -1.1387e+00,\n",
      "         -2.2818e+00,  2.7093e-02, -9.9056e-01,  1.9482e+00, -1.8654e-01,\n",
      "          7.7565e-01, -9.1664e-03,  1.7520e+00, -1.4748e+00, -3.2317e+00,\n",
      "         -7.3682e-01, -3.3504e-01, -2.7732e-01, -1.0262e+00, -1.7822e+00],\n",
      "        [-4.1990e-01,  6.4952e-01, -2.2711e-01, -2.8818e-01,  2.4474e+00,\n",
      "          3.3509e-01, -5.7916e-01,  2.0084e+00,  1.0572e-01,  8.0953e-01,\n",
      "          7.2657e-01, -2.3105e-01, -4.9629e-02, -1.7815e+00, -6.2457e-01,\n",
      "          5.2591e-01,  1.2305e+00, -1.2783e+00,  9.2464e-01,  7.5697e-01,\n",
      "          1.0529e+00, -8.9628e-02, -6.8566e-01,  3.2546e-01,  2.2851e+00,\n",
      "         -9.7288e-01, -8.7175e-02, -1.9356e+00,  1.6408e+00, -5.0863e-01,\n",
      "          3.3805e-01,  2.5812e-01,  7.2136e-01,  3.9424e-01,  1.8113e+00,\n",
      "          1.2918e+00, -6.7653e-01, -2.2338e+00, -1.4579e+00,  3.8604e-01,\n",
      "          1.1212e+00, -1.9959e+00,  1.4223e+00,  5.2255e-01,  1.7108e+00,\n",
      "         -1.2134e+00, -4.4563e-01, -5.2081e-01,  1.3619e+00, -9.2085e-01],\n",
      "        [-1.2303e+00,  1.4799e+00,  9.4564e-01, -5.2794e-01,  4.6466e-02,\n",
      "         -8.8403e-01,  8.4733e-01, -8.8282e-01,  5.4403e-02, -1.2694e+00,\n",
      "         -1.8902e+00,  6.9568e-03,  7.2024e-01,  1.1469e+00, -2.0095e+00,\n",
      "         -1.4922e-01,  7.5467e-03,  1.5220e-01, -1.4137e+00,  2.4486e+00,\n",
      "         -1.7251e+00,  1.7138e+00, -3.1146e-02,  1.2561e+00, -6.1646e-02,\n",
      "          5.5024e-01, -9.2912e-01,  9.4755e-01, -1.0607e+00,  1.2579e-01,\n",
      "          4.3038e-01,  4.8560e-01,  7.0508e-01, -1.2365e+00, -7.8044e-01,\n",
      "          6.8428e-01,  9.2912e-01, -2.5755e+00, -1.1393e+00, -2.8052e-02,\n",
      "          9.7470e-01,  6.0230e-01,  7.6677e-01,  4.3549e-01, -2.9552e-01,\n",
      "         -1.6586e+00, -4.2115e-01,  9.3432e-01,  2.8530e-01,  2.8099e-01],\n",
      "        [-2.3097e-01,  8.0984e-01, -1.2132e+00, -7.9918e-01, -5.1690e-01,\n",
      "         -6.0505e-01, -1.5951e+00,  6.1614e-01, -1.8394e+00, -2.4851e+00,\n",
      "          8.1724e-01,  3.4494e-02,  1.2860e-01, -8.3592e-01, -3.2638e-01,\n",
      "         -5.7772e-01, -1.1188e+00, -5.1052e-01, -3.2454e-02, -5.5704e-01,\n",
      "          6.1861e-01,  1.2450e+00,  1.1178e+00,  8.9218e-01, -4.4130e-01,\n",
      "          2.5200e-01,  9.2034e-01, -7.5581e-01,  4.4856e-01, -1.5526e+00,\n",
      "         -1.1723e+00, -7.5123e-01, -2.3902e-01,  3.3067e-01,  1.4869e+00,\n",
      "         -2.8446e-01, -3.0512e-01,  7.7971e-02, -1.0615e-01,  3.3957e-02,\n",
      "         -2.7742e+00, -3.3822e-02,  7.4769e-01,  1.0494e+00,  6.8211e-01,\n",
      "         -2.6398e-01, -9.5464e-01, -1.2045e+00, -1.7900e+00,  1.3091e+00],\n",
      "        [-5.6109e-01, -6.4883e-01,  6.1613e-01,  4.9075e-01,  4.4483e-01,\n",
      "          3.1964e-02, -2.5911e-01, -1.2130e+00,  9.9881e-01,  8.6801e-01,\n",
      "         -6.4011e-01, -8.5258e-01, -2.6947e-01,  7.8624e-01,  3.8721e-01,\n",
      "          2.1353e-01,  3.5406e-01,  1.2437e+00,  1.9389e+00,  4.2121e-01,\n",
      "          2.7953e-01, -9.6196e-01, -7.4057e-01,  4.6411e-01,  2.2588e+00,\n",
      "          2.0383e-01, -6.3638e-01, -1.4440e+00, -7.4808e-01, -2.6692e-01,\n",
      "         -3.9596e-01,  8.9374e-01,  6.9641e-01, -1.2680e-01,  8.0724e-01,\n",
      "          1.8433e+00, -9.5230e-01, -8.4266e-01,  1.1941e+00,  5.1096e-01,\n",
      "         -2.7639e-01, -1.6911e-01,  9.0800e-01,  1.2170e-01,  1.1128e+00,\n",
      "          2.1032e-01,  1.9471e+00,  4.0825e-02,  7.9023e-01, -1.1590e+00],\n",
      "        [-5.9045e-01,  1.4425e+00,  4.7693e-01,  9.4551e-02,  1.4487e+00,\n",
      "          3.0868e-01,  1.3530e+00,  5.7152e-01,  8.1081e-01,  7.4402e-01,\n",
      "         -1.5533e-02,  1.2785e+00, -4.3114e-01,  7.0307e-01,  4.7632e-01,\n",
      "          8.2535e-01, -3.0160e+00, -1.2686e+00,  1.1524e+00, -1.2985e+00,\n",
      "          9.9668e-01, -8.2894e-01,  8.7867e-01,  1.5807e-01, -2.9779e-01,\n",
      "         -2.6839e-01,  3.1170e-01,  3.6845e-01,  1.9701e+00, -1.2459e-02,\n",
      "          2.4447e+00,  7.7178e-01,  1.7323e+00, -5.0891e-01,  9.1223e-01,\n",
      "         -7.3073e-01,  5.5527e-01, -7.4792e-01,  1.5293e+00, -8.5541e-01,\n",
      "         -1.2111e-01,  8.6484e-01,  2.3143e-02,  2.4747e+00,  1.6570e+00,\n",
      "         -8.6233e-01,  5.6260e-01,  6.1009e-01, -5.4948e-02, -3.8743e-01],\n",
      "        [ 5.9345e-01,  3.3325e-01, -1.6928e+00,  1.1747e-01,  2.3175e-01,\n",
      "         -8.1829e-01,  2.7553e-01,  1.5388e-01, -5.1888e-01, -4.7810e-01,\n",
      "          1.0692e+00,  1.1491e+00,  8.4264e-01, -6.7629e-01, -6.1824e-01,\n",
      "         -2.3774e+00, -3.4738e-01, -1.4622e-01,  7.8018e-01, -2.1200e-01,\n",
      "         -2.3971e-01,  2.2076e+00, -8.4960e-01,  9.7394e-02, -1.5810e+00,\n",
      "         -6.9205e-01, -3.1874e+00, -9.7703e-01,  2.7938e+00, -4.4058e-01,\n",
      "          1.3721e-01, -6.1069e-01, -2.9405e-01, -1.1765e+00,  2.0726e-01,\n",
      "          1.5542e-01,  1.3324e+00, -9.9604e-01,  5.6711e-01,  1.2597e+00,\n",
      "         -2.2088e+00, -6.5507e-01,  9.8936e-01,  1.5588e-01, -1.6069e-01,\n",
      "         -9.4509e-01, -1.1980e+00, -5.1341e-01,  1.9580e+00, -9.4707e-01],\n",
      "        [-5.9045e-01,  1.4425e+00,  4.7693e-01,  9.4551e-02,  1.4487e+00,\n",
      "          3.0868e-01,  1.3530e+00,  5.7152e-01,  8.1081e-01,  7.4402e-01,\n",
      "         -1.5533e-02,  1.2785e+00, -4.3114e-01,  7.0307e-01,  4.7632e-01,\n",
      "          8.2535e-01, -3.0160e+00, -1.2686e+00,  1.1524e+00, -1.2985e+00,\n",
      "          9.9668e-01, -8.2894e-01,  8.7867e-01,  1.5807e-01, -2.9779e-01,\n",
      "         -2.6839e-01,  3.1170e-01,  3.6845e-01,  1.9701e+00, -1.2459e-02,\n",
      "          2.4447e+00,  7.7178e-01,  1.7323e+00, -5.0891e-01,  9.1223e-01,\n",
      "         -7.3073e-01,  5.5527e-01, -7.4792e-01,  1.5293e+00, -8.5541e-01,\n",
      "         -1.2111e-01,  8.6484e-01,  2.3143e-02,  2.4747e+00,  1.6570e+00,\n",
      "         -8.6233e-01,  5.6260e-01,  6.1009e-01, -5.4948e-02, -3.8743e-01],\n",
      "        [ 1.1757e+00, -1.4255e+00,  3.8521e-01,  9.1592e-02, -1.4898e+00,\n",
      "          2.2374e-01,  2.4062e-01,  3.8842e-01, -1.5243e+00, -1.5987e+00,\n",
      "          3.2847e-01,  3.0895e-01, -4.2695e-01,  6.2531e-01,  7.8772e-01,\n",
      "          5.3301e-01,  3.2754e-01,  7.1113e-02, -2.1675e+00,  9.2755e-02,\n",
      "          1.1179e+00,  3.9242e-01, -7.1463e-01, -1.0086e+00,  1.1544e+00,\n",
      "          7.9229e-01,  9.4759e-01,  7.8335e-01, -3.9145e-01, -4.9845e-01,\n",
      "         -3.2271e-01, -9.8117e-01,  1.5225e+00,  7.4199e-01, -8.4274e-02,\n",
      "         -1.8585e+00, -5.7062e-01, -3.9314e-02,  6.7454e-01,  4.2542e-01,\n",
      "         -1.8869e+00,  9.0030e-01, -6.2168e-01, -7.9052e-01, -8.7706e-01,\n",
      "          1.2387e+00,  4.0656e-01,  1.4623e+00,  1.1781e+00, -4.6894e-01]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the embedded data to verify\n",
    "print(\"Shape of embedded data:\", embedded_data.shape)\n",
    "\n",
    "# Print the first embedded sequence for verification\n",
    "print(\"First embedded sequence:\", embedded_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b6c9cc",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f6d2a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, model_dim, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "    \n",
    "        positional_encoding = np.zeros((max_sequence_length, model_dim))\n",
    "        \n",
    "        # calculating encoding for each position and dim\n",
    "        for pos in range(self.max_sequence_length):\n",
    "            for i in range(0, self.model_dim, 2):\n",
    "                # sin to even indices\n",
    "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.model_dim)))\n",
    "                \n",
    "                # cos to odd indices\n",
    "                if i + 1 < self.model_dim:\n",
    "                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / self.model_dim)))\n",
    "                    \n",
    "        \n",
    "        self.positional_encoding = torch.from_numpy(positional_encoding).unsqueeze(0).float()\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.positional_encoding[: x.size(1), :]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "266f7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding = PositionalEncoding(model_dim, max_sequence_length)\n",
    "encoded_data = pos_encoding(embedded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0a20c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoded data: torch.Size([100, 20, 50])\n",
      "First encoded sequence: tensor([[ 2.5216e+00, -3.8561e-01,  1.2489e+00, -4.1352e-01, -3.5567e-01,\n",
      "          1.2079e+00, -3.5718e-01,  1.1578e+00,  3.1399e-01, -6.4878e-01,\n",
      "         -2.5254e+00,  2.3124e+00, -1.1850e+00,  4.0226e-01, -1.0789e+00,\n",
      "          2.0215e+00, -1.9647e+00,  7.7468e-01, -1.0717e+00,  3.8881e-01,\n",
      "          8.8459e-01,  1.2458e+00, -2.2455e+00,  3.8134e-01, -4.6586e-02,\n",
      "          2.0210e+00, -5.7810e-01,  1.4405e+00,  1.1728e+00, -3.6915e-01,\n",
      "          9.0041e-01,  1.1673e+00,  4.7806e-01,  1.5137e+00,  1.7862e-01,\n",
      "          1.2337e+00,  1.0847e+00, -1.2002e+00,  9.4830e-01,  2.4117e+00,\n",
      "         -1.0248e+00,  1.1987e+00,  7.5797e-01,  7.3487e-01, -5.0675e-01,\n",
      "          1.5752e+00,  6.3405e-01, -1.3527e+00,  8.8564e-01,  1.5105e-01],\n",
      "        [ 4.2158e-01,  1.1898e+00,  2.3345e-01,  5.9945e-01,  2.6744e+00,\n",
      "          1.3090e+00, -4.6974e-01,  3.0024e+00,  1.5818e-01,  1.8082e+00,\n",
      "          7.5169e-01,  7.6864e-01, -3.7607e-02, -7.8157e-01, -6.1882e-01,\n",
      "          1.5259e+00,  1.2332e+00, -2.7831e-01,  9.2596e-01,  1.7570e+00,\n",
      "          1.0535e+00,  9.1037e-01, -6.8536e-01,  1.3255e+00,  2.2852e+00,\n",
      "          2.7121e-02, -8.7106e-02, -9.3561e-01,  1.6408e+00,  4.9137e-01,\n",
      "          3.3806e-01,  1.2581e+00,  7.2137e-01,  1.3942e+00,  1.8113e+00,\n",
      "          2.2918e+00, -6.7653e-01, -1.2338e+00, -1.4579e+00,  1.3860e+00,\n",
      "          1.1212e+00, -9.9589e-01,  1.4223e+00,  1.5226e+00,  1.7108e+00,\n",
      "         -2.1337e-01, -4.4563e-01,  4.7919e-01,  1.3619e+00,  7.9147e-02],\n",
      "        [ 1.7357e+00, -2.4296e-01, -1.3235e+00,  2.5602e+00,  1.4711e+00,\n",
      "          1.4757e+00,  1.9213e-01,  1.6852e+00, -2.8607e-01, -7.1993e-01,\n",
      "          1.2362e-01,  2.3785e+00,  4.1556e-01,  6.3193e-01,  7.6485e-01,\n",
      "         -2.6566e-02, -7.9525e-01, -8.0235e-01, -1.7351e-01,  1.0807e+00,\n",
      "          9.4949e-03,  4.1622e-01,  1.5327e+00,  6.7487e-01,  7.4884e-02,\n",
      "          2.2314e+00, -5.3944e-01,  1.1484e+00, -1.9365e-01,  2.1276e+00,\n",
      "         -4.1250e-01, -2.3750e-03, -1.0823e+00,  1.0964e+00, -1.6802e+00,\n",
      "          6.9223e-01, -5.5811e-01,  3.3226e-01, -2.3473e-01,  1.7307e+00,\n",
      "          3.8018e-01,  2.6907e-01, -2.7297e-02,  1.6244e+00,  1.4322e+00,\n",
      "          4.2485e-01, -1.1140e+00, -7.2447e-01,  2.9913e-01,  7.2797e-01],\n",
      "        [ 8.9586e-02, -2.6368e-01,  1.1317e+00, -1.0091e+00,  6.1503e-01,\n",
      "          2.1763e+00,  2.2202e+00,  1.8295e+00, -6.9687e-02,  9.3731e-01,\n",
      "          4.1232e-01, -2.0784e+00,  8.3875e-01,  2.6949e+00, -3.0601e-01,\n",
      "          5.6516e-01, -1.5745e+00,  1.6774e+00, -1.3649e+00,  3.8268e-01,\n",
      "         -1.4213e-01, -1.0059e+00, -2.4316e+00,  2.4404e+00,  8.5436e-01,\n",
      "          9.9463e-01, -6.2472e-01,  1.6396e+00, -4.2807e-01,  7.2767e-01,\n",
      "         -8.0316e-01, -9.9981e-04,  7.0498e-01,  6.1682e-01,  8.5681e-01,\n",
      "          2.0394e+00, -8.8443e-01,  1.6463e+00, -8.2567e-01,  1.8470e+00,\n",
      "          2.3632e-02,  3.9778e-01, -8.7082e-01, -4.4369e-01, -6.9764e-01,\n",
      "         -1.3571e-01,  4.6006e-01,  2.7804e-01, -4.4625e-01,  2.1828e+00],\n",
      "        [ 6.9587e-02, -4.8046e-01, -1.1996e+00,  1.6474e+00,  1.8222e+00,\n",
      "          1.1876e+00,  3.9926e-01,  1.6145e+00, -1.8245e-01, -7.3638e-01,\n",
      "          1.7371e-01,  2.3747e+00,  4.3959e-01,  6.3106e-01,  7.7636e-01,\n",
      "         -2.6765e-02, -7.8974e-01, -8.0240e-01, -1.7087e-01,  1.0807e+00,\n",
      "          1.0757e-02,  4.1622e-01,  1.5333e+00,  6.7486e-01,  7.5173e-02,\n",
      "          2.2314e+00, -5.3930e-01,  1.1484e+00, -1.9358e-01,  2.1276e+00,\n",
      "         -4.1247e-01, -2.3750e-03, -1.0823e+00,  1.0964e+00, -1.6802e+00,\n",
      "          6.9223e-01, -5.5810e-01,  3.3226e-01, -2.3472e-01,  1.7307e+00,\n",
      "          3.8018e-01,  2.6907e-01, -2.7297e-02,  1.6244e+00,  1.4322e+00,\n",
      "          4.2485e-01, -1.1140e+00, -7.2447e-01,  2.9913e-01,  7.2797e-01],\n",
      "        [-2.1892e+00,  1.7635e+00,  1.6261e+00, -1.2607e+00,  9.5736e-01,\n",
      "         -4.7138e-01,  1.3685e+00, -2.9379e-02,  3.1381e-01, -3.0364e-01,\n",
      "         -1.7650e+00,  9.9908e-01,  7.8032e-01,  2.1450e+00, -1.9807e+00,\n",
      "          8.5037e-01,  2.1317e-02,  1.1521e+00, -1.4071e+00,  3.4486e+00,\n",
      "         -1.7219e+00,  2.7138e+00, -2.9636e-02,  2.2561e+00, -6.0923e-02,\n",
      "          1.5502e+00, -9.2878e-01,  1.9476e+00, -1.0605e+00,  1.1258e+00,\n",
      "          4.3046e-01,  1.4856e+00,  7.0512e-01, -2.3646e-01, -7.8042e-01,\n",
      "          1.6843e+00,  9.2913e-01, -1.5755e+00, -1.1393e+00,  9.7195e-01,\n",
      "          9.7470e-01,  1.6023e+00,  7.6677e-01,  1.4355e+00, -2.9552e-01,\n",
      "         -6.5859e-01, -4.2115e-01,  1.9343e+00,  2.8530e-01,  1.2810e+00],\n",
      "        [-2.5758e+00,  1.2221e+00, -3.9058e-01,  1.3895e+00,  5.8050e-01,\n",
      "         -1.4143e+00,  1.4668e+00,  1.2936e+00, -2.1116e+00,  1.0844e+00,\n",
      "          5.8712e-01,  2.1301e+00, -5.3944e-01, -1.5410e-01,  9.2295e-02,\n",
      "          1.2288e+00, -3.1190e-01,  3.8096e-01,  9.0357e-01,  2.0107e+00,\n",
      "         -6.1869e-01,  9.1355e-01,  3.6391e-01,  1.6553e+00,  2.3522e+00,\n",
      "          1.0711e+00, -1.7970e-01,  1.4914e+00,  1.3440e-01,  1.9433e+00,\n",
      "         -1.3142e+00,  1.4282e+00, -1.7910e-01,  1.4963e+00,  7.3673e-01,\n",
      "          1.4498e+00,  7.0420e-01,  6.3167e-01, -2.9253e-01, -1.7497e+00,\n",
      "          4.3007e-01,  1.0940e+00, -7.2479e-01, -2.1387e-01,  7.1203e-01,\n",
      "         -4.2890e-01,  6.6008e-01,  1.4007e+00, -9.8105e-01,  1.3498e+00],\n",
      "        [ 9.7401e-01, -1.1004e+00, -1.0409e-01,  1.2441e-01,  5.5595e-01,\n",
      "          1.4868e-01,  1.9476e-01,  1.1135e+00,  1.7166e-01,  8.4877e-01,\n",
      "         -5.1895e-01,  2.8203e-01,  1.7060e+00,  2.0658e+00,  6.8422e-01,\n",
      "          2.6563e+00, -2.4813e-01,  1.2740e+00,  3.8511e-01,  2.2657e+00,\n",
      "         -1.0264e+00,  1.6098e+00,  5.1259e-01,  6.5814e-01,  1.2156e+00,\n",
      "          6.5395e-01, -2.1889e-01,  2.0691e+00, -7.2614e-01,  1.2106e+00,\n",
      "         -3.0519e-01, -6.1056e-02, -1.0426e+00,  6.2012e-01, -2.0655e-01,\n",
      "          1.2766e+00,  1.8764e+00,  1.8195e-01,  1.5588e+00,  1.2700e+00,\n",
      "         -1.2675e+00,  1.8261e-02, -7.0765e-01,  1.4036e+00, -1.4430e+00,\n",
      "          8.6173e-01, -8.5831e-01,  7.5961e-01,  1.8025e+00,  9.8728e-01],\n",
      "        [-2.4090e-01,  1.3344e+00,  3.1107e-01, -1.3008e+00,  1.0124e+00,\n",
      "         -1.1429e+00,  1.6163e+00, -2.4350e-01,  4.6202e-01, -3.5626e-01,\n",
      "         -1.6906e+00,  9.8683e-01,  8.1627e-01,  2.1422e+00, -1.9635e+00,\n",
      "          8.4972e-01,  2.9579e-02,  1.1520e+00, -1.4031e+00,  3.4486e+00,\n",
      "         -1.7200e+00,  2.7138e+00, -2.8731e-02,  2.2561e+00, -6.0489e-02,\n",
      "          1.5502e+00, -9.2857e-01,  1.9476e+00, -1.0604e+00,  1.1258e+00,\n",
      "          4.3051e-01,  1.4856e+00,  7.0514e-01, -2.3646e-01, -7.8041e-01,\n",
      "          1.6843e+00,  9.2913e-01, -1.5755e+00, -1.1393e+00,  9.7195e-01,\n",
      "          9.7470e-01,  1.6023e+00,  7.6677e-01,  1.4355e+00, -2.9552e-01,\n",
      "         -6.5859e-01, -4.2115e-01,  1.9343e+00,  2.8530e-01,  1.2810e+00],\n",
      "        [-1.0206e+00,  1.5225e-01, -2.7824e-01,  5.7237e-01,  1.6935e+00,\n",
      "         -1.8228e+00,  1.0337e+00,  4.0437e-01,  1.1181e+00,  1.1556e+00,\n",
      "          2.2099e+00,  1.5639e-01,  1.6217e-01,  2.0174e+00,  1.0328e+00,\n",
      "          1.8374e+00,  3.5301e+00,  1.2901e-01, -4.1268e-02,  1.6154e+00,\n",
      "          2.8881e-01,  7.7288e-02,  2.3559e-01,  1.4164e+00,  1.5667e+00,\n",
      "          1.0539e-01, -2.9446e-01,  1.3431e+00,  1.4884e+00,  1.7895e+00,\n",
      "         -1.5496e+00,  1.3575e+00, -1.8501e-01,  9.9539e-01,  4.6271e-01,\n",
      "          3.2070e+00, -1.4282e+00,  1.6362e+00,  3.6311e-01,  1.4297e+00,\n",
      "          1.5050e-01, -1.0387e+00, -1.1731e+00,  2.0395e+00,  1.0220e+00,\n",
      "          8.3100e-01,  1.4259e+00,  1.6011e+00, -3.1402e-01,  9.6744e-01],\n",
      "        [ 2.8237e-01, -6.6588e-01, -3.1384e+00,  2.0583e+00,  1.7805e+00,\n",
      "         -8.0555e-02,  8.6419e-01,  1.1659e+00,  1.1021e-01, -8.4900e-01,\n",
      "          3.2196e-01,  2.3484e+00,  5.1145e-01,  6.2500e-01,  8.1086e-01,\n",
      "         -2.8155e-02, -7.7322e-01, -8.0272e-01, -1.6296e-01,  1.0806e+00,\n",
      "          1.4542e-02,  4.1620e-01,  1.5351e+00,  6.7486e-01,  7.6040e-02,\n",
      "          2.2314e+00, -5.3889e-01,  1.1484e+00, -1.9338e-01,  2.1276e+00,\n",
      "         -4.1238e-01, -2.3750e-03, -1.0822e+00,  1.0964e+00, -1.6802e+00,\n",
      "          6.9223e-01, -5.5809e-01,  3.3226e-01, -2.3472e-01,  1.7307e+00,\n",
      "          3.8019e-01,  2.6907e-01, -2.7296e-02,  1.6244e+00,  1.4322e+00,\n",
      "          4.2485e-01, -1.1140e+00, -7.2447e-01,  2.9913e-01,  7.2797e-01],\n",
      "        [-6.5370e-01,  8.8377e-01, -2.0586e-01, -1.4138e+00, -1.6702e-01,\n",
      "         -1.7712e+00,  7.7633e-02,  3.5874e-01,  9.1383e-01,  2.6824e+00,\n",
      "          5.1683e-01,  1.7028e+00, -1.2081e-01,  1.0697e+00, -2.0107e-01,\n",
      "          1.0789e-01, -1.6159e+00, -1.4355e-01, -1.2942e+00,  1.2237e-01,\n",
      "          8.2741e-01,  5.8884e-01,  3.1481e-01,  1.2987e+00, -8.6549e-01,\n",
      "          1.3610e+00,  1.8451e+00,  2.1533e+00,  7.2547e-01, -6.4676e-01,\n",
      "          2.8546e+00,  8.7603e-01,  2.4776e-01, -3.7561e-01, -1.1387e+00,\n",
      "         -1.2818e+00,  2.7112e-02,  9.4448e-03,  1.9482e+00,  8.1346e-01,\n",
      "          7.7565e-01,  9.9083e-01,  1.7520e+00, -4.7481e-01, -3.2317e+00,\n",
      "          2.6318e-01, -3.3504e-01,  7.2268e-01, -1.0262e+00, -7.8224e-01],\n",
      "        [-9.5647e-01,  1.4934e+00, -7.4092e-01,  5.6972e-01,  2.8299e+00,\n",
      "         -5.8885e-01,  3.8849e-01,  2.2606e+00,  6.9468e-01,  1.6177e+00,\n",
      "          1.0235e+00,  7.2387e-01,  9.4143e-02, -7.9189e-01, -5.5557e-01,\n",
      "          1.5235e+00,  1.2635e+00, -2.7886e-01,  9.4046e-01,  1.7568e+00,\n",
      "          1.0605e+00,  9.1034e-01, -6.8204e-01,  1.3255e+00,  2.2868e+00,\n",
      "          2.7120e-02, -8.6345e-02, -9.3561e-01,  1.6412e+00,  4.9137e-01,\n",
      "          3.3824e-01,  1.2581e+00,  7.2145e-01,  1.3942e+00,  1.8114e+00,\n",
      "          2.2918e+00, -6.7651e-01, -1.2338e+00, -1.4579e+00,  1.3860e+00,\n",
      "          1.1212e+00, -9.9589e-01,  1.4223e+00,  1.5226e+00,  1.7108e+00,\n",
      "         -2.1337e-01, -4.4563e-01,  4.7919e-01,  1.3619e+00,  7.9147e-02],\n",
      "        [-8.1009e-01,  2.3873e+00,  8.8468e-01,  4.7020e-01,  2.0920e-01,\n",
      "         -1.8707e+00,  1.8368e+00, -7.3796e-01,  6.8494e-01, -4.9325e-01,\n",
      "         -1.5695e+00,  9.5411e-01,  8.7590e-01,  2.1347e+00, -1.9348e+00,\n",
      "          8.4799e-01,  4.3344e-02,  1.1516e+00, -1.3965e+00,  3.4485e+00,\n",
      "         -1.7169e+00,  2.7138e+00, -2.7221e-02,  2.2561e+00, -5.9767e-02,\n",
      "          1.5502e+00, -9.2822e-01,  1.9476e+00, -1.0603e+00,  1.1258e+00,\n",
      "          4.3059e-01,  1.4856e+00,  7.0518e-01, -2.3646e-01, -7.8040e-01,\n",
      "          1.6843e+00,  9.2914e-01, -1.5755e+00, -1.1393e+00,  9.7195e-01,\n",
      "          9.7471e-01,  1.6023e+00,  7.6677e-01,  1.4355e+00, -2.9552e-01,\n",
      "         -6.5859e-01, -4.2115e-01,  1.9343e+00,  2.8530e-01,  1.2810e+00],\n",
      "        [ 7.5964e-01,  9.4657e-01, -8.0756e-01,  1.1487e-01, -5.8247e-01,\n",
      "         -1.6029e+00, -5.9570e-01,  6.5186e-01, -1.1690e+00, -1.7431e+00,\n",
      "          1.1617e+00,  9.7330e-01,  2.9612e-01,  1.4995e-01, -2.4590e-01,\n",
      "          4.1904e-01, -1.0802e+00,  4.8874e-01, -1.3999e-02,  4.4279e-01,\n",
      "          6.2744e-01,  2.2450e+00,  1.1221e+00,  1.8922e+00, -4.3927e-01,\n",
      "          1.2520e+00,  9.2131e-01,  2.4419e-01,  4.4902e-01, -5.5264e-01,\n",
      "         -1.1721e+00,  2.4877e-01, -2.3891e-01,  1.3307e+00,  1.4870e+00,\n",
      "          7.1554e-01, -3.0509e-01,  1.0780e+00, -1.0613e-01,  1.0340e+00,\n",
      "         -2.7742e+00,  9.6618e-01,  7.4769e-01,  2.0494e+00,  6.8211e-01,\n",
      "          7.3602e-01, -9.5464e-01, -2.0445e-01, -1.7900e+00,  2.3091e+00],\n",
      "        [ 8.9198e-02, -1.4085e+00,  1.3971e+00,  1.1153e+00,  1.5437e-01,\n",
      "         -9.2492e-01,  7.3816e-01, -1.2869e+00,  1.7072e+00,  1.5738e+00,\n",
      "         -2.7218e-01,  7.7275e-02, -9.0111e-02,  1.7700e+00,  4.7342e-01,\n",
      "          1.2098e+00,  3.9537e-01,  2.2428e+00,  1.9587e+00,  1.4210e+00,\n",
      "          2.8899e-01,  3.7999e-02, -7.3604e-01,  1.4641e+00,  2.2610e+00,\n",
      "          1.2038e+00, -6.3534e-01, -4.4403e-01, -7.4758e-01,  7.3308e-01,\n",
      "         -3.9572e-01,  1.8937e+00,  6.9652e-01,  8.7320e-01,  8.0730e-01,\n",
      "          2.8433e+00, -9.5227e-01,  1.5734e-01,  1.1941e+00,  1.5110e+00,\n",
      "         -2.7638e-01,  8.3089e-01,  9.0800e-01,  1.1217e+00,  1.1128e+00,\n",
      "          1.2103e+00,  1.9471e+00,  1.0408e+00,  7.9023e-01, -1.5900e-01],\n",
      "        [-8.7835e-01,  4.8480e-01,  1.4578e+00,  2.8920e-01,  9.4856e-01,\n",
      "         -5.5725e-01,  2.3362e+00,  3.8898e-01,  1.5552e+00,  1.4117e+00,\n",
      "          3.7564e-01,  2.1988e+00, -2.3996e-01,  1.6846e+00,  5.6826e-01,\n",
      "          1.8211e+00, -2.9719e+00, -2.6957e-01,  1.1735e+00, -2.9872e-01,\n",
      "          1.0068e+00,  1.7101e-01,  8.8350e-01,  1.1581e+00, -2.9548e-01,\n",
      "          7.3161e-01,  3.1281e-01,  1.3684e+00,  1.9706e+00,  9.8754e-01,\n",
      "          2.4450e+00,  1.7718e+00,  1.7324e+00,  4.9109e-01,  9.1228e-01,\n",
      "          2.6927e-01,  5.5530e-01,  2.5208e-01,  1.5293e+00,  1.4459e-01,\n",
      "         -1.2111e-01,  1.8648e+00,  2.3146e-02,  3.4747e+00,  1.6570e+00,\n",
      "          1.3767e-01,  5.6260e-01,  1.6101e+00, -5.4947e-02,  6.1257e-01],\n",
      "        [-3.6795e-01,  5.8085e-02, -7.3255e-01, -1.6151e-01, -4.5199e-01,\n",
      "         -1.5480e+00,  1.2328e+00, -1.3515e-01,  2.5956e-01,  1.4963e-01,\n",
      "          1.4834e+00,  2.0593e+00,  1.0456e+00,  3.0290e-01, -5.2057e-01,\n",
      "         -1.3822e+00, -3.0058e-01,  8.5268e-01,  8.0259e-01,  7.8775e-01,\n",
      "         -2.2898e-01,  3.2076e+00, -8.4447e-01,  1.0974e+00, -1.5785e+00,\n",
      "          3.0795e-01, -3.1862e+00,  2.2971e-02,  2.7944e+00,  5.5942e-01,\n",
      "          1.3748e-01,  3.8931e-01, -2.9393e-01, -1.7646e-01,  2.0732e-01,\n",
      "          1.1554e+00,  1.3324e+00,  3.9572e-03,  5.6712e-01,  2.2597e+00,\n",
      "         -2.2088e+00,  3.4493e-01,  9.8936e-01,  1.1559e+00, -1.6069e-01,\n",
      "          5.4911e-02, -1.1980e+00,  4.8659e-01,  1.9580e+00,  5.2925e-02],\n",
      "        [-1.3414e+00,  2.1028e+00,  1.2008e+00, -5.9536e-01,  6.1713e-01,\n",
      "         -2.4671e-01,  2.2730e+00,  1.7946e-01,  1.6211e+00,  1.3300e+00,\n",
      "          4.2136e-01,  2.1780e+00, -2.1641e-01,  1.6797e+00,  5.7971e-01,\n",
      "          1.8200e+00, -2.9664e+00, -2.6982e-01,  1.1761e+00, -2.9878e-01,\n",
      "          1.0080e+00,  1.7099e-01,  8.8410e-01,  1.1581e+00, -2.9519e-01,\n",
      "          7.3161e-01,  3.1294e-01,  1.3684e+00,  1.9707e+00,  9.8754e-01,\n",
      "          2.4450e+00,  1.7718e+00,  1.7324e+00,  4.9109e-01,  9.1229e-01,\n",
      "          2.6927e-01,  5.5530e-01,  2.5208e-01,  1.5293e+00,  1.4459e-01,\n",
      "         -1.2111e-01,  1.8648e+00,  2.3146e-02,  3.4747e+00,  1.6570e+00,\n",
      "          1.3767e-01,  5.6260e-01,  1.6101e+00, -5.4947e-02,  6.1257e-01],\n",
      "        [ 1.3256e+00, -4.3680e-01,  7.1002e-01, -8.5419e-01, -2.4258e+00,\n",
      "         -1.2829e-01,  1.1121e+00, -1.0195e-01, -6.8434e-01, -1.0560e+00,\n",
      "          7.8781e-01,  1.1972e+00, -2.0050e-01,  1.5993e+00,  8.9684e-01,\n",
      "          1.5270e+00,  3.7985e-01,  1.0697e+00, -2.1425e+00,  1.0924e+00,\n",
      "          1.1299e+00,  1.3923e+00, -7.0889e-01, -8.6437e-03,  1.1571e+00,\n",
      "          1.7923e+00,  9.4890e-01,  1.7833e+00, -3.9082e-01,  5.0155e-01,\n",
      "         -3.2241e-01,  1.8832e-02,  1.5226e+00,  1.7420e+00, -8.4205e-02,\n",
      "         -8.5851e-01, -5.7058e-01,  9.6069e-01,  6.7456e-01,  1.4254e+00,\n",
      "         -1.8869e+00,  1.9003e+00, -6.2167e-01,  2.0948e-01, -8.7706e-01,\n",
      "          2.2387e+00,  4.0656e-01,  2.4623e+00,  1.1781e+00,  5.3106e-01]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the encoded data to verify\n",
    "print(\"Shape of encoded data:\", encoded_data.shape)\n",
    "\n",
    "# Print the first encoded sequence for verification\n",
    "print(\"First encoded sequence:\", encoded_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9242bf15",
   "metadata": {},
   "source": [
    "## Masking and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99c5ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, embedding_dimension, head_dimension):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.head_dimension = head_dimension\n",
    "        \n",
    "        self.query_layer = torch.nn.Linear(self.embedding_dimension, self.head_dimension)\n",
    "        self.key_layer = torch.nn.Linear(self.embedding_dimension, self.head_dimension)\n",
    "        self.value_layer = torch.nn.Linear(self.embedding_dimension, self.head_dimension)\n",
    "        self.softmax = torch.nn.Softmax(dim = -1)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # x dim - (batch_size, sequence_length, embedding_dim)\n",
    "        # mask dim - (batch_size, sequence_length, head_dim)\n",
    "        # output dim - (batch_size, sequence_length)\n",
    "        \n",
    "        query = self.query_layer(x)\n",
    "        key = self.key_layer(x)\n",
    "        value = self.value_layer(x)\n",
    "        \n",
    "        # calculating attention weights and scaling\n",
    "        attention_weights = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(self.head_dimension)\n",
    "        \n",
    "        # masking\n",
    "        if mask is not None:\n",
    "            attention_weights = attention_weights.masked_filled(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_scores = self.softmax(attention_weights)\n",
    "        return torch.matmul(attention_scores, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6936347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadedSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, embedding_dimension, num_heads):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.head_dimension = embedding_dimension // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.self_attentions = torch.nn.ModuleList(\n",
    "            [MaskedSelfAttention(embedding_dimension, self.head_dimension) for _ in range(self.num_heads)]\n",
    "        )\n",
    "        \n",
    "        self.output_layer = torch.nn.Linear(self.num_heads * self.head_dimension, self.embedding_dimension)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        self_attention_outputs = [self_attention(x, mask) for self_attention in self.self_attentions]\n",
    "        \n",
    "        # concatenating outputs\n",
    "        concatenated_outputs = torch.cat(self_attention_outputs, dim = 2)\n",
    "        return self.output_layer(concatenated_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e8df0d",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35723d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, feed_forward_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.multi_attention = MaskedMultiHeadedSelfAttention(embedding_dim, num_heads)\n",
    "        self.feed_forward = FeedForward(embedding_dim, feed_forward_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm_2 = torch.nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x_norm = self.layer_norm_1(x)\n",
    "        attention_output = self.multi_attention(x_norm, mask)\n",
    "        residual_output = x + attention_output\n",
    "        residual_output_norm = self.layer_norm_2(residual_output)\n",
    "        \n",
    "        feed_forward_output = self.feed_forward(residual_output_norm)\n",
    "        \n",
    "        if self.training:\n",
    "            feed_forward_output = self.dropout(feed_forward_output)\n",
    "            \n",
    "        return residual_output + feed_forward_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1707d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, num_layers, num_heads, feed_forward_dim, dropout_rate, max_sequence_length):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.decoder_layers = torch.nn.ModuleList(\n",
    "            [DecoderLayer(embedding_dim, num_heads, feed_forward_dim, dropout_rate) for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        outputs = x\n",
    "        for layer in self.decoder_layers:\n",
    "            outputs = layer(outputs, mask)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "547725c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, feed_forward_dim):\n",
    "        super().__init__()\n",
    "        self.linear_1 = torch.nn.Linear(embedding_dim, feed_forward_dim)\n",
    "        self.linear_2 = torch.nn.Linear(feed_forward_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13da614c",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09766e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(torch.nn.Module):\n",
    "    def __init__(self, num_tokens, max_sequence_length = 100, embedding_dim = 512, num_layers = 6, num_heads = 4, feed_forward_dim = None, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        if feed_forward_dim is None:\n",
    "            self.feed_forward_dim = embedding_dim * 4\n",
    "        else:\n",
    "            self.feed_forward_dim = feed_forward_dim\n",
    "        \n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.token_embedding = TokenEmbedding(embedding_dim, num_tokens)\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim, max_sequence_length)\n",
    "        self.layer_norm = torch.nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        self.decoder = DecoderStack(embedding_dim, num_layers, num_heads, feed_forward_dim, dropout_rate, max_sequence_length)\n",
    "        self.generator_head = GeneratorHead(embedding_dim, num_tokens)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        token_embedding = self.token_embedding(x)\n",
    "        positional_encoding = self.positional_encoding(token_embedding)\n",
    "        positional_encoding_norm = self.layer_norm(positional_encoding)\n",
    "        decoder_outputs = self.decoder(positional_encoding_norm, mask)\n",
    "        generator_outputs = self.generator_head(decoder_outputs)\n",
    "        \n",
    "        return generator_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8526c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorHead(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, num_tokens):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(embedding_dim, num_tokens)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd66a493",
   "metadata": {},
   "source": [
    "## Autoregressive Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e046b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        inputs, targets = x[:, :-1], x[:, 1:]\n",
    "        mask = mask[:, :-1]\n",
    "        \n",
    "        output = self.model(inputs, mask)\n",
    "        return output, targets\n",
    "    \n",
    "    def next_token_probabilities(self, x, mask, temperature = 1.0):\n",
    "        logits = self.model(x, mask)[:, -1]\n",
    "        \n",
    "        if temperature != 1.0:\n",
    "            logits /= temperature\n",
    "        \n",
    "        probabilities = torch.softmax(logits, dim = -1)\n",
    "        \n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafb117",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "616a9277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, tokenizer: Tokenizer, optimizer = None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        if optimizer is None:\n",
    "            self.optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def train(self, data, epochs, batch_size):\n",
    "        loss_epoch = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            random.shuffle(data)\n",
    "            \n",
    "            batches = []\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                sequence = torch.tensor(data[i: i + batch_size], dtype = torch.long)\n",
    "                mask_tensor = torch.ones_like(sequence)\n",
    "                mask_tensor[sequence == self.tokenizer.character_to_token('<pad>')] = 0\n",
    "                \n",
    "                batches.append((sequence, mask_tensor))\n",
    "                \n",
    "            for batch in batches:\n",
    "                self.model.train()\n",
    "                \n",
    "                input_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype = torch.long)\n",
    "                mask_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype = torch.long)\n",
    "                \n",
    "                for i, inp in enumerate(batch[0]):\n",
    "                    input_tensor[i] = inp\n",
    "                \n",
    "                for i, mask in enumerate(batch[1]):\n",
    "                    mask_tensor[i] = mask\n",
    "                    \n",
    "                model_output, target = self.model.forward(x = input_tensor, mask = mask_tensor)\n",
    "                \n",
    "                loss = self.loss_function(model.output.transpose(1, 2), target)\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                losses.append(loss.item())\n",
    "                \n",
    "            epoch_loss = np.average(losses)\n",
    "            loss_epoch.append(epoch_loss)\n",
    "            print(f\"Epoch: {epoch}, Loss: {epoch_loss}\")\n",
    "        \n",
    "        return loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af952b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4a4ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30c766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b55e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4010b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0972c19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6390a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92239ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff60f9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2f715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5fd2de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
