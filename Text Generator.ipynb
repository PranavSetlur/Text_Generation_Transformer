{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a2e19d",
   "metadata": {},
   "source": [
    "# Text Generator\n",
    "Implementing a text generation model from scratch using a transformer (decoder only).\\\n",
    "Steps:\n",
    "1. Tokenization\n",
    "2. Input embedding\n",
    "3. Positional encoding\n",
    "4. Masking\n",
    "5. Self-attention\n",
    "6. Decoder stack\n",
    "7. Predicting token probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b58e15",
   "metadata": {},
   "source": [
    "## Creating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b1cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install pytorch torchvision torchaudio -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa7f1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6424458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class creating_data():\n",
    "    def __init__(self, filepath):\n",
    "        self.df = pd.read_csv(filepath)\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.df.to_csv(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea755bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = creating_data('medium_articles.csv')\n",
    "# dataset.save('training_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46dee6c",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5068fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        self.dictionary = {}\n",
    "        self.reverse_dictionary = {}\n",
    "        \n",
    "        # adding special tokens\n",
    "        self.__add_to_dict('<pad>')\n",
    "        self.__add_to_dict('<unk>')\n",
    "        \n",
    "        # add characters and numbers to dictionary\n",
    "        for i in range(10):\n",
    "            self.__add_to_dict(str(i))\n",
    "        \n",
    "        for i in range(26):\n",
    "            self.__add_to_dict(chr(ord('a') + i))\n",
    "            self.__add_to_dict(chr(ord('A') + i))\n",
    "            \n",
    "        # adding space and punctuation\n",
    "        for char in ['.', ' ', ',', '!', '?', '\\n']:\n",
    "            self.__add_to_dict(char)\n",
    "        \n",
    "    def __add_to_dict(self, character):\n",
    "        if character not in self.dictionary:\n",
    "            index = self.size()\n",
    "            self.dictionary[character] = index\n",
    "            self.reverse_dictionary[index] = character\n",
    "            \n",
    "    def tokenize(self, text):\n",
    "        return [self.dictionary.get(c, self.dictionary['<unk>']) for c in text]\n",
    "    \n",
    "    def character_to_token(self, character):\n",
    "        return self.dictionary.get(character, self.dictionary['<unk>'])\n",
    "    \n",
    "    def token_to_character(self, token):\n",
    "        return self.reverse_dictionary.get(token, '<unk>')\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e166c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('training_data.csv')\n",
    "training_data = training_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75dbb597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...\n",
       "1    Your Brain On Coronavirus\\n\\nA guide to the cu...\n",
       "2    Mind Your Nose\\n\\nHow smell training can chang...\n",
       "3    Passionate about the synergy between science a...\n",
       "4    You’ve heard of him, haven’t you? Phineas Gage...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4187758",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9938290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenized_data = [tokenizer.tokenize(sentence) for sentence in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c4a2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 20\n",
    "# padding and truncating\n",
    "padded_data = []\n",
    "\n",
    "for tokens in tokenized_data:\n",
    "    if len(tokens) < max_sequence_length:\n",
    "        # padding\n",
    "        tokens = [tokenizer.character_to_token('<pad>')] * (max_sequence_length - len(tokens)) + tokens\n",
    "    else:\n",
    "        # truncating\n",
    "        tokens = tokens[:max_sequence_length]\n",
    "    padded_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8be394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting data to tensors\n",
    "tensor_data = [torch.tensor(tokens) for tokens in padded_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d9d4c",
   "metadata": {},
   "source": [
    "## Input Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "113d5547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(torch.nn.Module):\n",
    "    # model that converts tokens into embeddings\n",
    "    \n",
    "    def __init__(self, model_dim, num_tokens):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = torch.nn.Embedding(\n",
    "            num_embeddings = num_tokens,\n",
    "            embedding_dim = model_dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58fb2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim = 50\n",
    "num_tokens = tokenizer.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eec917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing class\n",
    "embedding_model = TokenEmbedding(model_dim, num_tokens)\n",
    "# convert padded data to tensor\n",
    "tensor_data = torch.stack(tensor_data)\n",
    "embedded_data = embedding_model(tensor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f9a3f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedded data: torch.Size([100, 20, 50])\n",
      "First embedded sequence: tensor([[-1.1822e-01,  4.6802e-02, -1.6400e+00,  1.7318e+00, -2.9525e-01,\n",
      "          5.9873e-01, -2.4417e+00, -2.0057e+00, -6.0042e-01, -3.5858e-01,\n",
      "         -2.5327e-01,  4.0275e-01, -9.7319e-01, -2.7432e-01, -2.8670e-01,\n",
      "          6.1110e-01,  1.5894e+00,  8.1117e-01,  1.3971e+00,  9.6296e-01,\n",
      "          2.9800e-01, -3.9640e-01,  5.0827e-01, -1.4298e+00, -3.9643e-02,\n",
      "          1.6600e+00,  3.1453e+00, -1.6991e+00,  1.4632e+00, -4.4402e-01,\n",
      "          1.3571e-01, -1.1809e+00, -7.5177e-01, -8.1230e-01,  8.2789e-02,\n",
      "         -3.5891e-01, -7.5696e-02,  1.4842e-01,  1.2437e+00, -1.4303e-01,\n",
      "         -9.9537e-01, -7.1999e-01, -1.9865e+00,  4.1810e-01,  1.1808e+00,\n",
      "          6.1096e-01,  3.2342e-01, -6.0698e-01,  1.6589e-01, -4.8454e-01],\n",
      "        [-1.1566e+00, -5.7672e-01, -1.3489e+00, -2.1346e+00,  8.1467e-02,\n",
      "          1.1855e-01, -1.3347e+00,  1.2347e+00,  6.8216e-01,  2.4245e+00,\n",
      "          1.2339e+00, -1.3140e-01, -7.2360e-01, -1.0818e+00,  5.2465e-01,\n",
      "         -8.1128e-01,  2.1109e+00,  1.9335e+00,  2.0239e+00, -1.9529e-01,\n",
      "         -1.1146e+00, -1.3288e+00, -3.2032e-01, -4.3595e-01, -4.3279e-01,\n",
      "          1.4515e+00,  1.1961e+00,  5.2661e-01,  1.2331e+00, -4.7658e-01,\n",
      "         -5.9056e-01, -1.4326e-01,  9.9450e-01,  2.1075e-01,  8.2478e-01,\n",
      "          2.8807e-01,  3.3487e-01, -4.4131e-01, -7.4040e-01, -2.9621e-01,\n",
      "         -4.8741e-01, -8.0173e-01, -1.5895e-01, -4.0488e-01, -1.6584e+00,\n",
      "         -7.0529e-01,  7.9027e-01, -1.5692e+00,  6.1506e-01,  5.7544e-02],\n",
      "        [ 1.9984e-01,  6.1101e-01, -4.5994e-01,  3.5860e-01, -8.9364e-01,\n",
      "         -6.4595e-01,  6.8794e-01, -4.9265e-01, -3.7653e-01,  5.6537e-01,\n",
      "          3.8345e-01, -1.4023e-01,  2.0974e+00, -6.1529e-01,  1.2149e+00,\n",
      "         -8.1955e-01,  1.3353e+00, -1.4502e+00, -1.0299e+00, -3.8271e-01,\n",
      "          1.9979e-01,  1.1863e+00,  4.7864e-01,  1.5374e+00,  1.1530e+00,\n",
      "         -1.2379e+00,  9.9589e-01,  4.4977e-01, -2.5882e-01, -2.2869e-01,\n",
      "         -5.8700e-02, -1.3928e+00,  1.5308e+00,  1.2451e+00,  1.2878e+00,\n",
      "         -1.0114e+00, -1.8107e-02,  1.6963e+00, -1.1311e+00,  1.6896e+00,\n",
      "          4.4827e-01, -4.7763e-01, -1.1587e+00,  1.0919e+00,  9.3702e-02,\n",
      "          1.0489e+00,  7.7329e-01,  1.1162e+00,  7.6647e-01,  2.3435e-01],\n",
      "        [ 1.2844e+00, -5.4353e-01, -1.6801e+00, -1.0030e+00, -6.4565e-01,\n",
      "         -2.6276e-01,  8.3336e-01, -2.7470e-01, -9.2148e-01, -1.2977e+00,\n",
      "          1.4772e-01, -1.8924e+00, -1.6434e+00,  3.0460e+00, -7.7443e-01,\n",
      "          1.8254e-01,  1.3080e-01,  1.1753e-01,  1.2530e+00, -1.0612e+00,\n",
      "         -1.0303e+00, -1.1981e+00,  5.2989e-01,  3.6248e-01,  6.7021e-02,\n",
      "          1.4151e-01,  1.0854e+00,  4.2442e-01, -1.2463e+00, -1.0263e+00,\n",
      "         -1.4939e+00,  1.0916e+00, -5.3867e-01, -6.7261e-01, -1.3889e+00,\n",
      "          4.7476e-01,  1.1667e+00,  1.7122e+00, -2.0236e+00,  1.8932e+00,\n",
      "         -1.1176e-01,  1.6919e-01, -8.8170e-01,  4.2046e-01,  1.3595e-01,\n",
      "         -1.9986e+00,  6.9741e-01, -1.7417e+00, -3.0279e-01,  1.1992e+00],\n",
      "        [ 1.9984e-01,  6.1101e-01, -4.5994e-01,  3.5860e-01, -8.9364e-01,\n",
      "         -6.4595e-01,  6.8794e-01, -4.9265e-01, -3.7653e-01,  5.6537e-01,\n",
      "          3.8345e-01, -1.4023e-01,  2.0974e+00, -6.1529e-01,  1.2149e+00,\n",
      "         -8.1955e-01,  1.3353e+00, -1.4502e+00, -1.0299e+00, -3.8271e-01,\n",
      "          1.9979e-01,  1.1863e+00,  4.7864e-01,  1.5374e+00,  1.1530e+00,\n",
      "         -1.2379e+00,  9.9589e-01,  4.4977e-01, -2.5882e-01, -2.2869e-01,\n",
      "         -5.8700e-02, -1.3928e+00,  1.5308e+00,  1.2451e+00,  1.2878e+00,\n",
      "         -1.0114e+00, -1.8107e-02,  1.6963e+00, -1.1311e+00,  1.6896e+00,\n",
      "          4.4827e-01, -4.7763e-01, -1.1587e+00,  1.0919e+00,  9.3702e-02,\n",
      "          1.0489e+00,  7.7329e-01,  1.1162e+00,  7.6647e-01,  2.3435e-01],\n",
      "        [-8.8094e-02, -2.0012e+00, -5.2095e-01, -2.2508e-01, -6.9639e-01,\n",
      "          5.5729e-01,  1.8053e-01,  1.8104e+00,  1.4280e+00,  7.8604e-02,\n",
      "         -5.7302e-01, -7.1067e-01, -5.0283e-01,  1.2438e+00, -1.0876e+00,\n",
      "         -1.8058e+00, -2.0741e-01,  1.5244e+00,  3.7289e-01, -1.1477e-01,\n",
      "         -2.0493e+00, -3.8060e-01,  9.9798e-01,  2.8153e-01, -1.5778e+00,\n",
      "          9.1334e-01,  5.5482e-01, -1.7109e+00, -1.8993e+00, -1.3937e+00,\n",
      "          5.3219e-01, -4.1158e-01, -2.2620e-02, -8.9919e-01, -1.0065e+00,\n",
      "         -1.5487e+00, -1.1775e+00,  5.7722e-01,  1.1321e-01,  1.7976e+00,\n",
      "          8.0679e-01,  4.8508e-02, -7.3651e-03, -1.4009e-01, -3.4750e+00,\n",
      "          8.9731e-01, -7.4795e-01, -2.1456e+00, -9.1603e-02,  1.8315e+00],\n",
      "        [-6.9649e-01, -4.3216e-02, -5.5884e-01,  7.5495e-01, -2.2744e+00,\n",
      "          1.4752e+00, -1.7946e-01, -1.1019e+00, -4.6631e-02,  9.9194e-01,\n",
      "          3.9270e-01, -7.9812e-01,  3.6176e-01, -6.1786e-01,  3.5778e-01,\n",
      "          6.5181e-01,  5.1159e-01,  8.1366e-01,  1.0602e+00, -8.0772e-01,\n",
      "          9.1490e-01,  8.6029e-01, -1.4224e+00,  2.2492e+00, -6.6559e-01,\n",
      "          1.5580e+00, -1.1191e+00, -4.2132e-01,  9.7076e-01,  5.1821e-01,\n",
      "          4.9100e-01, -2.0595e-02, -1.9548e-01, -1.5396e-01,  1.5092e+00,\n",
      "          8.3495e-01, -1.5482e+00, -6.1640e-01,  1.5300e-01,  2.1602e+00,\n",
      "         -7.9212e-01, -6.4470e-01,  5.6059e-01, -4.5125e-01,  2.0580e-01,\n",
      "         -5.6873e-01,  6.0664e-01, -9.5007e-01, -2.9235e-01,  3.1256e-01],\n",
      "        [ 2.3597e-01,  5.1947e-01,  7.9286e-01,  1.2234e-01, -8.4902e-01,\n",
      "         -1.2967e+00, -8.5748e-01,  5.5137e-02, -1.2504e-01,  2.8066e-01,\n",
      "         -1.3330e+00,  5.6354e-02, -9.4621e-01,  1.7885e+00,  5.8774e-02,\n",
      "         -1.0380e+00, -4.5642e-01,  8.2895e-01,  7.1508e-01, -4.7653e-01,\n",
      "         -9.4358e-01,  4.6810e-01, -2.0009e+00,  1.5052e-01,  5.8307e-01,\n",
      "          5.1107e-01,  3.3578e-01,  1.4920e+00, -1.4136e+00,  3.0453e-01,\n",
      "          3.7906e-01,  1.0350e+00, -6.2182e-01,  2.1328e+00,  5.6961e-01,\n",
      "         -1.6509e-01,  1.1782e+00, -1.6354e+00,  2.3869e+00,  6.5401e-02,\n",
      "         -5.8264e-01, -6.8406e-01, -5.5027e-01, -1.7837e-01,  7.9103e-01,\n",
      "          5.9329e-01, -1.4962e+00, -2.9675e-01, -8.0997e-01,  9.7670e-01],\n",
      "        [-8.8094e-02, -2.0012e+00, -5.2095e-01, -2.2508e-01, -6.9639e-01,\n",
      "          5.5729e-01,  1.8053e-01,  1.8104e+00,  1.4280e+00,  7.8604e-02,\n",
      "         -5.7302e-01, -7.1067e-01, -5.0283e-01,  1.2438e+00, -1.0876e+00,\n",
      "         -1.8058e+00, -2.0741e-01,  1.5244e+00,  3.7289e-01, -1.1477e-01,\n",
      "         -2.0493e+00, -3.8060e-01,  9.9798e-01,  2.8153e-01, -1.5778e+00,\n",
      "          9.1334e-01,  5.5482e-01, -1.7109e+00, -1.8993e+00, -1.3937e+00,\n",
      "          5.3219e-01, -4.1158e-01, -2.2620e-02, -8.9919e-01, -1.0065e+00,\n",
      "         -1.5487e+00, -1.1775e+00,  5.7722e-01,  1.1321e-01,  1.7976e+00,\n",
      "          8.0679e-01,  4.8508e-02, -7.3651e-03, -1.4009e-01, -3.4750e+00,\n",
      "          8.9731e-01, -7.4795e-01, -2.1456e+00, -9.1603e-02,  1.8315e+00],\n",
      "        [ 1.9175e+00, -4.6463e-01,  1.1779e+00,  3.3887e-02, -2.3939e-01,\n",
      "          1.7516e-01, -5.2952e-01,  1.2086e+00,  7.5201e-03, -4.4733e-01,\n",
      "         -8.0190e-01,  1.3159e+00, -8.7666e-01, -8.5077e-02,  1.1351e-01,\n",
      "          9.5535e-01,  1.6798e+00, -4.2963e-01,  1.4350e+00,  1.7287e+00,\n",
      "          9.5977e-01,  2.1969e-01,  4.6150e-02,  1.2363e+00, -5.0944e-01,\n",
      "          8.5098e-01, -4.5072e-01,  1.8271e+00, -9.9047e-01,  5.9350e-01,\n",
      "          2.4914e-01, -6.3636e-01, -5.7341e-01, -4.4333e-01,  5.4614e-01,\n",
      "         -1.4072e+00, -2.5762e+00,  8.5261e-02, -2.2335e+00,  6.6475e-01,\n",
      "          2.7268e-01, -1.8356e-01, -1.1025e+00,  7.7848e-01,  1.3256e+00,\n",
      "         -1.2628e+00,  8.7947e-01,  1.6008e+00,  3.0504e-01, -1.3372e+00],\n",
      "        [ 1.9984e-01,  6.1101e-01, -4.5994e-01,  3.5860e-01, -8.9364e-01,\n",
      "         -6.4595e-01,  6.8794e-01, -4.9265e-01, -3.7653e-01,  5.6537e-01,\n",
      "          3.8345e-01, -1.4023e-01,  2.0974e+00, -6.1529e-01,  1.2149e+00,\n",
      "         -8.1955e-01,  1.3353e+00, -1.4502e+00, -1.0299e+00, -3.8271e-01,\n",
      "          1.9979e-01,  1.1863e+00,  4.7864e-01,  1.5374e+00,  1.1530e+00,\n",
      "         -1.2379e+00,  9.9589e-01,  4.4977e-01, -2.5882e-01, -2.2869e-01,\n",
      "         -5.8700e-02, -1.3928e+00,  1.5308e+00,  1.2451e+00,  1.2878e+00,\n",
      "         -1.0114e+00, -1.8107e-02,  1.6963e+00, -1.1311e+00,  1.6896e+00,\n",
      "          4.4827e-01, -4.7763e-01, -1.1587e+00,  1.0919e+00,  9.3702e-02,\n",
      "          1.0489e+00,  7.7329e-01,  1.1162e+00,  7.6647e-01,  2.3435e-01],\n",
      "        [ 9.4830e-01,  2.4013e+00,  1.2345e+00,  6.9895e-01, -1.3332e+00,\n",
      "         -1.1288e-01,  1.0547e+00, -8.3239e-01,  6.5369e-01, -1.4173e+00,\n",
      "         -1.8732e-02,  1.3709e+00,  1.2093e+00,  2.1905e+00, -9.4325e-02,\n",
      "         -1.5820e+00,  8.2981e-01,  5.9242e-01, -5.4287e-01, -9.4341e-01,\n",
      "          9.5250e-01, -3.4745e-03, -1.3700e+00,  5.0983e-01, -6.6748e-01,\n",
      "          2.4330e+00,  8.3922e-01,  1.9216e+00,  9.0611e-01, -1.6828e+00,\n",
      "         -7.5508e-01, -1.4778e+00,  5.4299e-01, -2.2993e-01,  3.0491e-01,\n",
      "         -4.5678e-01,  1.7322e+00, -1.2544e+00, -7.0076e-01, -1.1710e+00,\n",
      "         -5.1627e-01, -7.2068e-01,  1.4390e-01, -3.2994e-01,  1.4676e-01,\n",
      "          6.1740e-01,  3.3020e-01, -6.4942e-01,  9.8548e-01, -9.6092e-01],\n",
      "        [-1.1566e+00, -5.7672e-01, -1.3489e+00, -2.1346e+00,  8.1467e-02,\n",
      "          1.1855e-01, -1.3347e+00,  1.2347e+00,  6.8216e-01,  2.4245e+00,\n",
      "          1.2339e+00, -1.3140e-01, -7.2360e-01, -1.0818e+00,  5.2465e-01,\n",
      "         -8.1128e-01,  2.1109e+00,  1.9335e+00,  2.0239e+00, -1.9529e-01,\n",
      "         -1.1146e+00, -1.3288e+00, -3.2032e-01, -4.3595e-01, -4.3279e-01,\n",
      "          1.4515e+00,  1.1961e+00,  5.2661e-01,  1.2331e+00, -4.7658e-01,\n",
      "         -5.9056e-01, -1.4326e-01,  9.9450e-01,  2.1075e-01,  8.2478e-01,\n",
      "          2.8807e-01,  3.3487e-01, -4.4131e-01, -7.4040e-01, -2.9621e-01,\n",
      "         -4.8741e-01, -8.0173e-01, -1.5895e-01, -4.0488e-01, -1.6584e+00,\n",
      "         -7.0529e-01,  7.9027e-01, -1.5692e+00,  6.1506e-01,  5.7544e-02],\n",
      "        [-8.8094e-02, -2.0012e+00, -5.2095e-01, -2.2508e-01, -6.9639e-01,\n",
      "          5.5729e-01,  1.8053e-01,  1.8104e+00,  1.4280e+00,  7.8604e-02,\n",
      "         -5.7302e-01, -7.1067e-01, -5.0283e-01,  1.2438e+00, -1.0876e+00,\n",
      "         -1.8058e+00, -2.0741e-01,  1.5244e+00,  3.7289e-01, -1.1477e-01,\n",
      "         -2.0493e+00, -3.8060e-01,  9.9798e-01,  2.8153e-01, -1.5778e+00,\n",
      "          9.1334e-01,  5.5482e-01, -1.7109e+00, -1.8993e+00, -1.3937e+00,\n",
      "          5.3219e-01, -4.1158e-01, -2.2620e-02, -8.9919e-01, -1.0065e+00,\n",
      "         -1.5487e+00, -1.1775e+00,  5.7722e-01,  1.1321e-01,  1.7976e+00,\n",
      "          8.0679e-01,  4.8508e-02, -7.3651e-03, -1.4009e-01, -3.4750e+00,\n",
      "          8.9731e-01, -7.4795e-01, -2.1456e+00, -9.1603e-02,  1.8315e+00],\n",
      "        [ 2.1131e+00, -9.0723e-01, -4.3322e-01, -3.8256e-01,  2.0272e+00,\n",
      "          9.9981e-01,  4.4087e-01, -7.4710e-01,  3.8768e-01,  7.0282e-01,\n",
      "         -1.9428e-01, -1.7892e-01,  8.3896e-02, -4.2233e-03,  1.6497e-01,\n",
      "          6.4329e-01, -7.1817e-01, -5.3080e-01, -1.3841e+00, -2.0215e-01,\n",
      "          7.8179e-01, -4.2905e-01, -1.3120e+00, -2.9533e-01,  6.7838e-01,\n",
      "         -7.7071e-01, -1.7076e-01,  1.4975e-01, -5.0095e-01, -1.7184e-01,\n",
      "         -1.0277e+00,  9.7882e-01,  2.1696e-04, -1.9767e-01, -9.9900e-01,\n",
      "         -1.8090e-01,  1.0649e+00, -6.1225e-01, -1.2831e+00, -1.8379e+00,\n",
      "         -4.2490e-02,  5.2034e-01,  1.2244e+00,  2.4370e-01,  4.6230e-01,\n",
      "         -6.5646e-01, -4.3385e-01,  1.2120e+00, -5.6133e-01, -7.0688e-01],\n",
      "        [ 5.8729e-02, -1.1411e+00, -4.0178e-01, -1.2132e+00,  1.5346e+00,\n",
      "         -5.5263e-02,  4.2550e-01,  1.3722e-02,  2.4991e-01,  1.4479e+00,\n",
      "          1.4216e-01,  4.7037e-01,  9.0000e-01,  6.5884e-02,  2.5236e+00,\n",
      "         -1.1260e+00, -1.3685e-01,  1.5576e+00, -9.8934e-03, -1.0251e+00,\n",
      "          1.1670e-02, -3.5835e-01,  2.6632e-01, -4.2147e-01, -4.0946e-01,\n",
      "          6.9795e-01,  9.0762e-01,  8.3856e-01,  5.4875e-02,  2.2028e-01,\n",
      "         -3.6209e-01, -8.9719e-01,  2.3011e-01, -2.0123e-01, -2.9176e-01,\n",
      "          1.3182e+00,  8.8402e-01,  4.4688e-01, -9.7994e-03, -5.0644e-01,\n",
      "          8.3134e-01,  2.3465e-02,  8.5519e-01, -6.1851e-01,  2.0846e+00,\n",
      "         -8.0457e-01,  1.0765e+00, -2.1893e-01,  1.4192e-01, -1.1630e+00],\n",
      "        [ 1.4525e+00,  3.0222e-02,  1.2790e+00, -8.3894e-01,  1.6597e-01,\n",
      "         -3.4394e-01,  4.4443e-01,  1.5745e+00,  6.6511e-01,  8.4670e-02,\n",
      "         -9.6547e-01,  1.0473e+00, -3.3500e-01, -4.3696e-01,  1.6810e-03,\n",
      "         -1.1109e+00, -1.1754e+00,  5.5313e-01,  1.0630e+00,  1.4133e+00,\n",
      "          3.9817e-01,  2.1824e-01,  1.2966e-01, -1.3486e+00,  4.1701e-01,\n",
      "          1.0554e+00,  7.7863e-01,  3.8776e+00,  7.2632e-01, -8.1382e-01,\n",
      "         -7.0315e-01, -2.5578e-01,  1.0658e+00,  1.6551e+00, -2.9571e-01,\n",
      "          1.0268e+00, -2.6823e-01,  2.0002e-01, -6.5549e-01,  3.1790e-02,\n",
      "          6.6694e-01,  6.3226e-01,  9.2368e-01,  2.1440e+00, -4.8346e-01,\n",
      "         -5.6717e-01, -6.1878e-01,  4.8532e-01,  1.3945e-01,  1.2198e+00],\n",
      "        [-9.7477e-01, -4.3361e-01,  1.5261e-01,  1.7031e-01,  7.3709e-02,\n",
      "          2.3980e-01,  1.5263e+00, -4.4268e-01,  2.5543e-01, -7.1258e-01,\n",
      "         -3.7881e-01,  6.8324e-01,  4.6024e-02,  4.8334e-01,  1.6332e-01,\n",
      "         -5.9669e-01,  2.1676e-01, -2.8882e-01, -9.2595e-01,  1.0620e-01,\n",
      "          2.8472e-01,  1.1443e+00, -1.5004e+00,  4.6658e-01, -3.9674e-01,\n",
      "         -1.3271e+00,  1.3266e+00,  6.0102e-01,  3.5373e-02, -5.6995e-01,\n",
      "         -8.1387e-01,  2.2280e-01, -2.1762e+00,  2.6072e-01, -6.9903e-01,\n",
      "         -2.0361e-02,  1.3610e+00,  7.7178e-04, -3.8126e-01, -5.8257e-01,\n",
      "         -1.1506e+00, -1.0637e+00, -6.7149e-01, -5.7219e-01,  1.2895e+00,\n",
      "         -2.6315e+00,  1.1933e+00, -1.3198e+00, -2.2050e-02,  1.0710e+00],\n",
      "        [ 1.4525e+00,  3.0222e-02,  1.2790e+00, -8.3894e-01,  1.6597e-01,\n",
      "         -3.4394e-01,  4.4443e-01,  1.5745e+00,  6.6511e-01,  8.4670e-02,\n",
      "         -9.6547e-01,  1.0473e+00, -3.3500e-01, -4.3696e-01,  1.6810e-03,\n",
      "         -1.1109e+00, -1.1754e+00,  5.5313e-01,  1.0630e+00,  1.4133e+00,\n",
      "          3.9817e-01,  2.1824e-01,  1.2966e-01, -1.3486e+00,  4.1701e-01,\n",
      "          1.0554e+00,  7.7863e-01,  3.8776e+00,  7.2632e-01, -8.1382e-01,\n",
      "         -7.0315e-01, -2.5578e-01,  1.0658e+00,  1.6551e+00, -2.9571e-01,\n",
      "          1.0268e+00, -2.6823e-01,  2.0002e-01, -6.5549e-01,  3.1790e-02,\n",
      "          6.6694e-01,  6.3226e-01,  9.2368e-01,  2.1440e+00, -4.8346e-01,\n",
      "         -5.6717e-01, -6.1878e-01,  4.8532e-01,  1.3945e-01,  1.2198e+00],\n",
      "        [-2.2049e+00,  8.0422e-01,  8.8473e-01,  6.2508e-01,  1.2541e-01,\n",
      "         -1.3585e+00,  8.7737e-02, -4.7312e-01,  1.5274e+00,  1.0043e-01,\n",
      "          6.9083e-01,  3.2189e-01, -1.4056e+00, -1.7829e+00, -9.7150e-03,\n",
      "          3.5125e-01,  1.6597e+00, -1.2111e+00, -1.6529e+00,  7.2628e-01,\n",
      "         -1.3865e+00,  5.1305e-01,  3.8381e-01, -1.1848e-01, -4.4755e-01,\n",
      "          2.6288e-01,  7.3388e-01,  5.1426e-01,  1.3500e+00, -2.5900e-01,\n",
      "          2.5870e-01, -4.0349e-01, -1.9734e+00,  2.4313e-01,  1.7781e+00,\n",
      "         -1.3914e+00, -3.8661e-01,  5.2148e-01, -1.8121e+00, -2.4636e-01,\n",
      "          1.2173e+00,  3.4503e-01,  1.0789e+00,  3.1851e+00, -3.9117e-01,\n",
      "         -8.9804e-01,  3.1080e+00, -2.0684e+00, -4.1565e-02, -1.4462e+00]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the embedded data to verify\n",
    "print(\"Shape of embedded data:\", embedded_data.shape)\n",
    "\n",
    "# Print the first embedded sequence for verification\n",
    "print(\"First embedded sequence:\", embedded_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b6c9cc",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f6d2a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, model_dim, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "    \n",
    "        positional_encoding = np.zeros((max_sequence_length, model_dim))\n",
    "        \n",
    "        # calculating encoding for each position and dim\n",
    "        for pos in range(self.max_sequence_length):\n",
    "            for i in range(0, self.model_dim, 2):\n",
    "                # sin to even indices\n",
    "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.model_dim)))\n",
    "                \n",
    "                # cos to odd indices\n",
    "                if i + 1 < self.model_dim:\n",
    "                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / self.model_dim)))\n",
    "                    \n",
    "        \n",
    "        self.positional_encoding = torch.from_numpy(positional_encoding).unsqueeze(0).float()\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.positional_encoding[: x.size(1), :]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "266f7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding = PositionalEncoding(model_dim, max_sequence_length)\n",
    "encoded_data = pos_encoding(embedded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0a20c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoded data: torch.Size([100, 20, 50])\n",
      "First encoded sequence: tensor([[-1.1822e-01,  1.0468e+00, -1.6400e+00,  2.7318e+00, -2.9525e-01,\n",
      "          1.5987e+00, -2.4417e+00, -1.0057e+00, -6.0042e-01,  6.4142e-01,\n",
      "         -2.5327e-01,  1.4028e+00, -9.7319e-01,  7.2568e-01, -2.8670e-01,\n",
      "          1.6111e+00,  1.5894e+00,  1.8112e+00,  1.3971e+00,  1.9630e+00,\n",
      "          2.9800e-01,  6.0360e-01,  5.0827e-01, -4.2977e-01, -3.9643e-02,\n",
      "          2.6600e+00,  3.1453e+00, -6.9908e-01,  1.4632e+00,  5.5598e-01,\n",
      "          1.3571e-01, -1.8087e-01, -7.5177e-01,  1.8770e-01,  8.2789e-02,\n",
      "          6.4109e-01, -7.5696e-02,  1.1484e+00,  1.2437e+00,  8.5697e-01,\n",
      "         -9.9537e-01,  2.8001e-01, -1.9865e+00,  1.4181e+00,  1.1808e+00,\n",
      "          1.6110e+00,  3.2342e-01,  3.9302e-01,  1.6589e-01,  5.1546e-01],\n",
      "        [-3.1518e-01, -3.6421e-02, -8.8835e-01, -1.2470e+00,  3.0856e-01,\n",
      "          1.0924e+00, -1.2253e+00,  2.2287e+00,  7.3462e-01,  3.4231e+00,\n",
      "          1.2591e+00,  8.6828e-01, -7.1158e-01, -8.1893e-02,  5.3041e-01,\n",
      "          1.8870e-01,  2.1137e+00,  2.9335e+00,  2.0252e+00,  8.0471e-01,\n",
      "         -1.1140e+00, -3.2878e-01, -3.2002e-01,  5.6405e-01, -4.3264e-01,\n",
      "          2.4515e+00,  1.1962e+00,  1.5266e+00,  1.2331e+00,  5.2342e-01,\n",
      "         -5.9055e-01,  8.5674e-01,  9.9451e-01,  1.2108e+00,  8.2478e-01,\n",
      "          1.2881e+00,  3.3487e-01,  5.5869e-01, -7.4040e-01,  7.0379e-01,\n",
      "         -4.8741e-01,  1.9827e-01, -1.5895e-01,  5.9512e-01, -1.6584e+00,\n",
      "          2.9471e-01,  7.9027e-01, -5.6922e-01,  6.1506e-01,  1.0575e+00],\n",
      "        [ 1.1091e+00,  1.9486e-01,  3.5767e-01,  9.3436e-01, -4.5133e-01,\n",
      "          2.5091e-01,  9.0548e-01,  4.8340e-01, -2.7176e-01,  1.5599e+00,\n",
      "          4.3367e-01,  8.5851e-01,  2.1214e+00,  3.8442e-01,  1.2264e+00,\n",
      "          1.8038e-01,  1.3408e+00, -4.5020e-01, -1.0272e+00,  6.1729e-01,\n",
      "          2.0105e-01,  2.1863e+00,  4.7924e-01,  2.5374e+00,  1.1532e+00,\n",
      "         -2.3790e-01,  9.9603e-01,  1.4498e+00, -2.5876e-01,  7.7131e-01,\n",
      "         -5.8669e-02, -3.9277e-01,  1.5308e+00,  2.2451e+00,  1.2878e+00,\n",
      "         -1.1414e-02, -1.8104e-02,  2.6963e+00, -1.1311e+00,  2.6896e+00,\n",
      "          4.4828e-01,  5.2237e-01, -1.1587e+00,  2.0919e+00,  9.3702e-02,\n",
      "          2.0489e+00,  7.7329e-01,  2.1162e+00,  7.6647e-01,  1.2344e+00],\n",
      "        [ 1.4255e+00, -1.5335e+00, -6.8918e-01, -8.6846e-01, -1.1228e-02,\n",
      "          5.1023e-01,  1.1564e+00,  6.7169e-01, -7.6469e-01, -3.1005e-01,\n",
      "          2.2300e-01, -8.9520e-01, -1.6074e+00,  4.0454e+00, -7.5716e-01,\n",
      "          1.1824e+00,  1.3907e-01,  1.1175e+00,  1.2569e+00, -6.1163e-02,\n",
      "         -1.0284e+00, -1.9812e-01,  5.3080e-01,  1.3625e+00,  6.7454e-02,\n",
      "          1.1415e+00,  1.0856e+00,  1.4244e+00, -1.2462e+00, -2.6323e-02,\n",
      "         -1.4939e+00,  2.0916e+00, -5.3864e-01,  3.2739e-01, -1.3889e+00,\n",
      "          1.4748e+00,  1.1667e+00,  2.7122e+00, -2.0236e+00,  2.8932e+00,\n",
      "         -1.1176e-01,  1.1692e+00, -8.8170e-01,  1.4205e+00,  1.3595e-01,\n",
      "         -9.9864e-01,  6.9741e-01, -7.4167e-01, -3.0279e-01,  2.1992e+00],\n",
      "        [-5.5696e-01, -4.2637e-02,  4.8156e-01,  2.1602e-02, -1.0026e-01,\n",
      "         -3.7224e-02,  1.1126e+00,  4.1270e-01, -1.6814e-01,  1.5434e+00,\n",
      "          4.8376e-01,  8.5473e-01,  2.1454e+00,  3.8355e-01,  1.2379e+00,\n",
      "          1.8018e-01,  1.3463e+00, -4.5024e-01, -1.0246e+00,  6.1728e-01,\n",
      "          2.0231e-01,  2.1863e+00,  4.7985e-01,  2.5374e+00,  1.1535e+00,\n",
      "         -2.3790e-01,  9.9616e-01,  1.4498e+00, -2.5869e-01,  7.7131e-01,\n",
      "         -5.8637e-02, -3.9277e-01,  1.5308e+00,  2.2451e+00,  1.2878e+00,\n",
      "         -1.1414e-02, -1.8100e-02,  2.6963e+00, -1.1311e+00,  2.6896e+00,\n",
      "          4.4828e-01,  5.2237e-01, -1.1587e+00,  2.0919e+00,  9.3702e-02,\n",
      "          2.0489e+00,  7.7329e-01,  2.1162e+00,  7.6647e-01,  1.2344e+00],\n",
      "        [-1.0470e+00, -1.7176e+00,  1.5955e-01, -9.5783e-01,  2.1450e-01,\n",
      "          9.6994e-01,  7.0171e-01,  2.6639e+00,  1.6875e+00,  1.0444e+00,\n",
      "         -4.4776e-01,  2.8145e-01, -4.4276e-01,  2.2420e+00, -1.0588e+00,\n",
      "         -8.0621e-01, -1.9364e-01,  2.5243e+00,  3.7948e-01,  8.8520e-01,\n",
      "         -2.0461e+00,  6.1939e-01,  9.9949e-01,  1.2815e+00, -1.5771e+00,\n",
      "          1.9133e+00,  5.5517e-01, -7.1091e-01, -1.8991e+00, -3.9373e-01,\n",
      "          5.3227e-01,  5.8842e-01, -2.2582e-02,  1.0081e-01, -1.0065e+00,\n",
      "         -5.4872e-01, -1.1775e+00,  1.5772e+00,  1.1321e-01,  2.7976e+00,\n",
      "          8.0679e-01,  1.0485e+00, -7.3642e-03,  8.5991e-01, -3.4750e+00,\n",
      "          1.8973e+00, -7.4795e-01, -1.1456e+00, -9.1603e-02,  2.8315e+00],\n",
      "        [-9.7590e-01,  9.1695e-01, -2.9229e-01, -2.0887e-01, -1.2936e+00,\n",
      "          1.6702e+00,  4.3198e-01, -3.1060e-01,  2.6308e-01,  1.9428e+00,\n",
      "          5.4284e-01,  1.9054e-01,  4.3383e-01,  3.7954e-01,  3.9230e-01,\n",
      "          1.6512e+00,  5.2811e-01,  1.8135e+00,  1.0681e+00,  1.9225e-01,\n",
      "          9.1869e-01,  1.8603e+00, -1.4206e+00,  3.2492e+00, -6.6473e-01,\n",
      "          2.5580e+00, -1.1187e+00,  5.7868e-01,  9.7096e-01,  1.5182e+00,\n",
      "          4.9110e-01,  9.7941e-01, -1.9544e-01,  8.4604e-01,  1.5092e+00,\n",
      "          1.8350e+00, -1.5482e+00,  3.8360e-01,  1.5301e-01,  3.1602e+00,\n",
      "         -7.9211e-01,  3.5530e-01,  5.6059e-01,  5.4875e-01,  2.0580e-01,\n",
      "          4.3127e-01,  6.0664e-01,  4.9928e-02, -2.9235e-01,  1.3126e+00],\n",
      "        [ 8.9296e-01,  1.2734e+00,  5.8556e-01, -8.5593e-01,  1.5044e-01,\n",
      "         -1.3295e+00, -1.6312e-01,  7.7476e-01,  2.3412e-01,  1.2139e+00,\n",
      "         -1.1580e+00,  1.0409e+00, -8.6215e-01,  2.7849e+00,  9.9044e-02,\n",
      "         -3.8833e-02, -4.3715e-01,  1.8288e+00,  7.2431e-01,  5.2343e-01,\n",
      "         -9.3916e-01,  1.4681e+00, -1.9988e+00,  1.1505e+00,  5.8408e-01,\n",
      "          1.5111e+00,  3.3627e-01,  2.4920e+00, -1.4134e+00,  1.3045e+00,\n",
      "          3.7917e-01,  2.0350e+00, -6.2177e-01,  3.1328e+00,  5.6963e-01,\n",
      "          8.3491e-01,  1.1782e+00, -6.3537e-01,  2.3869e+00,  1.0654e+00,\n",
      "         -5.8264e-01,  3.1594e-01, -5.5026e-01,  8.2163e-01,  7.9103e-01,\n",
      "          1.5933e+00, -1.4962e+00,  7.0325e-01, -8.0996e-01,  1.9767e+00],\n",
      "        [ 9.0126e-01, -2.1467e+00, -1.1555e+00, -9.9795e-01,  2.6951e-01,\n",
      "          2.9837e-01,  9.4947e-01,  2.4497e+00,  1.8357e+00,  9.9176e-01,\n",
      "         -3.7342e-01,  2.6921e-01, -4.0680e-01,  2.2392e+00, -1.0416e+00,\n",
      "         -8.0686e-01, -1.8538e-01,  2.5241e+00,  3.8343e-01,  8.8517e-01,\n",
      "         -2.0442e+00,  6.1938e-01,  1.0004e+00,  1.2815e+00, -1.5766e+00,\n",
      "          1.9133e+00,  5.5537e-01, -7.1091e-01, -1.8990e+00, -3.9373e-01,\n",
      "          5.3232e-01,  5.8842e-01, -2.2560e-02,  1.0081e-01, -1.0065e+00,\n",
      "         -5.4872e-01, -1.1775e+00,  1.5772e+00,  1.1321e-01,  2.7976e+00,\n",
      "          8.0679e-01,  1.0485e+00, -7.3636e-03,  8.5991e-01, -3.4750e+00,\n",
      "          1.8973e+00, -7.4795e-01, -1.1456e+00, -9.1603e-02,  2.8315e+00],\n",
      "        [ 2.3296e+00, -1.3758e+00,  2.5867e-01, -3.5987e-01,  6.4248e-01,\n",
      "         -2.9633e-01,  3.0476e-01,  1.7600e+00,  4.6248e-01,  4.4318e-01,\n",
      "         -5.7775e-01,  2.2905e+00, -7.6867e-01,  9.0908e-01,  1.6528e-01,\n",
      "          1.9540e+00,  1.7045e+00,  5.7006e-01,  1.4468e+00,  2.7286e+00,\n",
      "          9.6545e-01,  1.2197e+00,  4.8868e-02,  2.2363e+00, -5.0814e-01,\n",
      "          1.8510e+00, -4.5010e-01,  2.8271e+00, -9.9017e-01,  1.5935e+00,\n",
      "          2.4928e-01,  3.6364e-01, -5.7334e-01,  5.5667e-01,  5.4618e-01,\n",
      "         -4.0725e-01, -2.5762e+00,  1.0853e+00, -2.2335e+00,  1.6647e+00,\n",
      "          2.7269e-01,  8.1644e-01, -1.1025e+00,  1.7785e+00,  1.3256e+00,\n",
      "         -2.6276e-01,  8.7947e-01,  2.6008e+00,  3.0504e-01, -3.3720e-01],\n",
      "        [-3.4418e-01, -2.2806e-01, -1.4572e+00,  4.3244e-01, -1.4188e-01,\n",
      "         -1.3054e+00,  1.5775e+00, -3.5921e-02,  1.2452e-01,  1.4308e+00,\n",
      "          6.3201e-01,  8.2839e-01,  2.2173e+00,  3.7749e-01,  1.2724e+00,\n",
      "          1.7879e-01,  1.3628e+00, -4.5056e-01, -1.0167e+00,  6.1721e-01,\n",
      "          2.0610e-01,  2.1863e+00,  4.8166e-01,  2.5374e+00,  1.1544e+00,\n",
      "         -2.3790e-01,  9.9658e-01,  1.4498e+00, -2.5849e-01,  7.7131e-01,\n",
      "         -5.8542e-02, -3.9277e-01,  1.5309e+00,  2.2451e+00,  1.2878e+00,\n",
      "         -1.1414e-02, -1.8090e-02,  2.6963e+00, -1.1310e+00,  2.6896e+00,\n",
      "          4.4828e-01,  5.2237e-01, -1.1587e+00,  2.0919e+00,  9.3703e-02,\n",
      "          2.0489e+00,  7.7329e-01,  2.1162e+00,  7.6647e-01,  1.2344e+00],\n",
      "        [-5.1688e-02,  2.4057e+00,  3.8335e-01,  1.2238e+00, -7.5081e-01,\n",
      "         -9.2581e-01,  1.9889e+00, -4.7575e-01,  1.1994e+00, -5.7936e-01,\n",
      "          2.5407e-01,  2.3330e+00,  1.3411e+00,  3.1818e+00, -3.1069e-02,\n",
      "         -5.8401e-01,  8.6010e-01,  1.5920e+00, -5.2837e-01,  5.6490e-02,\n",
      "          9.5944e-01,  9.9650e-01, -1.3667e+00,  1.5098e+00, -6.6589e-01,\n",
      "          3.4330e+00,  8.3998e-01,  2.9216e+00,  9.0647e-01, -6.8283e-01,\n",
      "         -7.5491e-01, -4.7780e-01,  5.4307e-01,  7.7007e-01,  3.0495e-01,\n",
      "          5.4322e-01,  1.7322e+00, -2.5444e-01, -7.0075e-01, -1.7099e-01,\n",
      "         -5.1627e-01,  2.7932e-01,  1.4390e-01,  6.7006e-01,  1.4676e-01,\n",
      "          1.6174e+00,  3.3020e-01,  3.5058e-01,  9.8548e-01,  3.9082e-02],\n",
      "        [-1.6932e+00,  2.6713e-01, -1.8627e+00, -1.2767e+00,  4.6401e-01,\n",
      "         -8.0539e-01, -3.6707e-01,  1.4869e+00,  1.2711e+00,  3.2326e+00,\n",
      "          1.5308e+00,  8.2351e-01, -5.7983e-01, -9.2209e-02,  5.9365e-01,\n",
      "          1.8633e-01,  2.1439e+00,  2.9329e+00,  2.0397e+00,  8.0459e-01,\n",
      "         -1.1070e+00, -3.2881e-01, -3.1669e-01,  5.6404e-01, -4.3105e-01,\n",
      "          2.4515e+00,  1.1969e+00,  1.5266e+00,  1.2335e+00,  5.2342e-01,\n",
      "         -5.9037e-01,  8.5674e-01,  9.9459e-01,  1.2108e+00,  8.2482e-01,\n",
      "          1.2881e+00,  3.3489e-01,  5.5869e-01, -7.4039e-01,  7.0379e-01,\n",
      "         -4.8741e-01,  1.9827e-01, -1.5895e-01,  5.9512e-01, -1.6584e+00,\n",
      "          2.9471e-01,  7.9027e-01, -5.6922e-01,  6.1506e-01,  1.0575e+00],\n",
      "        [ 3.3207e-01, -1.0938e+00, -5.8191e-01,  7.7306e-01, -5.3365e-01,\n",
      "         -4.2938e-01,  1.1700e+00,  1.9553e+00,  2.0586e+00,  8.5476e-01,\n",
      "         -2.5225e-01,  2.3649e-01, -3.4717e-01,  2.2317e+00, -1.0128e+00,\n",
      "         -8.0859e-01, -1.7161e-01,  2.5237e+00,  3.9002e-01,  8.8508e-01,\n",
      "         -2.0411e+00,  6.1936e-01,  1.0019e+00,  1.2815e+00, -1.5759e+00,\n",
      "          1.9133e+00,  5.5572e-01, -7.1091e-01, -1.8989e+00, -3.9373e-01,\n",
      "          5.3240e-01,  5.8842e-01, -2.2522e-02,  1.0081e-01, -1.0065e+00,\n",
      "         -5.4872e-01, -1.1775e+00,  1.5772e+00,  1.1322e-01,  2.7976e+00,\n",
      "          8.0679e-01,  1.0485e+00, -7.3627e-03,  8.5991e-01, -3.4750e+00,\n",
      "          1.8973e+00, -7.4795e-01, -1.1456e+00, -9.1603e-02,  2.8315e+00],\n",
      "        [ 3.1037e+00, -7.7049e-01, -2.7623e-02,  5.3149e-01,  1.9617e+00,\n",
      "          1.9580e-03,  1.4402e+00, -7.1138e-01,  1.0581e+00,  1.4448e+00,\n",
      "          1.5018e-01,  7.5988e-01,  2.5142e-01,  9.8164e-01,  2.4545e-01,\n",
      "          1.6400e+00, -6.7962e-01,  4.6845e-01, -1.3657e+00,  7.9768e-01,\n",
      "          7.9062e-01,  5.7091e-01, -1.3078e+00,  7.0466e-01,  6.8040e-01,\n",
      "          2.2929e-01, -1.6979e-01,  1.1498e+00, -5.0049e-01,  8.2816e-01,\n",
      "         -1.0275e+00,  1.9788e+00,  3.2317e-04,  8.0233e-01, -9.9894e-01,\n",
      "          8.1910e-01,  1.0649e+00,  3.8775e-01, -1.2831e+00, -8.3788e-01,\n",
      "         -4.2485e-02,  1.5203e+00,  1.2244e+00,  1.2437e+00,  4.6230e-01,\n",
      "          3.4354e-01, -4.3384e-01,  2.2120e+00, -5.6133e-01,  2.9312e-01],\n",
      "        [ 7.0902e-01, -1.9008e+00,  3.7922e-01, -5.8868e-01,  1.2442e+00,\n",
      "         -1.0121e+00,  1.4228e+00, -6.0131e-02,  9.5829e-01,  2.1537e+00,\n",
      "          5.1010e-01,  1.4002e+00,  1.0794e+00,  1.0497e+00,  2.6098e+00,\n",
      "         -1.2976e-01, -9.5543e-02,  2.5568e+00,  9.8792e-03, -2.5339e-02,\n",
      "          2.1134e-02,  6.4161e-01,  2.7085e-01,  5.7852e-01, -4.0729e-01,\n",
      "          1.6980e+00,  9.0866e-01,  1.8386e+00,  5.5372e-02,  1.2203e+00,\n",
      "         -3.6185e-01,  1.0281e-01,  2.3022e-01,  7.9877e-01, -2.9170e-01,\n",
      "          2.3182e+00,  8.8404e-01,  1.4469e+00, -9.7869e-03,  4.9356e-01,\n",
      "          8.3134e-01,  1.0235e+00,  8.5520e-01,  3.8149e-01,  2.0846e+00,\n",
      "          1.9543e-01,  1.0765e+00,  7.8107e-01,  1.4192e-01, -1.6298e-01],\n",
      "        [ 1.1646e+00, -9.2744e-01,  2.2599e+00, -6.4429e-01, -3.3420e-01,\n",
      "         -1.2099e+00,  1.4276e+00,  1.3919e+00,  1.4095e+00,  7.5236e-01,\n",
      "         -5.7430e-01,  1.9676e+00, -1.4382e-01,  5.4459e-01,  9.3621e-02,\n",
      "         -1.1513e-01, -1.1314e+00,  1.5522e+00,  1.0841e+00,  2.4131e+00,\n",
      "          4.0826e-01,  1.2182e+00,  1.3449e-01, -3.4859e-01,  4.1932e-01,\n",
      "          2.0554e+00,  7.7974e-01,  4.8776e+00,  7.2685e-01,  1.8618e-01,\n",
      "         -7.0290e-01,  7.4422e-01,  1.0659e+00,  2.6551e+00, -2.9565e-01,\n",
      "          2.0268e+00, -2.6820e-01,  1.2000e+00, -6.5548e-01,  1.0318e+00,\n",
      "          6.6695e-01,  1.6323e+00,  9.2368e-01,  3.1440e+00, -4.8346e-01,\n",
      "          4.3283e-01, -6.1878e-01,  1.4853e+00,  1.3945e-01,  2.2198e+00],\n",
      "        [-1.9362e+00, -7.0878e-01,  1.1129e+00, -1.0867e-01, -6.1004e-01,\n",
      "         -4.8992e-01,  2.4836e+00, -7.3172e-01,  1.0339e+00, -8.4857e-02,\n",
      "          3.5348e-02,  1.5934e+00,  2.4899e-01,  1.4625e+00,  2.6099e-01,\n",
      "          3.9853e-01,  2.6356e-01,  7.1008e-01, -9.0355e-01,  1.1060e+00,\n",
      "          2.9545e-01,  2.1443e+00, -1.4952e+00,  1.4666e+00, -3.9428e-01,\n",
      "         -3.2713e-01,  1.3278e+00,  1.6010e+00,  3.5936e-02,  4.3005e-01,\n",
      "         -8.1360e-01,  1.2228e+00, -2.1760e+00,  1.2607e+00, -6.9897e-01,\n",
      "          9.7964e-01,  1.3610e+00,  1.0008e+00, -3.8124e-01,  4.1743e-01,\n",
      "         -1.1506e+00, -6.3691e-02, -6.7148e-01,  4.2781e-01,  1.2895e+00,\n",
      "         -1.6315e+00,  1.1933e+00, -3.1979e-01, -2.2050e-02,  2.0710e+00],\n",
      "        [ 7.0149e-01,  6.9054e-01,  2.0029e+00, -1.5289e+00, -6.6562e-01,\n",
      "         -8.9933e-01,  1.3644e+00,  1.1824e+00,  1.4754e+00,  6.7069e-01,\n",
      "         -5.2858e-01,  1.9468e+00, -1.2027e-01,  5.3971e-01,  1.0508e-01,\n",
      "         -1.1626e-01, -1.1259e+00,  1.5519e+00,  1.0867e+00,  2.4130e+00,\n",
      "          4.0953e-01,  1.2182e+00,  1.3509e-01, -3.4859e-01,  4.1961e-01,\n",
      "          2.0553e+00,  7.7988e-01,  4.8776e+00,  7.2692e-01,  1.8618e-01,\n",
      "         -7.0287e-01,  7.4422e-01,  1.0659e+00,  2.6551e+00, -2.9564e-01,\n",
      "          2.0268e+00, -2.6820e-01,  1.2000e+00, -6.5548e-01,  1.0318e+00,\n",
      "          6.6695e-01,  1.6323e+00,  9.2368e-01,  3.1440e+00, -4.8346e-01,\n",
      "          4.3283e-01, -6.1878e-01,  1.4853e+00,  1.3945e-01,  2.2198e+00],\n",
      "        [-2.0550e+00,  1.7929e+00,  1.2095e+00, -3.2070e-01, -8.1058e-01,\n",
      "         -1.7106e+00,  9.5925e-01, -9.6349e-01,  2.3673e+00,  6.4315e-01,\n",
      "          1.1502e+00,  1.2101e+00, -1.1792e+00, -8.0889e-01,  9.9401e-02,\n",
      "          1.3453e+00,  1.7121e+00, -2.1248e-01, -1.6279e+00,  1.7260e+00,\n",
      "         -1.3745e+00,  1.5130e+00,  3.8955e-01,  8.8150e-01, -4.4480e-01,\n",
      "          1.2629e+00,  7.3520e-01,  1.5143e+00,  1.3507e+00,  7.4100e-01,\n",
      "          2.5900e-01,  5.9651e-01, -1.9733e+00,  1.2431e+00,  1.7781e+00,\n",
      "         -3.9145e-01, -3.8658e-01,  1.5215e+00, -1.8121e+00,  7.5364e-01,\n",
      "          1.2173e+00,  1.3450e+00,  1.0789e+00,  4.1851e+00, -3.9116e-01,\n",
      "          1.0196e-01,  3.1080e+00, -1.0684e+00, -4.1565e-02, -4.4624e-01]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the encoded data to verify\n",
    "print(\"Shape of encoded data:\", encoded_data.shape)\n",
    "\n",
    "# Print the first encoded sequence for verification\n",
    "print(\"First encoded sequence:\", encoded_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9242bf15",
   "metadata": {},
   "source": [
    "## Masking and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99c5ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, embedding_dimension, head_dimension):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.head_dimension = head_dimension\n",
    "        \n",
    "        self.query_layer = torch.nn.Linear(self.embedding_dimension, self.head_dimension)\n",
    "        self.key_layer = torch.nn.Linear(self.embedding_dimension, self.head_dimension)\n",
    "        self.value_layer = torch.nn.Linear(self.embedding_dimension, self.head_dimension)\n",
    "        self.softmax = torch.nn.Softmax(dim = -1)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # x dim - (batch_size, sequence_length, embedding_dim)\n",
    "        # mask dim - (batch_size, sequence_length, head_dim)\n",
    "        # output dim - (batch_size, sequence_length)\n",
    "        \n",
    "        query = self.query_layer(x)\n",
    "        key = self.key_layer(x)\n",
    "        value = self.value_layer(x)\n",
    "        \n",
    "        # calculating attention weights and scaling\n",
    "        attention_weights = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(self.head_dimension)\n",
    "        \n",
    "        # masking\n",
    "        if mask is not None:\n",
    "            attention_weights = attention_weights.masked_filled(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_scores = self.softmax(attention_weights)\n",
    "        return torch.matmul(attention_scores, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6936347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadedSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, embedding_dimension, num_heads):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.head_dimension = embedding_dimension // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.self_attentions = torch.nn.ModuleList(\n",
    "            [MaskedSelfAttention(embedding_dimension, self.head_dimension) for _ in range(self.num_heads)]\n",
    "        )\n",
    "        \n",
    "        self.output_layer = torch.nn.Linear(self.num_heads * self.head_dimension, self.embedding_dimension)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        self_attention_outputs = [self_attention(x, mask) for self_attention in self.self_attentions]\n",
    "        \n",
    "        # concatenating outputs\n",
    "        concatenated_outputs = torch.cat(self_attention_outputs, dim = 2)\n",
    "        return self.output_layer(concatenated_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e8df0d",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35723d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, feed_forward_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.multi_attention = MaskedMultiHeadedSelfAttention(embedding_dim, num_heads)\n",
    "        self.feed_forward = FeedForward(embedding_dim, feed_forward_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm_2 = torch.nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x_norm = self.layer_norm_1(x)\n",
    "        attention_output = self.multi_attention(x_norm, mask)\n",
    "        residual_output = x + attention_output\n",
    "        residual_output_norm = self.layer_norm_2(residual_output)\n",
    "        \n",
    "        feed_forward_output = self.feed_forward(residual_output_norm)\n",
    "        \n",
    "        if self.training:\n",
    "            feed_forward_output = self.dropout(feed_forward_output)\n",
    "            \n",
    "        return residual_output + feed_forward_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1707d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, num_layers, num_heads, feed_forward_dim, dropout_rate, max_sequence_length):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.decoder_layers = torch.nn.ModuleList(\n",
    "            [DecoderLayer(embedding_dim, num_heads, feed_forward_dim, dropout_rate) for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        outputs = x\n",
    "        for layer in self.decoder_layers:\n",
    "            outputs = layer(outputs, mask)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "547725c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, feed_forward_dim):\n",
    "        super().__init__()\n",
    "        self.linear_1 = torch.nn.Linear(embedding_dim, feed_forward_dim)\n",
    "        self.linear_2 = torch.nn.Linear(feed_forward_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13da614c",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09766e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(torch.nn.Module):\n",
    "    def __init__(self, num_tokens, max_sequence_length = 100, embedding_dim = 512, num_layers = 6, num_heads = 4, feed_forward_dim = None, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        if feed_forward_dim is None:\n",
    "            self.feed_forward_dim = embedding_dim * 4\n",
    "        else:\n",
    "            self.feed_forward_dim = feed_forward_dim\n",
    "        \n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.token_embedding = TokenEmbedding(embedding_dim, num_tokens)\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim, max_sequence_length)\n",
    "        self.layer_norm = torch.nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        self.decoder = DecoderStack(embedding_dim, num_layers, num_heads, feed_forward_dim, dropout_rate, max_sequence_length)\n",
    "        self.generator_head = GeneratorHead(embedding_dim, num_tokens)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        token_embedding = self.token_embedding(x)\n",
    "        positional_encoding = self.positional_encoding(token_embedding)\n",
    "        positional_encoding_norm = self.layer_norm(positional_encoding)\n",
    "        decoder_outputs = self.decoder(positional_encoding_norm, mask)\n",
    "        generator_outputs = self.generator_head(decoder_outputs)\n",
    "        \n",
    "        return generator_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8526c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorHead(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, num_tokens):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(embedding_dim, num_tokens)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd66a493",
   "metadata": {},
   "source": [
    "## Autoregressive Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e046b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        inputs, targets = x[:, :-1], x[:, 1:]\n",
    "        mask = mask[:, :-1]\n",
    "        \n",
    "        output = self.model(inputs, mask)\n",
    "        return output, targets\n",
    "    \n",
    "    def next_token_probabilities(self, x, mask, temperature = 1.0):\n",
    "        logits = self.model(x, mask)[:, -1]\n",
    "        \n",
    "        if temperature != 1.0:\n",
    "            logits /= temperature\n",
    "        \n",
    "        probabilities = torch.softmax(logits, dim = -1)\n",
    "        \n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafb117",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "616a9277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, tokenizer: Tokenizer, optimizer = None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        if optimizer is None:\n",
    "            self.optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def train(self, data, epochs, batch_size):\n",
    "        loss_epoch = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            random.shuffle(data)\n",
    "            \n",
    "            batches = []\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                sequence = torch.tensor(data[i: i + batch_size], dtype = torch.long)\n",
    "                mask_tensor = torch.ones_like(sequence)\n",
    "                mask_tensor[sequence == self.tokenizer.character_to_token('<pad>')] = 0\n",
    "                \n",
    "                batches.append((sequence, mask_tensor))\n",
    "                \n",
    "            for batch in batches:\n",
    "                self.model.train()\n",
    "                \n",
    "                input_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype = torch.long)\n",
    "                mask_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype = torch.long)\n",
    "                \n",
    "                for i, inp in enumerate(batch[0]):\n",
    "                    input_tensor[i, :len(inp)] = inp\n",
    "                \n",
    "                for i, mask in enumerate(batch[1]):\n",
    "                    mask_tensor[i, :len(mask)] = mask\n",
    "                    \n",
    "                model_output, target = self.model(input_tensor, mask_tensor)\n",
    "                \n",
    "                loss = self.loss_function(model.output.transpose(1, 2), target)\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                losses.append(loss.item())\n",
    "                \n",
    "            epoch_loss = np.average(losses)\n",
    "            loss_epoch.append(epoch_loss)\n",
    "            print(f\"Epoch: {epoch}, Loss: {epoch_loss}\")\n",
    "        \n",
    "        return loss_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f6ad2",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de4a4ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def pad_left(self, sequence, final_length, padding_token):\n",
    "        return [padding_token] * (final_length - len(sequence)) + sequence\n",
    "        \n",
    "    def generate(self, max_tokens, prompt = None, temperature = 1.0, eos_token = None, padding_token = 0):\n",
    "        self.model.eval()\n",
    "        \n",
    "        if prompt is None:\n",
    "            start_tokens = [padding_token]\n",
    "        else:\n",
    "            start_tokens = self.tokenizer.tokenize(prompt)\n",
    "            \n",
    "        input_tensor = torch.tensor(\n",
    "            self.pad_left(start_tokens, self.model.max_sequence_length, padding_token), dtype = torch.long\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "\n",
    "        for _ in range(max_tokens):\n",
    "            x = input_tensor[:, -self.model.max_sequence_length:]\n",
    "            \n",
    "            mask = torch.ones_like(x)\n",
    "            mask[x == padding_token] = 0\n",
    "            \n",
    "            next_token_prob = self.model.next_token_probabilities(x = x, temperature = temperature, mask = mask)\n",
    "            \n",
    "            next_token = torch.multinomial(next_token_prob, num_samples = 1)\n",
    "            \n",
    "            input_tensor = torch.cat([input_tensor, next_token.unsqueeze(0)], dim = 1)\n",
    "            \n",
    "            if eos_token is not None and next_token == eos_token:\n",
    "                break\n",
    "        \n",
    "        generated_tokens = input_tensor[0].tolist()\n",
    "        return ''.join([self.tokenizer.token_to_character(token) for token in generated_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9d8ca",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2b55e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_sequences(max_sequence_length, tokenized_data):\n",
    "    sequences = []\n",
    "    for i in range(0, len(tokenized_data) - max_sequence_length - 1):\n",
    "        sequences.append(tokenized_data[i: i + max_sequence_length + 1])\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b4010b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data):\n",
    "    tokenized_data = tokenizer.tokenize(training_data)\n",
    "    for _ in range(max_sequence_length):\n",
    "        tokenized_data.insert(0, tokenizer.character_to_token('<pad>'))\n",
    "    \n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0972c19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6390a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92239ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff60f9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2f715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5fd2de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
